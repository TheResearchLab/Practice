{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f98cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlglot in c:\\users\\aaron\\documents\\repos\\practice\\practice_env\\lib\\site-packages (27.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlglot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62555486",
   "metadata": {},
   "source": [
    "# Evaluation steps\n",
    "\n",
    " \n",
    "\n",
    "### Model Eval\n",
    "   - Unions\n",
    "\n",
    "   - CTEs & Recursive CTEs\n",
    "\n",
    "   - Time travel syntax\n",
    "\n",
    "   - Sub-queries\n",
    "\n",
    " \n",
    "\n",
    "### Column Eval\n",
    "  - Aliases\n",
    "\n",
    "  - \"SELECT *\"\n",
    "\n",
    "  - Calculated/Multi-column fields\n",
    "\n",
    "  - Window Functions\n",
    "\n",
    "    - Qualified Column Refs\n",
    "\n",
    " \n",
    "\n",
    "### Other\n",
    "\n",
    "   - Masking salt key in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffc193",
   "metadata": {},
   "source": [
    "# Models Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bac002",
   "metadata": {},
   "source": [
    "## Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc0e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake tables found:\n",
      "  - ECOMMERCE_DB.SALES.ONLINE_ORDERS\n",
      "  - ECOMMERCE_DB.SALES.RETAIL_SALES\n",
      "  - MOBILE_APP_DB.TRANSACTIONS.MOBILE_TRANSACTIONS\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "\"\"\"\n",
    "Can I assume all snowflake compiled models will be formatted as db.schema.tbl? I think?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example UNION query in Snowflake syntax (no quotes)\n",
    "union_query = \"\"\"\n",
    "SELECT CUSTOMER_ID, ORDER_DATE, 'online' AS CHANNEL\n",
    "FROM ECOMMERCE_DB.SALES.ONLINE_ORDERS\n",
    "WHERE ORDER_DATE >= '2024-01-01'\n",
    "UNION ALL\n",
    "SELECT CUSTOMER_ID, PURCHASE_DATE AS ORDER_DATE, 'retail' AS CHANNEL\n",
    "FROM ECOMMERCE_DB.SALES.RETAIL_SALES\n",
    "WHERE PURCHASE_DATE >= '2024-01-01'\n",
    "UNION\n",
    "SELECT CUST_ID AS CUSTOMER_ID, TRANSACTION_DATE AS ORDER_DATE, 'mobile' AS CHANNEL\n",
    "FROM MOBILE_APP_DB.TRANSACTIONS.MOBILE_TRANSACTIONS\n",
    "WHERE TRANSACTION_DATE >= '2024-01-01'\n",
    "\"\"\"\n",
    "\n",
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    tables = set()\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            # Build full table name: DATABASE.SCHEMA.TABLE (no quotes)\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            tables.add(full_name)\n",
    "    return sorted(tables)\n",
    "\n",
    "# Test extraction\n",
    "tables = extract_snowflake_tables(union_query)\n",
    "print(\"Snowflake tables found:\")\n",
    "for t in tables:\n",
    "    print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587a46f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  - db1.schema1.tableA\n",
      "  - db2.schema2.tableB\n",
      "\n",
      "Test case 2:\n",
      "  - analytics.inactive_users\n",
      "  - analytics.orders\n",
      "  - analytics.users\n",
      "\n",
      "Test case 3:\n",
      "  - marketing.leads\n",
      "  - recent_orders\n",
      "  - sales.customers\n",
      "  - sales.orders\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    # 1. Simple UNION with single tables\n",
    "    \"\"\"\n",
    "    SELECT id FROM db1.schema1.tableA\n",
    "    UNION\n",
    "    SELECT id FROM db2.schema2.tableB\n",
    "    \"\"\",\n",
    "\n",
    "    # 2. UNION ALL with JOIN and subquery\n",
    "    \"\"\"\n",
    "    SELECT u.user_id, o.order_id\n",
    "    FROM analytics.users u\n",
    "    JOIN analytics.orders o ON u.user_id = o.user_id\n",
    "    UNION ALL\n",
    "    SELECT user_id, NULL\n",
    "    FROM analytics.inactive_users\n",
    "    WHERE last_login < '2024-01-01'\n",
    "    \"\"\",\n",
    "\n",
    "    # 3. UNION with nested SELECT and CTE\n",
    "    \"\"\"\n",
    "    WITH recent_orders AS (\n",
    "        SELECT order_id, customer_id\n",
    "        FROM sales.orders\n",
    "        WHERE order_date > '2025-01-01'\n",
    "    )\n",
    "    SELECT customer_id FROM recent_orders\n",
    "    UNION\n",
    "    SELECT customer_id FROM sales.customers\n",
    "    WHERE signup_date > '2025-01-01'\n",
    "    UNION ALL\n",
    "    SELECT customer_id FROM marketing.leads\n",
    "    WHERE source = 'web'\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    tables = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dfe4d",
   "metadata": {},
   "source": [
    "## CTEs and Recursive CTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d70fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  - db1.schema1.tableA\n",
      "  - db2.schema2.tableB\n",
      "\n",
      "Test case 2:\n",
      "  - analytics.inactive_users\n",
      "  - analytics.orders\n",
      "  - analytics.users\n",
      "\n",
      "Test case 3:\n",
      "  - marketing.leads\n",
      "  - recent_orders\n",
      "  - sales.customers\n",
      "  - sales.orders\n",
      "\n",
      "Test case 4:\n",
      "  - active_customers\n",
      "  - crm_db.marketing.leads\n",
      "  - crm_db.sales.customers\n",
      "  - crm_db.sales.orders\n",
      "  - crm_db.sales.products\n",
      "  - crm_db.sales.returns\n",
      "  - recent_orders\n"
     ]
    }
   ],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH active_customers AS (\n",
    "        SELECT customer_id\n",
    "        FROM crm_db.sales.customers\n",
    "        WHERE status = 'active'\n",
    "    ),\n",
    "    recent_orders AS (\n",
    "        SELECT order_id, customer_id\n",
    "        FROM crm_db.sales.orders\n",
    "        WHERE order_date > '2025-01-01'\n",
    "    ),\n",
    "    top_products AS (\n",
    "        SELECT product_id\n",
    "        FROM crm_db.sales.products\n",
    "        WHERE rating > 4.5\n",
    "    )\n",
    "    SELECT ac.customer_id, ro.order_id\n",
    "    FROM active_customers ac\n",
    "    JOIN recent_orders ro ON ac.customer_id = ro.customer_id\n",
    "    UNION\n",
    "    SELECT customer_id, NULL\n",
    "    FROM crm_db.marketing.leads\n",
    "    WHERE source = 'web'\n",
    "    UNION ALL\n",
    "    SELECT NULL, order_id\n",
    "    FROM recent_orders\n",
    "    WHERE order_id NOT IN (SELECT order_id FROM crm_db.sales.returns)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    tables = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17aafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  - db1.schema1.tableA\n",
      "  - db2.schema2.tableB\n",
      "\n",
      "Test case 2:\n",
      "  - analytics.inactive_users\n",
      "  - analytics.orders\n",
      "  - analytics.users\n",
      "\n",
      "Test case 3:\n",
      "  - marketing.leads\n",
      "  - recent_orders\n",
      "  - sales.customers\n",
      "  - sales.orders\n",
      "\n",
      "Test case 4:\n",
      "  - active_customers\n",
      "  - crm_db.marketing.leads\n",
      "  - crm_db.sales.customers\n",
      "  - crm_db.sales.orders\n",
      "  - crm_db.sales.products\n",
      "  - crm_db.sales.returns\n",
      "  - recent_orders\n",
      "\n",
      "Test case 5:\n",
      "  - __dbt__cte__dummy_data\n",
      "  - dummy_schema.dummy_dim\n",
      "  - dummy_schema.dummy_table\n",
      "  - get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        dd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN dummy_schema.dummy_dim dd ON gd.dummy_ver_name = dd.dummy_ver_name\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    tables = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53c31d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  Physical tables:\n",
      "    - db1.schema1.tableA\n",
      "    - db2.schema2.tableB\n",
      "  CTE names:\n",
      "\n",
      "Test case 2:\n",
      "  Physical tables:\n",
      "    - analytics.inactive_users\n",
      "    - analytics.orders\n",
      "    - analytics.users\n",
      "  CTE names:\n",
      "\n",
      "Test case 3:\n",
      "  Physical tables:\n",
      "    - marketing.leads\n",
      "    - sales.customers\n",
      "    - sales.orders\n",
      "  CTE names:\n",
      "    - recent_orders\n",
      "\n",
      "Test case 4:\n",
      "  Physical tables:\n",
      "    - crm_db.marketing.leads\n",
      "    - crm_db.sales.customers\n",
      "    - crm_db.sales.orders\n",
      "    - crm_db.sales.products\n",
      "    - crm_db.sales.returns\n",
      "  CTE names:\n",
      "    - active_customers\n",
      "    - recent_orders\n",
      "    - top_products\n",
      "\n",
      "Test case 5:\n",
      "  Physical tables:\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    tables = set()\n",
    "    cte_names = set()\n",
    "\n",
    "    # Collect CTE names\n",
    "    for node in parsed.find_all(exp.CTE):\n",
    "        if node.alias:\n",
    "            cte_names.add(node.alias)\n",
    "\n",
    "    # Collect all table references\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            tables.add(full_name)\n",
    "\n",
    "    # Separate physical tables from CTEs\n",
    "    physical_tables = [t for t in tables if t not in cte_names]\n",
    "    return sorted(physical_tables), sorted(cte_names)\n",
    "\n",
    "\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    physical_tables, cte_names = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    print(\"  Physical tables:\")\n",
    "    for t in physical_tables:\n",
    "        print(f\"    - {t}\")\n",
    "    print(\"  CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"    - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06db9a4",
   "metadata": {},
   "source": [
    "## Timestamp Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22cb355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  Physical tables:\n",
      "    - db1.schema1.tableA\n",
      "    - db2.schema2.tableB\n",
      "  CTE names:\n",
      "\n",
      "Test case 2:\n",
      "  Physical tables:\n",
      "    - analytics.inactive_users\n",
      "    - analytics.orders\n",
      "    - analytics.users\n",
      "  CTE names:\n",
      "\n",
      "Test case 3:\n",
      "  Physical tables:\n",
      "    - marketing.leads\n",
      "    - sales.customers\n",
      "    - sales.orders\n",
      "  CTE names:\n",
      "    - recent_orders\n",
      "\n",
      "Test case 4:\n",
      "  Physical tables:\n",
      "    - crm_db.marketing.leads\n",
      "    - crm_db.sales.customers\n",
      "    - crm_db.sales.orders\n",
      "    - crm_db.sales.products\n",
      "    - crm_db.sales.returns\n",
      "  CTE names:\n",
      "    - active_customers\n",
      "    - recent_orders\n",
      "    - top_products\n",
      "\n",
      "Test case 5:\n",
      "  Physical tables:\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "\n",
      "Test case 6:\n",
      "  Physical tables:\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        dd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN dummy_schema.dummy_dim dd ON gd.dummy_ver_name = dd.dummy_ver_name\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    physical_tables, cte_names = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    print(\"  Physical tables:\")\n",
    "    for t in physical_tables:\n",
    "        print(f\"    - {t}\")\n",
    "    print(\"  CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"    - {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2f25c",
   "metadata": {},
   "source": [
    "## Derived/Sub-query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95601eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        gd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef,\n",
    "        sub.latest_status\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            MAX(status_date) AS latest_status\n",
    "        FROM dummy_schema.dummy_status\n",
    "        WHERE status_code IN (\n",
    "            SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
    "        )\n",
    "        GROUP BY dummy_ver_name\n",
    "    ) sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
    "    WHERE gd.dummy_coef > (\n",
    "        SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5530a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  Source/target tables (all physical tables):\n",
      "    - db1.schema1.tableA\n",
      "    - db2.schema2.tableB\n",
      "  CTE names:\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "\n",
      "Test case 2:\n",
      "  Source/target tables (all physical tables):\n",
      "    - analytics.inactive_users\n",
      "    - analytics.orders\n",
      "    - analytics.users\n",
      "  CTE names:\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    u -> analytics.users\n",
      "    o -> analytics.orders\n",
      "\n",
      "Test case 3:\n",
      "  Source/target tables (all physical tables):\n",
      "    - marketing.leads\n",
      "    - sales.customers\n",
      "    - sales.orders\n",
      "  CTE names:\n",
      "    - recent_orders\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "\n",
      "Test case 4:\n",
      "  Source/target tables (all physical tables):\n",
      "    - crm_db.marketing.leads\n",
      "    - crm_db.sales.customers\n",
      "    - crm_db.sales.orders\n",
      "    - crm_db.sales.products\n",
      "    - crm_db.sales.returns\n",
      "  CTE names:\n",
      "    - active_customers\n",
      "    - recent_orders\n",
      "    - top_products\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    ac -> active_customers\n",
      "    ro -> recent_orders\n",
      "\n",
      "Test case 5:\n",
      "  Source/target tables (all physical tables):\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    gd -> get_dummy_data\n",
      "    dd -> dummy_schema.dummy_dim\n",
      "\n",
      "Test case 6:\n",
      "  Source/target tables (all physical tables):\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    gd -> get_dummy_data\n",
      "    dd -> dummy_schema.dummy_dim\n",
      "\n",
      "Test case 7:\n",
      "  Source/target tables (all physical tables):\n",
      "    - dummy_schema.dummy_status\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "    - dummy_schema.dummy_status\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "    - dummy_schema.dummy_table\n",
      "    - dummy_schema.status_codes\n",
      "  Table aliases:\n",
      "    gd -> get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    cte_names = set()\n",
    "    table_aliases = dict()\n",
    "    all_physical_tables = set()\n",
    "    join_subquery_tables = set()\n",
    "    where_subquery_tables = set()\n",
    "    cte_source_tables = set()\n",
    "\n",
    "    # Collect CTE names and their source tables\n",
    "    for cte in parsed.find_all(exp.CTE):\n",
    "        if cte.alias:\n",
    "            cte_names.add(cte.alias)\n",
    "        # Find tables referenced inside CTE definitions\n",
    "        for node in cte.find_all(exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            if full_name not in cte_names:\n",
    "                cte_source_tables.add(full_name)\n",
    "\n",
    "    # Helper to get full table name\n",
    "    def get_full_name(node):\n",
    "        db = node.catalog or \"\"\n",
    "        schema = node.db or \"\"\n",
    "        name = node.name\n",
    "        if db and schema:\n",
    "            return f\"{db}.{schema}.{name}\"\n",
    "        elif schema:\n",
    "            return f\"{schema}.{name}\"\n",
    "        else:\n",
    "            return name\n",
    "\n",
    "    # Collect all physical tables (not CTEs) anywhere in the query\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            full_name = get_full_name(node)\n",
    "            if full_name not in cte_names:\n",
    "                all_physical_tables.add(full_name)\n",
    "            if node.alias:\n",
    "                table_aliases[node.alias] = full_name\n",
    "\n",
    "    # Collect tables in JOIN subqueries and derived tables\n",
    "    for join in parsed.find_all(exp.Join):\n",
    "        for subquery in join.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        join_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    # Collect tables in WHERE subqueries\n",
    "    for where in parsed.find_all(exp.Where):\n",
    "        for subquery in where.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        where_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    valuable_join_tables = sorted(join_subquery_tables - where_subquery_tables)\n",
    "\n",
    "    # Only include as source/target if:\n",
    "    # - referenced in a CTE definition (cte_source_tables)\n",
    "    # - or referenced outside of WHERE subqueries (i.e., not only in where_subquery_tables)\n",
    "    source_target_tables = sorted(\n",
    "        t for t in all_physical_tables\n",
    "        if t in cte_source_tables or t not in where_subquery_tables\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        source_target_tables,\n",
    "        sorted(cte_names),\n",
    "        valuable_join_tables,\n",
    "        sorted(where_subquery_tables),\n",
    "        table_aliases\n",
    "    )\n",
    "\n",
    "# Example usage and test logic:\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    print(\"  Source/target tables (all physical tables):\")\n",
    "    for t in target_tables:\n",
    "        print(f\"    - {t}\")\n",
    "    print(\"  CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"    - {c}\")\n",
    "    print(\"  JOIN/derived subquery tables (valuable for lineage):\")\n",
    "    for j in join_subquery_tables:\n",
    "        print(f\"    - {j}\")\n",
    "    print(\"  WHERE subquery tables (not useful for lineage):\")\n",
    "    for w in where_subquery_tables:\n",
    "        print(f\"    - {w}\")\n",
    "    print(\"  Table aliases:\")\n",
    "    for alias, table in table_aliases.items():\n",
    "        print(f\"    {alias} -> {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbb336",
   "metadata": {},
   "source": [
    "# Columns Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23457fb0",
   "metadata": {},
   "source": [
    "## Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2427349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_column': 'a', 'expression': 'a', 'source_columns': [('', 'a')], 'type': 'direct'}\n",
      "{'target_column': 'sum_col', 'expression': 'b + c AS sum_col', 'source_columns': [('', 'c'), ('', 'b')], 'type': 'calculated'}\n",
      "{'target_column': 'const_col', 'expression': \"'foo' AS const_col\", 'source_columns': [], 'type': 'constant'}\n",
      "{'target_column': 'd_alias', 'expression': 't1.d AS d_alias', 'source_columns': [('t1', 'd')], 'type': 'calculated'}\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of dicts, each describing an output column.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    columns = []\n",
    "\n",
    "    # Helper to get the string representation of an expression\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    # Helper to recursively collect all column references in an expression\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # node.table can be None if unqualified\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Find the outermost SELECT (not inside a subquery)\n",
    "    select = parsed\n",
    "    while not isinstance(select, exp.Select) and select:\n",
    "        select = select.args.get(\"this\") if hasattr(select, \"args\") else None\n",
    "\n",
    "    if not isinstance(select, exp.Select):\n",
    "        # Try to find any SELECT if not top-level\n",
    "        select = next(parsed.find_all(exp.Select), None)\n",
    "\n",
    "    if select:\n",
    "        for proj in select.expressions:\n",
    "            # Target/output column name\n",
    "            alias = proj.alias_or_name\n",
    "            # Raw SQL for the expression\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            # Source columns referenced in the expression\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            # Type: direct, calculated, or constant\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            columns.append({\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "    return columns\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    example_query = \"\"\"\n",
    "    SELECT\n",
    "        a,\n",
    "        b + c AS sum_col,\n",
    "        'foo' AS const_col,\n",
    "        t1.d AS d_alias\n",
    "    FROM my_schema.my_table t1\n",
    "    \"\"\"\n",
    "    cols = extract_snowflake_columns(example_query)\n",
    "    for col in cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24920054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source/target tables (all physical tables):\n",
      "  - my_schema.my_table\n",
      "CTE names:\n",
      "Table aliases:\n",
      "  t1 -> my_schema.my_table\n",
      "\n",
      "Columns lineage:\n",
      "Target column: a\n",
      "  Expression: a\n",
      "  Source columns: [('', 'a')]\n",
      "  Resolved source columns: [('', 'a')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: sum_col\n",
      "  Expression: b + c AS sum_col\n",
      "  Source columns: [('', 'c'), ('', 'b')]\n",
      "  Resolved source columns: [('', 'c'), ('', 'b')]\n",
      "  Type: calculated\n",
      "\n",
      "Target column: const_col\n",
      "  Expression: 'foo' AS const_col\n",
      "  Source columns: []\n",
      "  Resolved source columns: []\n",
      "  Type: constant\n",
      "\n",
      "Target column: d_alias\n",
      "  Expression: t1.d AS d_alias\n",
      "  Source columns: [('t1', 'd')]\n",
      "  Resolved source columns: [('my_schema.my_table', 'd')]\n",
      "  Type: calculated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    cte_names = set()\n",
    "    table_aliases = dict()\n",
    "    all_physical_tables = set()\n",
    "    join_subquery_tables = set()\n",
    "    where_subquery_tables = set()\n",
    "    cte_source_tables = set()\n",
    "\n",
    "    # Collect CTE names and their source tables\n",
    "    for cte in parsed.find_all(exp.CTE):\n",
    "        if cte.alias:\n",
    "            cte_names.add(cte.alias)\n",
    "        # Find tables referenced inside CTE definitions\n",
    "        for node in cte.find_all(exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            if full_name not in cte_names:\n",
    "                cte_source_tables.add(full_name)\n",
    "\n",
    "    # Helper to get full table name\n",
    "    def get_full_name(node):\n",
    "        db = node.catalog or \"\"\n",
    "        schema = node.db or \"\"\n",
    "        name = node.name\n",
    "        if db and schema:\n",
    "            return f\"{db}.{schema}.{name}\"\n",
    "        elif schema:\n",
    "            return f\"{schema}.{name}\"\n",
    "        else:\n",
    "            return name\n",
    "\n",
    "    # Collect all physical tables (not CTEs) anywhere in the query\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            full_name = get_full_name(node)\n",
    "            if full_name not in cte_names:\n",
    "                all_physical_tables.add(full_name)\n",
    "            if node.alias:\n",
    "                table_aliases[node.alias] = full_name\n",
    "\n",
    "    # Collect tables in JOIN subqueries and derived tables\n",
    "    for join in parsed.find_all(exp.Join):\n",
    "        for subquery in join.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        join_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    # Collect tables in WHERE subqueries\n",
    "    for where in parsed.find_all(exp.Where):\n",
    "        for subquery in where.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        where_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    valuable_join_tables = sorted(join_subquery_tables - where_subquery_tables)\n",
    "\n",
    "    # Only include as source/target if:\n",
    "    # - referenced in a CTE definition (cte_source_tables)\n",
    "    # - or referenced outside of WHERE subqueries (i.e., not only in where_subquery_tables)\n",
    "    source_target_tables = sorted(\n",
    "        t for t in all_physical_tables\n",
    "        if t in cte_source_tables or t not in where_subquery_tables\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        source_target_tables,\n",
    "        sorted(cte_names),\n",
    "        valuable_join_tables,\n",
    "        sorted(where_subquery_tables),\n",
    "        table_aliases\n",
    "    )\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of dicts, each describing an output column.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    columns = []\n",
    "\n",
    "    # Helper to get the string representation of an expression\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    # Helper to recursively collect all column references in an expression\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # node.table can be None if unqualified\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Find the outermost SELECT (not inside a subquery)\n",
    "    select = parsed\n",
    "    while not isinstance(select, exp.Select) and select:\n",
    "        select = select.args.get(\"this\") if hasattr(select, \"args\") else None\n",
    "\n",
    "    if not isinstance(select, exp.Select):\n",
    "        # Try to find any SELECT if not top-level\n",
    "        select = next(parsed.find_all(exp.Select), None)\n",
    "\n",
    "    if select:\n",
    "        for proj in select.expressions:\n",
    "            # Target/output column name\n",
    "            alias = proj.alias_or_name\n",
    "            # Raw SQL for the expression\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            # Source columns referenced in the expression\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            # Type: direct, calculated, or constant\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            columns.append({\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "    return columns\n",
    "\n",
    "# Example usage and integration:\n",
    "test_sql = \"\"\"\n",
    "SELECT\n",
    "    a,\n",
    "    b + c AS sum_col,\n",
    "    'foo' AS const_col,\n",
    "    t1.d AS d_alias\n",
    "FROM my_schema.my_table t1\n",
    "\"\"\"\n",
    "\n",
    "# Extract tables and aliases\n",
    "tables_result = extract_snowflake_tables(test_sql)\n",
    "source_target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = tables_result\n",
    "\n",
    "# Extract columns\n",
    "columns_result = extract_snowflake_columns(test_sql)\n",
    "\n",
    "# Resolve source tables for each column using table_aliases\n",
    "for col in columns_result:\n",
    "    resolved_sources = []\n",
    "    for alias, col_name in col[\"source_columns\"]:\n",
    "        if alias in table_aliases:\n",
    "            resolved_sources.append((table_aliases[alias], col_name))\n",
    "        elif alias is None and len(source_target_tables) == 1:\n",
    "            # Unqualified column, only one table in FROM\n",
    "            resolved_sources.append((source_target_tables[0], col_name))\n",
    "        else:\n",
    "            resolved_sources.append((alias, col_name))  # Could be None or a CTE\n",
    "    col[\"resolved_source_columns\"] = resolved_sources\n",
    "\n",
    "# Print results\n",
    "print(\"Source/target tables (all physical tables):\")\n",
    "for t in source_target_tables:\n",
    "    print(f\"  - {t}\")\n",
    "print(\"CTE names:\")\n",
    "for c in cte_names:\n",
    "    print(f\"  - {c}\")\n",
    "print(\"Table aliases:\")\n",
    "for alias, table in table_aliases.items():\n",
    "    print(f\"  {alias} -> {table}\")\n",
    "\n",
    "print(\"\\nColumns lineage:\")\n",
    "for col in columns_result:\n",
    "    print(f\"Target column: {col['target_column']}\")\n",
    "    print(f\"  Expression: {col['expression']}\")\n",
    "    print(f\"  Source columns: {col['source_columns']}\")\n",
    "    print(f\"  Resolved source columns: {col['resolved_source_columns']}\")\n",
    "    print(f\"  Type: {col['type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10eecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEW TEST CASE ===\n",
      "All table variables:\n",
      "  source_target_tables: ['dummy_schema.dummy_status', 'dummy_schema.dummy_table']\n",
      "  cte_names: ['__dbt__cte__dummy_data', 'get_dummy_data']\n",
      "  join_subquery_tables: ['dummy_schema.dummy_status']\n",
      "  where_subquery_tables: ['dummy_schema.dummy_table', 'dummy_schema.status_codes']\n",
      "  table_aliases: {'gd': 'get_dummy_data'}\n",
      "\n",
      "Source/target tables (all physical tables):\n",
      "  - dummy_schema.dummy_status\n",
      "  - dummy_schema.dummy_table\n",
      "CTE names:\n",
      "  - __dbt__cte__dummy_data\n",
      "  - get_dummy_data\n",
      "Table aliases:\n",
      "  gd -> get_dummy_data\n",
      "\n",
      "Columns lineage:\n",
      "Target column: dummy_id\n",
      "  Expression: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "  Source columns: [('gd', 'dummy_ver_name'), ('gd', 'dummy_var_name'), ('gd', 'dummy_pop_name'), ('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name'), ('get_dummy_data', 'dummy_var_name'), ('get_dummy_data', 'dummy_pop_name'), ('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: calculated\n",
      "\n",
      "Target column: dummy_key\n",
      "  Expression: gd.dummy_key\n",
      "  Source columns: [('gd', 'dummy_key')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_key')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_ver_name\n",
      "  Expression: gd.dummy_ver_name\n",
      "  Source columns: [('gd', 'dummy_ver_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_pop_name\n",
      "  Expression: gd.dummy_pop_name\n",
      "  Source columns: [('gd', 'dummy_pop_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_pop_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_level_cd\n",
      "  Expression: gd.dummy_level_cd\n",
      "  Source columns: [('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_var_name\n",
      "  Expression: gd.dummy_var_name\n",
      "  Source columns: [('gd', 'dummy_var_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_var_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_coef\n",
      "  Expression: gd.dummy_coef\n",
      "  Source columns: [('gd', 'dummy_coef')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_coef')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: latest_status\n",
      "  Expression: gd.latest_status\n",
      "  Source columns: [('gd', 'latest_status')]\n",
      "  Resolved source columns: [('get_dummy_data', 'latest_status')]\n",
      "  Type: direct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage and integration:\n",
    "test_sql = \"\"\"\n",
    "SELECT\n",
    "    a,\n",
    "    b + c AS sum_col,\n",
    "    'foo' AS const_col,\n",
    "    t1.d AS d_alias\n",
    "FROM my_schema.my_table t1\n",
    "\"\"\"\n",
    "\n",
    "# Add the complex CTE/subquery example as a test case\n",
    "test_sql_2 = \"\"\"\n",
    "WITH __dbt__cte__dummy_data AS (\n",
    "    SELECT\n",
    "        upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "        upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "        upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "        upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "        nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "    FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
    "),\n",
    "get_dummy_data AS (\n",
    "    SELECT\n",
    "        dummy_ver_name,\n",
    "        dummy_pop_name,\n",
    "        dummy_level_cd,\n",
    "        dummy_var_name,\n",
    "        dummy_coef\n",
    "    FROM __dbt__cte__dummy_data\n",
    ")\n",
    "SELECT\n",
    "    COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "    gd.dummy_key,\n",
    "    gd.dummy_ver_name,\n",
    "    gd.dummy_pop_name,\n",
    "    gd.dummy_level_cd,\n",
    "    gd.dummy_var_name,\n",
    "    gd.dummy_coef,\n",
    "    gd.latest_status\n",
    "FROM get_dummy_data gd\n",
    "INNER JOIN (\n",
    "    SELECT\n",
    "        dummy_ver_name,\n",
    "        MAX(status_date) AS latest_status\n",
    "    FROM dummy_schema.dummy_status\n",
    "    WHERE status_code IN (\n",
    "        SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
    "    )\n",
    "    GROUP BY dummy_ver_name\n",
    ") sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
    "WHERE gd.dummy_coef > (\n",
    "    SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "for sql in [test_sql_2]:\n",
    "    print(\"\\n=== NEW TEST CASE ===\")\n",
    "    tables_result = extract_snowflake_tables(sql)\n",
    "    source_target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = tables_result\n",
    "\n",
    "    # --- Add this block to show all table-related variables ---\n",
    "    print(\"All table variables:\")\n",
    "    print(f\"  source_target_tables: {source_target_tables}\")\n",
    "    print(f\"  cte_names: {cte_names}\")\n",
    "    print(f\"  join_subquery_tables: {join_subquery_tables}\")\n",
    "    print(f\"  where_subquery_tables: {where_subquery_tables}\")\n",
    "    print(f\"  table_aliases: {table_aliases}\")\n",
    "    # If you want to see all_physical_tables and cte_source_tables, you need to modify extract_snowflake_tables to return them as well.\n",
    "    # For now, only the above are available from the return value.\n",
    "    print()\n",
    "\n",
    "    columns_result = extract_snowflake_columns(sql)\n",
    "\n",
    "    for col in columns_result:\n",
    "        resolved_sources = []\n",
    "        for alias, col_name in col[\"source_columns\"]:\n",
    "            if alias in table_aliases:\n",
    "                resolved_sources.append((table_aliases[alias], col_name))\n",
    "            elif alias is None and len(source_target_tables) == 1:\n",
    "                resolved_sources.append((source_target_tables[0], col_name))\n",
    "            else:\n",
    "                resolved_sources.append((alias, col_name))\n",
    "        col[\"resolved_source_columns\"] = resolved_sources\n",
    "\n",
    "    print(\"Source/target tables (all physical tables):\")\n",
    "    for t in source_target_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"  - {c}\")\n",
    "    print(\"Table aliases:\")\n",
    "    for alias, table in table_aliases.items():\n",
    "        print(f\"  {alias} -> {table}\")\n",
    "\n",
    "    print(\"\\nColumns lineage:\")\n",
    "    for col in columns_result:\n",
    "        print(f\"Target column: {col['target_column']}\")\n",
    "        print(f\"  Expression: {col['expression']}\")\n",
    "        print(f\"  Source columns: {col['source_columns']}\")\n",
    "        print(f\"  Resolved source columns: {col['resolved_source_columns']}\")\n",
    "        print(f\"  Type: {col['type']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0222ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEW TEST CASE ===\n",
      "\n",
      "    WITH __dbt__cte__dummy_data AS (\n",
      "        SELECT\n",
      "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
      "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
      "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
      "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
      "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
      "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
      "    ),\n",
      "    get_dummy_data AS (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            dummy_pop_name,\n",
      "            dummy_level_cd,\n",
      "            dummy_var_name,\n",
      "            dummy_coef\n",
      "        FROM __dbt__cte__dummy_data\n",
      "    )\n",
      "    SELECT\n",
      "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
      "        gd.dummy_key,\n",
      "        gd.dummy_ver_name,\n",
      "        gd.dummy_pop_name,\n",
      "        gd.dummy_level_cd,\n",
      "        gd.dummy_var_name,\n",
      "        gd.dummy_coef,\n",
      "        sub.latest_status\n",
      "    FROM get_dummy_data gd\n",
      "    INNER JOIN (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            MAX(status_date) AS latest_status\n",
      "        FROM dummy_schema.dummy_status\n",
      "        WHERE status_code IN (\n",
      "            SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
      "        )\n",
      "        GROUP BY dummy_ver_name\n",
      "    ) sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
      "    WHERE gd.dummy_coef > (\n",
      "        SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
      "    )\n",
      "    \n",
      "All table variables:\n",
      "  source_target_tables: ['dummy_schema.dummy_status', 'dummy_schema.dummy_table']\n",
      "  cte_names: ['__dbt__cte__dummy_data', 'get_dummy_data']\n",
      "  join_subquery_tables: ['dummy_schema.dummy_status']\n",
      "  where_subquery_tables: ['dummy_schema.dummy_table', 'dummy_schema.status_codes']\n",
      "  table_aliases: {'gd': 'get_dummy_data'}\n",
      "\n",
      "Source/target tables (all physical tables):\n",
      "  - dummy_schema.dummy_status\n",
      "  - dummy_schema.dummy_table\n",
      "CTE names:\n",
      "  - __dbt__cte__dummy_data\n",
      "  - get_dummy_data\n",
      "Table aliases:\n",
      "  gd -> get_dummy_data\n",
      "\n",
      "Columns lineage:\n",
      "Target column: dummy_id\n",
      "  Expression: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "  Source columns: [('gd', 'dummy_ver_name'), ('gd', 'dummy_var_name'), ('gd', 'dummy_pop_name'), ('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name'), ('get_dummy_data', 'dummy_var_name'), ('get_dummy_data', 'dummy_pop_name'), ('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: calculated\n",
      "\n",
      "Target column: dummy_key\n",
      "  Expression: gd.dummy_key\n",
      "  Source columns: [('gd', 'dummy_key')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_key')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_ver_name\n",
      "  Expression: gd.dummy_ver_name\n",
      "  Source columns: [('gd', 'dummy_ver_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_pop_name\n",
      "  Expression: gd.dummy_pop_name\n",
      "  Source columns: [('gd', 'dummy_pop_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_pop_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_level_cd\n",
      "  Expression: gd.dummy_level_cd\n",
      "  Source columns: [('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_var_name\n",
      "  Expression: gd.dummy_var_name\n",
      "  Source columns: [('gd', 'dummy_var_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_var_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_coef\n",
      "  Expression: gd.dummy_coef\n",
      "  Source columns: [('gd', 'dummy_coef')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_coef')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: latest_status\n",
      "  Expression: sub.latest_status\n",
      "  Source columns: [('sub', 'latest_status')]\n",
      "  Resolved source columns: [('sub', 'latest_status')]\n",
      "  Type: direct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sql in test_queries[6:7]:\n",
    "    print(\"\\n=== NEW TEST CASE ===\")\n",
    "    print(sql)\n",
    "    tables_result = extract_snowflake_tables(sql)\n",
    "    source_target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = tables_result\n",
    "\n",
    "    # --- Add this block to show all table-related variables ---\n",
    "    print(\"All table variables:\")\n",
    "    print(f\"  source_target_tables: {source_target_tables}\")\n",
    "    print(f\"  cte_names: {cte_names}\")\n",
    "    print(f\"  join_subquery_tables: {join_subquery_tables}\")\n",
    "    print(f\"  where_subquery_tables: {where_subquery_tables}\")\n",
    "    print(f\"  table_aliases: {table_aliases}\")\n",
    "    # If you want to see all_physical_tables and cte_source_tables, you need to modify extract_snowflake_tables to return them as well.\n",
    "    # For now, only the above are available from the return value.\n",
    "    print()\n",
    "\n",
    "    columns_result = extract_snowflake_columns(sql)\n",
    "\n",
    "    for col in columns_result:\n",
    "        resolved_sources = []\n",
    "        for alias, col_name in col[\"source_columns\"]:\n",
    "            if alias in table_aliases:\n",
    "                resolved_sources.append((table_aliases[alias], col_name))\n",
    "            elif alias is None and len(source_target_tables) == 1:\n",
    "                resolved_sources.append((source_target_tables[0], col_name))\n",
    "            else:\n",
    "                resolved_sources.append((alias, col_name))\n",
    "        col[\"resolved_source_columns\"] = resolved_sources\n",
    "\n",
    "    print(\"Source/target tables (all physical tables):\")\n",
    "    for t in source_target_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"  - {c}\")\n",
    "    print(\"Table aliases:\")\n",
    "    for alias, table in table_aliases.items():\n",
    "        print(f\"  {alias} -> {table}\")\n",
    "\n",
    "    print(\"\\nColumns lineage:\")\n",
    "    for col in columns_result:\n",
    "        print(f\"Target column: {col['target_column']}\")\n",
    "        print(f\"  Expression: {col['expression']}\")\n",
    "        print(f\"  Source columns: {col['source_columns']}\")\n",
    "        print(f\"  Resolved source columns: {col['resolved_source_columns']}\")\n",
    "        print(f\"  Type: {col['type']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6edc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No source column identified because col is coming from 3 tables?\n",
    "# How to align query alias \"sub\" with source table from within derived query? \n",
    "# Need to work on table to column association when is a column\n",
    "# for models that can't find their source is there a source search based on the tables in the model?\n",
    "# enforce best practice to add this aliases to all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdabf120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Query 1 ===\n",
      "\n",
      "    WITH __dbt__cte__dummy_data AS (\n",
      "        SELECT\n",
      "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
      "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
      "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
      "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
      "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
      "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
      "    ),\n",
      "    get_dummy_data AS (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            dummy_pop_name,\n",
      "            dummy_level_cd,\n",
      "            dummy_var_name,\n",
      "            dummy_coef\n",
      "        FROM __dbt__cte__dummy_data\n",
      "    )\n",
      "    SELECT\n",
      "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
      "        gd.dummy_key,\n",
      "        gd.dummy_ver_name,\n",
      "        gd.dummy_pop_name,\n",
      "        gd.dummy_level_cd,\n",
      "        gd.dummy_var_name,\n",
      "        gd.dummy_coef,\n",
      "        sub.latest_status\n",
      "    FROM get_dummy_data gd\n",
      "    INNER JOIN (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            MAX(status_date) AS latest_status\n",
      "        FROM dummy_schema.dummy_status\n",
      "        WHERE status_code IN (\n",
      "            SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
      "        )\n",
      "        GROUP BY dummy_ver_name\n",
      "    ) sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
      "    WHERE gd.dummy_coef > (\n",
      "        SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
      "    )\n",
      "    \n",
      "\n",
      "  SELECT branch 1:\n",
      "    Target column: dummy_id\n",
      "      Expression: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "      Source columns: [('gd', 'dummy_ver_name'), ('gd', 'dummy_var_name'), ('gd', 'dummy_pop_name'), ('gd', 'dummy_level_cd')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_key\n",
      "      Expression: gd.dummy_key\n",
      "      Source columns: [('gd', 'dummy_key')]\n",
      "      Type: direct\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: gd.dummy_ver_name\n",
      "      Source columns: [('gd', 'dummy_ver_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_pop_name\n",
      "      Expression: gd.dummy_pop_name\n",
      "      Source columns: [('gd', 'dummy_pop_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_level_cd\n",
      "      Expression: gd.dummy_level_cd\n",
      "      Source columns: [('gd', 'dummy_level_cd')]\n",
      "      Type: direct\n",
      "    Target column: dummy_var_name\n",
      "      Expression: gd.dummy_var_name\n",
      "      Source columns: [('gd', 'dummy_var_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_coef\n",
      "      Expression: gd.dummy_coef\n",
      "      Source columns: [('gd', 'dummy_coef')]\n",
      "      Type: direct\n",
      "    Target column: latest_status\n",
      "      Expression: sub.latest_status\n",
      "      Source columns: [('sub', 'latest_status')]\n",
      "      Type: direct\n",
      "  SELECT branch 2:\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: dummy_ver_name\n",
      "      Source columns: [('', 'dummy_ver_name')]\n",
      "      Type: direct\n",
      "    Target column: latest_status\n",
      "      Expression: MAX(status_date) AS latest_status\n",
      "      Source columns: [('', 'status_date')]\n",
      "      Type: calculated\n",
      "  SELECT branch 3:\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VER_NAME') AS TEXT), '')) AS dummy_ver_name\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_pop_name\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_POP_NAME') AS TEXT), '')) AS dummy_pop_name\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_level_cd\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_LEVEL_CD') AS TEXT), '')) AS dummy_level_cd\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_var_name\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VAR_NAME') AS TEXT), '')) AS dummy_var_name\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_coef\n",
      "      Expression: CAST(NULLIF(CAST(GET_PATH(v, 'DUMMY_COEF') AS TEXT), '') AS DECIMAL(8, 3)) AS dummy_coef\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "  SELECT branch 4:\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: dummy_ver_name\n",
      "      Source columns: [('', 'dummy_ver_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_pop_name\n",
      "      Expression: dummy_pop_name\n",
      "      Source columns: [('', 'dummy_pop_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_level_cd\n",
      "      Expression: dummy_level_cd\n",
      "      Source columns: [('', 'dummy_level_cd')]\n",
      "      Type: direct\n",
      "    Target column: dummy_var_name\n",
      "      Expression: dummy_var_name\n",
      "      Source columns: [('', 'dummy_var_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_coef\n",
      "      Expression: dummy_coef\n",
      "      Source columns: [('', 'dummy_coef')]\n",
      "      Type: direct\n",
      "  SELECT branch 5:\n",
      "    Target column: \n",
      "      Expression: AVG(dummy_coef)\n",
      "      Source columns: [('', 'dummy_coef')]\n",
      "      Type: calculated\n",
      "  SELECT branch 6:\n",
      "    Target column: code\n",
      "      Expression: code\n",
      "      Source columns: [('', 'code')]\n",
      "      Type: direct\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of dicts, each describing an output column.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    columns = []\n",
    "\n",
    "    # Helper to get the string representation of an expression\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    # Helper to recursively collect all column references in an expression\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Find all SELECTs in the query (including UNION branches, subqueries, etc.)\n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "\n",
    "    all_columns = []\n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        all_columns.append(select_columns)\n",
    "    return all_columns\n",
    "\n",
    "# Example usage:\n",
    "for i, sql in enumerate(test_queries[6:7], 1):\n",
    "    print(f\"\\n=== Test Query {i} ===\")\n",
    "    print(f'{sql}\\n')\n",
    "    all_columns = extract_snowflake_columns(sql)\n",
    "    for select_idx, select_columns in enumerate(all_columns):\n",
    "        print(f\"  SELECT branch {select_idx+1}:\")\n",
    "        for col in select_columns:\n",
    "            print(f\"    Target column: {col['target_column']}\")\n",
    "            print(f\"      Expression: {col['expression']}\")\n",
    "            print(f\"      Source columns: {col['source_columns']}\")\n",
    "            print(f\"      Type: {col['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8e96e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Query 1 ===\n",
      "\n",
      "\n",
      "    SELECT u.user_id, o.order_id\n",
      "    FROM analytics.users u\n",
      "    JOIN analytics.orders o ON u.user_id = o.user_id\n",
      "    UNION ALL\n",
      "    SELECT user_id, NULL\n",
      "    FROM analytics.inactive_users\n",
      "    WHERE last_login < '2024-01-01'\n",
      "    \n",
      "\n",
      "  SELECT branch 1:\n",
      "    Target column: user_id\n",
      "      Expression: u.user_id\n",
      "      Source columns: [('u', 'user_id')]\n",
      "      Resolved source columns: [('analytics.users', 'u', 'user_id')]\n",
      "      Type: direct\n",
      "    Target column: order_id\n",
      "      Expression: o.order_id\n",
      "      Source columns: [('o', 'order_id')]\n",
      "      Resolved source columns: [('o', 'order_id')]\n",
      "      Type: direct\n",
      "  SELECT branch 2:\n",
      "    Target column: user_id\n",
      "      Expression: user_id\n",
      "      Source columns: [('', 'user_id')]\n",
      "      Resolved source columns: [('analytics.inactive_users', 'user_id')]\n",
      "      Type: direct\n",
      "    Target column: NULL\n",
      "      Expression: NULL\n",
      "      Source columns: []\n",
      "      Resolved source columns: []\n",
      "      Type: constant\n"
     ]
    }
   ],
   "source": [
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        \"\"\"\n",
    "        tables = {}\n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                if db and schema:\n",
    "                    full_name = f\"{db}.{schema}.{name}\"\n",
    "                elif schema:\n",
    "                    full_name = f\"{schema}.{name}\"\n",
    "                else:\n",
    "                    full_name = name\n",
    "                alias = base.alias or name\n",
    "                tables[alias.lower()] = (full_name, alias)\n",
    "            # JOINed tables\n",
    "            for join in from_expr.find_all(exp.Join):\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = join_table.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "        return tables\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    all_columns = []\n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            for tbl_alias, col_name in source_columns:\n",
    "                if (not tbl_alias or tbl_alias == \"\") and only_table:\n",
    "                    resolved_sources.append((only_table, col_name))\n",
    "                elif tbl_alias:\n",
    "                    tbl_alias_lc = tbl_alias.lower()\n",
    "                    if tbl_alias_lc in from_tables:\n",
    "                        full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                        resolved_sources.append((full_table, real_alias, col_name))\n",
    "                    else:\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                else:\n",
    "                    resolved_sources.append((tbl_alias, col_name))\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        all_columns.append(select_columns)\n",
    "    return all_columns\n",
    "\n",
    "for i, sql in enumerate(test_queries[1:2], 1):\n",
    "    print(f\"\\n=== Test Query {i} ===\")\n",
    "    print(f'\\n{sql}\\n')\n",
    "    all_columns = extract_snowflake_columns(sql)\n",
    "    for select_idx, select_columns in enumerate(all_columns):\n",
    "        print(f\"  SELECT branch {select_idx+1}:\")\n",
    "        for col in select_columns:\n",
    "            print(f\"    Target column: {col['target_column']}\")\n",
    "            print(f\"      Expression: {col['expression']}\")\n",
    "            print(f\"      Source columns: {col['source_columns']}\")\n",
    "            print(f\"      Resolved source columns: {col.get('resolved_source_columns', [])}\")\n",
    "            print(f\"      Type: {col['type']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55b011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Query 1 ===\n",
      "\n",
      "\n",
      "    SELECT id FROM db1.schema1.tableA\n",
      "    UNION\n",
      "    SELECT id FROM db2.schema2.tableB\n",
      "    \n",
      "\n",
      "  SELECT branch 1:\n",
      "    Target column: id\n",
      "      Expression: id\n",
      "      Source columns: [('', 'id')]\n",
      "      Resolved source columns: [('db1.schema1.tableA', 'id')]\n",
      "      Type: direct\n",
      "  SELECT branch 2:\n",
      "    Target column: id\n",
      "      Expression: id\n",
      "      Source columns: [('', 'id')]\n",
      "      Resolved source columns: [('db2.schema2.tableB', 'id')]\n",
      "      Type: direct\n"
     ]
    }
   ],
   "source": [
    "# claude attempt\n",
    "\n",
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # Get table alias/name - could be empty string\n",
    "                table_ref = node.table if node.table else \"\"\n",
    "                sources.add((table_ref, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        \"\"\"\n",
    "        tables = {}\n",
    "        \n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                if db and schema:\n",
    "                    full_name = f\"{db}.{schema}.{name}\"\n",
    "                elif schema:\n",
    "                    full_name = f\"{schema}.{name}\"\n",
    "                else:\n",
    "                    full_name = name\n",
    "                alias = base.alias or name\n",
    "                tables[alias.lower()] = (full_name, alias)\n",
    "        \n",
    "        # JOINs are stored at the SELECT level, not FROM level\n",
    "        joins = select.args.get(\"joins\")\n",
    "        if joins:\n",
    "            for join in joins:\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = join_table.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "                        \n",
    "        return tables\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    all_columns = []\n",
    "    \n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        \n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            \n",
    "            for tbl_alias, col_name in source_columns:\n",
    "                if not tbl_alias and only_table:\n",
    "                    # No table alias and only one table - use that table\n",
    "                    resolved_sources.append((only_table, col_name))\n",
    "                elif tbl_alias:\n",
    "                    # Has table alias - look it up in from_tables\n",
    "                    tbl_alias_lc = tbl_alias.lower()\n",
    "                    if tbl_alias_lc in from_tables:\n",
    "                        full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                        resolved_sources.append((full_table, real_alias, col_name))\n",
    "                    else:\n",
    "                        # Alias not found in from_tables - keep as is\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                else:\n",
    "                    # No table alias and multiple tables - ambiguous\n",
    "                    resolved_sources.append((tbl_alias, col_name))\n",
    "            \n",
    "            # Determine column type\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            \n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        \n",
    "        all_columns.append(select_columns)\n",
    "    \n",
    "    return all_columns\n",
    "\n",
    "# Test with your example query\n",
    "test_query = \"\"\"\n",
    "SELECT u.user_id, o.order_id\n",
    "FROM analytics.users u\n",
    "JOIN analytics.orders o ON u.user_id = o.user_id\n",
    "UNION ALL\n",
    "SELECT user_id, NULL\n",
    "FROM analytics.inactive_users\n",
    "WHERE last_login < '2024-01-01'\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Test Query 1 ===\")\n",
    "print(f'\\n{test_queries[0]}\\n')\n",
    "all_columns = extract_snowflake_columns(test_queries[0])\n",
    "\n",
    "for select_idx, select_columns in enumerate(all_columns):\n",
    "    print(f\"  SELECT branch {select_idx+1}:\")\n",
    "    for col in select_columns:\n",
    "        print(f\"    Target column: {col['target_column']}\")\n",
    "        print(f\"      Expression: {col['expression']}\")\n",
    "        print(f\"      Source columns: {col['source_columns']}\")\n",
    "        print(f\"      Resolved source columns: {col.get('resolved_source_columns', [])}\")\n",
    "        print(f\"      Type: {col['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f2f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a799445",
   "metadata": {},
   "source": [
    "## Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5437869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nested CTE resolution fix...\n",
      "\n",
      "=== TESTING NESTED CTE RESOLUTION ===\n",
      "Expected: customer_id should trace through both CTEs to 'customers' table\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'with_clause' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 659\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting nested CTE resolution fix...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# Run tests\u001b[39;00m\n\u001b[1;32m--> 659\u001b[0m nested_ok \u001b[38;5;241m=\u001b[39m \u001b[43mtest_nested_cte_fix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m simple_ok \u001b[38;5;241m=\u001b[39m test_simple_cte_still_works()\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== RESULTS ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 600\u001b[0m, in \u001b[0;36mtest_nested_cte_fix\u001b[1;34m()\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: customer_id should trace through both CTEs to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomers\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m--> 600\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_column_lineage\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 240\u001b[0m, in \u001b[0;36mtrace_column_lineage\u001b[1;34m(sql_query, target_column_name, existing_cte_registry)\u001b[0m\n\u001b[0;32m    237\u001b[0m             cte_registry[cte_name\u001b[38;5;241m.\u001b[39mlower()] \u001b[38;5;241m=\u001b[39m cte_query\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Get basic column analysis (pass CTE registry for nested CTE detection)\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m base_columns \u001b[38;5;241m=\u001b[39m \u001b[43mextract_snowflake_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcte_registry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Find the target column in the final output\u001b[39;00m\n\u001b[0;32m    243\u001b[0m target_column_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 107\u001b[0m, in \u001b[0;36mextract_snowflake_columns\u001b[1;34m(sql_query, existing_cte_registry)\u001b[0m\n\u001b[0;32m    104\u001b[0m cte_select_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Mark all CTE definition selects\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mwith_clause\u001b[49m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cte \u001b[38;5;129;01min\u001b[39;00m with_clause\u001b[38;5;241m.\u001b[39mexpressions:\n\u001b[0;32m    109\u001b[0m         cte_select \u001b[38;5;241m=\u001b[39m cte\u001b[38;5;241m.\u001b[39mthis\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'with_clause' where it is not associated with a value"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a16160f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUICK NESTED CTE TEST ===\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'with_clause' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Reason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_reason\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39mcol\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace_reason\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[43mquick_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m, in \u001b[0;36mquick_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mWITH base_customers AS (\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    SELECT customer_id, customer_name\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124mFROM enriched_customers\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== QUICK NESTED CTE TEST ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_column_lineage\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDependencies found:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_columns_to_search\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "Cell \u001b[1;32mIn[18], line 240\u001b[0m, in \u001b[0;36mtrace_column_lineage\u001b[1;34m(sql_query, target_column_name, existing_cte_registry)\u001b[0m\n\u001b[0;32m    237\u001b[0m             cte_registry[cte_name\u001b[38;5;241m.\u001b[39mlower()] \u001b[38;5;241m=\u001b[39m cte_query\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Get basic column analysis (pass CTE registry for nested CTE detection)\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m base_columns \u001b[38;5;241m=\u001b[39m \u001b[43mextract_snowflake_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcte_registry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Find the target column in the final output\u001b[39;00m\n\u001b[0;32m    243\u001b[0m target_column_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 104\u001b[0m, in \u001b[0;36mextract_snowflake_columns\u001b[1;34m(sql_query, existing_cte_registry)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# FIXED: Find the outermost/final SELECT (not part of CTE definitions)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m final_selects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 104\u001b[0m \u001b[43mcte_select_ids\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Mark all CTE definition selects\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_clause:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'with_clause' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Copy the functions and test the nested CTE fix\n",
    "\n",
    "# [The complete fixed functions would be copied here - same as the artifact above]\n",
    "\n",
    "# Quick test for the nested CTE issue\n",
    "def quick_test():\n",
    "    sql = \"\"\"\n",
    "    WITH base_customers AS (\n",
    "        SELECT customer_id, customer_name\n",
    "        FROM customers\n",
    "    ),\n",
    "    enriched_customers AS (\n",
    "        SELECT customer_id, customer_name, 'active' as status\n",
    "        FROM base_customers  \n",
    "    )\n",
    "    SELECT customer_id, status\n",
    "    FROM enriched_customers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== QUICK NESTED CTE TEST ===\")\n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    \n",
    "    print(\"Dependencies found:\")\n",
    "    for col in result.get(\"next_columns_to_search\", []):\n",
    "        print(f\"  - {col['table']}.{col['column']} ({col.get('level', 'unknown')})\")\n",
    "    \n",
    "    # Success check\n",
    "    customers_found = any(col['table'] == 'customers' for col in result.get(\"next_columns_to_search\", []))\n",
    "    print(f\"\\nResult: {'✅ SUCCESS' if customers_found else '❌ STILL BROKEN'}\")\n",
    "    \n",
    "    if not customers_found:\n",
    "        print(\"\\nDebug - what we found instead:\")\n",
    "        for col in result.get(\"next_columns_to_search\", []):\n",
    "            print(f\"  Table: {col['table']}\")\n",
    "            print(f\"  Level: {col.get('level')}\")\n",
    "            print(f\"  Reason: {col.get('source_reason', col.get('trace_reason', 'unknown'))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10772007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': \"Column 'etg_cd' not found in query output\",\n",
       " 'llm_context': \"The column 'etg_cd' was not found in the final query output.\",\n",
       " 'next_columns_to_search': [],\n",
       " 'full_lineage': {}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_column_lineage(query,\"etg_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e0cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query, existing_cte_registry=None):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    FIXED: Now handles SELECT * properly and accepts existing CTE registry\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # Get table alias/name - could be empty string\n",
    "                table_ref = node.table if node.table else \"\"\n",
    "                sources.add((table_ref, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select, cte_registry=None):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        Now also considers CTEs in the registry\n",
    "        \"\"\"\n",
    "        if cte_registry is None:\n",
    "            cte_registry = {}\n",
    "            \n",
    "        tables = {}\n",
    "        \n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                \n",
    "                # Check if this is a CTE first\n",
    "                if name.lower() in cte_registry:\n",
    "                    # This is a CTE reference\n",
    "                    alias = base.alias or name\n",
    "                    tables[alias.lower()] = (f\"CTE:{name}\", alias)\n",
    "                else:\n",
    "                    # Regular table\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = base.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "        \n",
    "        # JOINs are stored at the SELECT level, not FROM level\n",
    "        joins = select.args.get(\"joins\")\n",
    "        if joins:\n",
    "            for join in joins:\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    \n",
    "                    # Check if this is a CTE first\n",
    "                    if name.lower() in cte_registry:\n",
    "                        # This is a CTE reference\n",
    "                        alias = join_table.alias or name\n",
    "                        tables[alias.lower()] = (f\"CTE:{name}\", alias)\n",
    "                    else:\n",
    "                        # Regular table\n",
    "                        if db and schema:\n",
    "                            full_name = f\"{db}.{schema}.{name}\"\n",
    "                        elif schema:\n",
    "                            full_name = f\"{schema}.{name}\"\n",
    "                        else:\n",
    "                            full_name = name\n",
    "                        alias = join_table.alias or name\n",
    "                        tables[alias.lower()] = (full_name, alias)\n",
    "                        \n",
    "        return tables\n",
    "    \n",
    "    # Build CTE registry first (or use existing one)\n",
    "    cte_registry = existing_cte_registry or {}\n",
    "    with_clause = parsed.args.get(\"with\")\n",
    "    \n",
    "    # If no existing registry, build it from this query\n",
    "    if not existing_cte_registry and with_clause:\n",
    "        for cte in with_clause.expressions:\n",
    "            cte_name = cte.alias\n",
    "            cte_query = cte.this  # The SELECT part of the CTE\n",
    "            cte_registry[cte_name.lower()] = cte_query\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    \n",
    "    # FIXED: Find the outermost/final SELECT (not part of CTE definitions)\n",
    "    final_selects = []\n",
    "    cte_select_ids = set()\n",
    "    \n",
    "    # Mark all CTE definition selects\n",
    "    if with_clause:\n",
    "        for cte in with_clause.expressions:\n",
    "            cte_select = cte.this\n",
    "            cte_select_ids.add(id(cte_select))\n",
    "    \n",
    "    # Only include selects that are NOT part of CTE definitions\n",
    "    for select in selects:\n",
    "        if id(select) not in cte_select_ids:\n",
    "            final_selects.append(select)\n",
    "    \n",
    "    # If no final selects found, fall back to all selects (for simple queries)\n",
    "    if not final_selects:\n",
    "        final_selects = selects\n",
    "    \n",
    "    all_columns = []\n",
    "    \n",
    "    for idx, select in enumerate(final_selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select, cte_registry)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        \n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            \n",
    "            # FIXED: Handle SELECT * expansion\n",
    "            if proj.is_star:\n",
    "                # For SELECT *, create dependencies on all source tables\n",
    "                for table_alias, (full_table, real_alias) in from_tables.items():\n",
    "                    resolved_sources.append((full_table, real_alias, \"*\"))\n",
    "                col_type = \"star\"\n",
    "            else:\n",
    "                for tbl_alias, col_name in source_columns:\n",
    "                    if not tbl_alias and only_table:\n",
    "                        # No table alias and only one table - use that table\n",
    "                        resolved_sources.append((only_table, col_name))\n",
    "                    elif tbl_alias:\n",
    "                        # Has table alias - look it up in from_tables\n",
    "                        tbl_alias_lc = tbl_alias.lower()\n",
    "                        if tbl_alias_lc in from_tables:\n",
    "                            full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                            resolved_sources.append((full_table, real_alias, col_name))\n",
    "                        else:\n",
    "                            # Alias not found in from_tables - keep as is\n",
    "                            resolved_sources.append((tbl_alias, col_name))\n",
    "                    else:\n",
    "                        # No table alias and multiple tables - ambiguous\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                \n",
    "                # Determine column type\n",
    "                if isinstance(proj, exp.Column):\n",
    "                    col_type = \"direct\"\n",
    "                elif not source_columns:\n",
    "                    col_type = \"constant\"\n",
    "                else:\n",
    "                    col_type = \"calculated\"\n",
    "            \n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        \n",
    "        all_columns.append(select_columns)\n",
    "    \n",
    "    return all_columns\n",
    "\n",
    "\n",
    "def trace_column_lineage(sql_query, target_column_name, existing_cte_registry=None):\n",
    "    \"\"\"\n",
    "    Traces a specific column through all transformations and builds LLM-ready context.\n",
    "    FIXED: Properly handles aliases, single names, and recursive CTE resolution\n",
    "    \"\"\"\n",
    "    \n",
    "    def should_stop_tracing(full_table_name, internal_prefixes=['ph_'], cte_registry=None):\n",
    "        \"\"\"Determine if we should STOP tracing (found external source)\"\"\"\n",
    "        \n",
    "        # Always continue for CTE references - they need recursive resolution\n",
    "        if full_table_name.startswith(\"CTE:\"):\n",
    "            return False, \"cte_reference\"\n",
    "        \n",
    "        # Check if this is actually a CTE name (without CTE: prefix)\n",
    "        if cte_registry and full_table_name.lower() in cte_registry:\n",
    "            return False, \"is_cte_name\"\n",
    "        \n",
    "        # Parse the table name\n",
    "        parts = full_table_name.split('.')\n",
    "        \n",
    "        if len(parts) >= 3:\n",
    "            # Full qualified name: database.schema.table\n",
    "            database = parts[0]\n",
    "            database_lower = database.lower()\n",
    "            starts_with_internal = any(database_lower.startswith(prefix.lower()) for prefix in internal_prefixes)\n",
    "            \n",
    "            if not starts_with_internal:\n",
    "                return True, \"external_database\"  # STOP - external database\n",
    "            else:\n",
    "                return False, \"internal_database\"  # CONTINUE - internal database\n",
    "                \n",
    "        elif len(parts) == 2:\n",
    "            # schema.table - check if schema indicates external\n",
    "            schema = parts[0].lower()\n",
    "            if any(schema.startswith(prefix.lower()) for prefix in internal_prefixes):\n",
    "                return False, \"internal_schema\"  # CONTINUE - internal schema\n",
    "            else:\n",
    "                return True, \"external_schema\"  # STOP - external schema\n",
    "            \n",
    "        else:\n",
    "            # Single name - check if it's a CTE first, then treat as external\n",
    "            if cte_registry and full_table_name.lower() in cte_registry:\n",
    "                return False, \"single_name_is_cte\"  # CONTINUE - it's a CTE\n",
    "            else:\n",
    "                return True, \"external_source_table\"  # STOP - treat as external\n",
    "    \n",
    "    # Parse and build CTE registry (or use existing one for nested calls)\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    cte_registry = existing_cte_registry or {}\n",
    "    with_clause = parsed.args.get(\"with\")\n",
    "    \n",
    "    # If no existing registry, build it from this query\n",
    "    if not existing_cte_registry and with_clause:\n",
    "        for cte in with_clause.expressions:\n",
    "            cte_name = cte.alias\n",
    "            cte_query = cte.this\n",
    "            cte_registry[cte_name.lower()] = cte_query\n",
    "    \n",
    "    # Get basic column analysis (pass CTE registry for nested CTE detection)\n",
    "    base_columns = extract_snowflake_columns(sql_query, cte_registry)\n",
    "    \n",
    "    # Find the target column in the final output\n",
    "    target_column_info = None\n",
    "    target_select_branch = None\n",
    "    \n",
    "    for select_idx, select_columns in enumerate(base_columns):\n",
    "        for col_info in select_columns:\n",
    "            if col_info['target_column'].lower() == target_column_name.lower():\n",
    "                target_column_info = col_info\n",
    "                target_select_branch = select_idx + 1\n",
    "                break\n",
    "            # Also check if target column could come from SELECT *\n",
    "            elif col_info['type'] == 'star':\n",
    "                # For star selections, assume any requested column could be available\n",
    "                target_column_info = {\n",
    "                    'target_column': target_column_name,\n",
    "                    'expression': f\"* (includes {target_column_name})\",\n",
    "                    'type': 'star',\n",
    "                    'resolved_source_columns': col_info['resolved_source_columns'],\n",
    "                    'select_idx': select_idx\n",
    "                }\n",
    "                target_select_branch = select_idx + 1\n",
    "                break\n",
    "        if target_column_info:\n",
    "            break\n",
    "    \n",
    "    if not target_column_info:\n",
    "        return {\n",
    "            \"error\": f\"Column '{target_column_name}' not found in query output\",\n",
    "            \"llm_context\": f\"The column '{target_column_name}' was not found in the final query output.\",\n",
    "            \"next_columns_to_search\": [],\n",
    "            \"full_lineage\": {}\n",
    "        }\n",
    "    \n",
    "    # Build LLM context\n",
    "    llm_context_parts = []\n",
    "    next_columns = []\n",
    "    cte_transformations = []\n",
    "    \n",
    "    # Basic column information\n",
    "    llm_context_parts.append(f\"COLUMN: {target_column_name}\")\n",
    "    llm_context_parts.append(f\"EXPRESSION: {target_column_info['expression']}\")\n",
    "    llm_context_parts.append(f\"TRANSFORMATION TYPE: {target_column_info['type']}\")\n",
    "    \n",
    "    if target_select_branch:\n",
    "        llm_context_parts.append(f\"FOUND IN: SELECT branch {target_select_branch}\")\n",
    "    \n",
    "    # Process resolved sources and trace through CTEs\n",
    "    resolved_sources = target_column_info.get('resolved_source_columns', [])\n",
    "    \n",
    "    if resolved_sources:\n",
    "        llm_context_parts.append(\"\\nSOURCE ANALYSIS:\")\n",
    "        \n",
    "        # Group resolved sources by table to detect multiple dependencies from same table/CTE\n",
    "        sources_by_table = {}\n",
    "        for source in resolved_sources:\n",
    "            if len(source) >= 3:  # (table, alias, column)\n",
    "                table, alias, column = source[:3]\n",
    "                table_key = table\n",
    "                # For star selections, use the target column name instead of \"*\"\n",
    "                if column == \"*\":\n",
    "                    column = target_column_name\n",
    "            elif len(source) == 2:  # (table, column)\n",
    "                table, column = source\n",
    "                alias = table\n",
    "                table_key = table\n",
    "                # For star selections, use the target column name\n",
    "                if column == \"*\":\n",
    "                    column = target_column_name\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if table_key not in sources_by_table:\n",
    "                sources_by_table[table_key] = []\n",
    "            sources_by_table[table_key].append((table, alias, column))\n",
    "        \n",
    "        # Process each table's dependencies\n",
    "        for table_key, table_sources in sources_by_table.items():\n",
    "            if len(table_sources) == 1:\n",
    "                # Single dependency from this table\n",
    "                table, alias, column = table_sources[0]\n",
    "                \n",
    "                # Check if this is a CTE reference\n",
    "                if table.startswith(\"CTE:\"):\n",
    "                    cte_name = table.replace(\"CTE:\", \"\")\n",
    "                    llm_context_parts.append(f\"  └─ CTE REFERENCE: {alias}.{column} → {cte_name}.{column}\")\n",
    "                    \n",
    "                    # Trace through the CTE\n",
    "                    if cte_name.lower() in cte_registry:\n",
    "                        cte_query = cte_registry[cte_name.lower()]\n",
    "                        cte_sql = cte_query.sql(dialect=\"snowflake\")\n",
    "                        llm_context_parts.append(f\"  └─ TRACING CTE '{cte_name}' (INTRA-FILE TRANSFORMATION):\")\n",
    "                        \n",
    "                        # FIXED: Recursively analyze the CTE with the current CTE registry\n",
    "                        # This ensures nested CTE references are properly resolved\n",
    "                        cte_trace = trace_column_lineage(cte_sql, column, cte_registry)\n",
    "                        if \"error\" not in cte_trace:\n",
    "                            # Add CTE transformation info\n",
    "                            cte_transformations.append({\n",
    "                                \"cte_name\": cte_name,\n",
    "                                \"column\": column,\n",
    "                                \"transformation_type\": \"intra_file_cte\",\n",
    "                                \"details\": cte_trace.get(\"llm_context\", \"\"),\n",
    "                                \"dependencies\": cte_trace.get(\"next_columns_to_search\", [])\n",
    "                            })\n",
    "                            \n",
    "                            # Add CTE's dependencies to our next_columns\n",
    "                            for cte_dep in cte_trace.get(\"next_columns_to_search\", []):\n",
    "                                next_columns.append({\n",
    "                                    \"table\": cte_dep[\"table\"],\n",
    "                                    \"column\": cte_dep[\"column\"],\n",
    "                                    \"context\": f\"External source for {target_column_name} via CTE {cte_name}\",\n",
    "                                    \"level\": \"external_via_cte\",\n",
    "                                    \"cte_intermediate\": cte_name\n",
    "                                })\n",
    "                            \n",
    "                            # Add CTE context to LLM output\n",
    "                            cte_context_lines = cte_trace[\"llm_context\"].split('\\n')\n",
    "                            for line in cte_context_lines:\n",
    "                                if line.strip():\n",
    "                                    llm_context_parts.append(f\"    {line}\")\n",
    "                        else:\n",
    "                            llm_context_parts.append(f\"    ERROR tracing CTE: {cte_trace['error']}\")\n",
    "                    else:\n",
    "                        llm_context_parts.append(f\"    WARNING: CTE '{cte_name}' not found in registry\")\n",
    "                        \n",
    "                else:\n",
    "                    # Regular table reference - check if it's actually a CTE first\n",
    "                    should_stop, reason = should_stop_tracing(table, cte_registry=cte_registry)\n",
    "                    \n",
    "                    if not should_stop and reason in [\"is_cte_name\", \"single_name_is_cte\"]:\n",
    "                        # This is actually a CTE that we need to trace through\n",
    "                        cte_name = table\n",
    "                        llm_context_parts.append(f\"  └─ NESTED CTE REFERENCE: {alias}.{column} → {cte_name}.{column}\")\n",
    "                        \n",
    "                        if cte_name.lower() in cte_registry:\n",
    "                            cte_query = cte_registry[cte_name.lower()]\n",
    "                            cte_sql = cte_query.sql(dialect=\"snowflake\")\n",
    "                            llm_context_parts.append(f\"  └─ TRACING NESTED CTE '{cte_name}' (INTRA-FILE TRANSFORMATION):\")\n",
    "                            \n",
    "                            # Recursively analyze the nested CTE\n",
    "                            cte_trace = trace_column_lineage(cte_sql, column, cte_registry)\n",
    "                            if \"error\" not in cte_trace:\n",
    "                                # Add CTE transformation info\n",
    "                                cte_transformations.append({\n",
    "                                    \"cte_name\": cte_name,\n",
    "                                    \"column\": column,\n",
    "                                    \"transformation_type\": \"nested_cte\",\n",
    "                                    \"details\": cte_trace.get(\"llm_context\", \"\"),\n",
    "                                    \"dependencies\": cte_trace.get(\"next_columns_to_search\", [])\n",
    "                                })\n",
    "                                \n",
    "                                # Add CTE's dependencies to our next_columns\n",
    "                                for cte_dep in cte_trace.get(\"next_columns_to_search\", []):\n",
    "                                    next_columns.append({\n",
    "                                        \"table\": cte_dep[\"table\"],\n",
    "                                        \"column\": cte_dep[\"column\"],\n",
    "                                        \"context\": f\"External source for {target_column_name} via nested CTE {cte_name}\",\n",
    "                                        \"level\": \"external_via_cte\",\n",
    "                                        \"cte_intermediate\": cte_name\n",
    "                                    })\n",
    "                                \n",
    "                                # Add CTE context to LLM output\n",
    "                                cte_context_lines = cte_trace[\"llm_context\"].split('\\n')\n",
    "                                for line in cte_context_lines:\n",
    "                                    if line.strip():\n",
    "                                        llm_context_parts.append(f\"    {line}\")\n",
    "                            else:\n",
    "                                llm_context_parts.append(f\"    ERROR tracing nested CTE: {cte_trace['error']}\")\n",
    "                        else:\n",
    "                            llm_context_parts.append(f\"    WARNING: Nested CTE '{cte_name}' not found in registry\")\n",
    "                    \n",
    "                    elif should_stop:\n",
    "                        # External table - add to dependencies\n",
    "                        llm_context_parts.append(f\"  └─ EXTERNAL TABLE: {table}.{column} (referenced as {alias}.{column}) - {reason}\")\n",
    "                        next_columns.append({\n",
    "                            \"table\": table,\n",
    "                            \"column\": column,\n",
    "                            \"context\": f\"External table dependency for {target_column_name}\",\n",
    "                            \"level\": \"external_table\",\n",
    "                            \"source_reason\": reason\n",
    "                        })\n",
    "                    else:\n",
    "                        # Internal table - would need further tracing in full system\n",
    "                        llm_context_parts.append(f\"  └─ INTERNAL TABLE: {table}.{column} (referenced as {alias}.{column}) - {reason}\")\n",
    "                        # In test mode, treat internal tables as external to show the dependency\n",
    "                        next_columns.append({\n",
    "                            \"table\": table,\n",
    "                            \"column\": column,\n",
    "                            \"context\": f\"Internal table dependency for {target_column_name}\",\n",
    "                            \"level\": \"internal_table\",\n",
    "                            \"trace_reason\": reason\n",
    "                        })\n",
    "            \n",
    "            else:\n",
    "                # Multiple dependencies from same table - group them\n",
    "                table_name = table_key.replace(\"CTE:\", \"\") if table_key.startswith(\"CTE:\") else table_key\n",
    "                columns = [col for _, _, col in table_sources]\n",
    "                \n",
    "                llm_context_parts.append(f\"  └─ MULTIPLE DEPENDENCIES FROM {table_name}:\")\n",
    "                for table, alias, column in table_sources:\n",
    "                    llm_context_parts.append(f\"    • {alias}.{column}\")\n",
    "                \n",
    "                if table_key.startswith(\"CTE:\"):\n",
    "                    # Multiple CTE dependencies - consolidate them\n",
    "                    cte_name = table_key.replace(\"CTE:\", \"\")\n",
    "                    llm_context_parts.append(f\"  └─ CONSOLIDATED CTE ANALYSIS for '{cte_name}':\")\n",
    "                    \n",
    "                    if cte_name.lower() in cte_registry:\n",
    "                        cte_query = cte_registry[cte_name.lower()]\n",
    "                        cte_sql = cte_query.sql(dialect=\"snowflake\")\n",
    "                        \n",
    "                        # Get all unique external dependencies from this CTE\n",
    "                        all_cte_deps = set()\n",
    "                        cte_column_details = []\n",
    "                        \n",
    "                        for table, alias, column in table_sources:\n",
    "                            # FIXED: Pass CTE registry to nested calls\n",
    "                            cte_trace = trace_column_lineage(cte_sql, column, cte_registry)\n",
    "                            if \"error\" not in cte_trace:\n",
    "                                cte_column_details.append({\n",
    "                                    \"column\": column,\n",
    "                                    \"trace\": cte_trace\n",
    "                                })\n",
    "                                \n",
    "                                # Collect external dependencies\n",
    "                                for cte_dep in cte_trace.get(\"next_columns_to_search\", []):\n",
    "                                    dep_key = (cte_dep[\"table\"], cte_dep[\"column\"])\n",
    "                                    all_cte_deps.add(dep_key)\n",
    "                        \n",
    "                        # Add consolidated CTE transformation\n",
    "                        cte_transformations.append({\n",
    "                            \"cte_name\": cte_name,\n",
    "                            \"columns\": columns,\n",
    "                            \"transformation_type\": \"consolidated_cte\",\n",
    "                            \"details\": f\"CTE processes {len(columns)} columns: {', '.join(columns)}\",\n",
    "                            \"column_details\": cte_column_details\n",
    "                        })\n",
    "                        \n",
    "                        # Add unique external dependencies\n",
    "                        for dep_table, dep_column in all_cte_deps:\n",
    "                            next_columns.append({\n",
    "                                \"table\": dep_table,\n",
    "                                \"column\": dep_column,\n",
    "                                \"context\": f\"External source for {target_column_name} via consolidated CTE {cte_name}\",\n",
    "                                \"level\": \"external_via_cte\",\n",
    "                                \"cte_intermediate\": cte_name\n",
    "                            })\n",
    "                        \n",
    "                        # Show consolidated CTE analysis\n",
    "                        llm_context_parts.append(f\"    └─ CTE '{cte_name}' processes {len(columns)} output columns\")\n",
    "                        llm_context_parts.append(f\"    └─ External dependencies: {len(all_cte_deps)} unique sources\")\n",
    "                        \n",
    "                else:\n",
    "                    # Multiple dependencies from regular table\n",
    "                    for table, alias, column in table_sources:\n",
    "                        should_stop, reason = should_stop_tracing(table, cte_registry=cte_registry)\n",
    "                        \n",
    "                        if not should_stop and reason in [\"is_cte_name\", \"single_name_is_cte\"]:\n",
    "                            # This is actually a nested CTE reference\n",
    "                            cte_name = table\n",
    "                            if cte_name.lower() in cte_registry:\n",
    "                                cte_query = cte_registry[cte_name.lower()]\n",
    "                                cte_sql = cte_query.sql(dialect=\"snowflake\")\n",
    "                                \n",
    "                                # Trace through this nested CTE\n",
    "                                cte_trace = trace_column_lineage(cte_sql, column, cte_registry)\n",
    "                                if \"error\" not in cte_trace:\n",
    "                                    # Add nested CTE's dependencies\n",
    "                                    for cte_dep in cte_trace.get(\"next_columns_to_search\", []):\n",
    "                                        next_columns.append({\n",
    "                                            \"table\": cte_dep[\"table\"],\n",
    "                                            \"column\": cte_dep[\"column\"],\n",
    "                                            \"context\": f\"External source for {target_column_name} via nested CTE {cte_name}\",\n",
    "                                            \"level\": \"external_via_cte\",\n",
    "                                            \"cte_intermediate\": cte_name\n",
    "                                        })\n",
    "                        \n",
    "                        elif should_stop:\n",
    "                            next_columns.append({\n",
    "                                \"table\": table,\n",
    "                                \"column\": column,\n",
    "                                \"context\": f\"External table dependency for {target_column_name}\",\n",
    "                                \"level\": \"external_table\",\n",
    "                                \"source_reason\": reason\n",
    "                            })\n",
    "                        else:\n",
    "                            next_columns.append({\n",
    "                                \"table\": table,\n",
    "                                \"column\": column,\n",
    "                                \"context\": f\"Internal table dependency for {target_column_name}\",\n",
    "                                \"level\": \"internal_table\",\n",
    "                                \"trace_reason\": reason\n",
    "                            })\n",
    "    \n",
    "    # Show CTE transformations summary\n",
    "    if cte_transformations:\n",
    "        llm_context_parts.append(f\"\\nINTRA-FILE CTE TRANSFORMATIONS:\")\n",
    "        for cte_info in cte_transformations:\n",
    "            # Handle both 'column' and 'columns' keys safely\n",
    "            if 'column' in cte_info:\n",
    "                column_info = cte_info['column']\n",
    "            elif 'columns' in cte_info:\n",
    "                columns_list = cte_info['columns']\n",
    "                if isinstance(columns_list, list):\n",
    "                    column_info = ', '.join(columns_list)\n",
    "                else:\n",
    "                    column_info = str(columns_list)\n",
    "            else:\n",
    "                column_info = 'unknown'\n",
    "            \n",
    "            llm_context_parts.append(f\"  CTE '{cte_info['cte_name']}' transforms {column_info}\")\n",
    "            llm_context_parts.append(f\"    └─ Type: {cte_info.get('transformation_type', 'unknown')}\")\n",
    "    \n",
    "    # Show CTE definitions if relevant\n",
    "    if cte_registry:\n",
    "        llm_context_parts.append(f\"\\nAVAILABLE CTEs IN THIS FILE:\")\n",
    "        for cte_name in cte_registry.keys():\n",
    "            llm_context_parts.append(f\"  - {cte_name}\")\n",
    "    \n",
    "    # Remove duplicates from next_columns\n",
    "    unique_next_columns = []\n",
    "    seen = set()\n",
    "    for col in next_columns:\n",
    "        key = (col['table'], col['column'], col.get('level', 'unknown'))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_next_columns.append(col)\n",
    "    \n",
    "    return {\n",
    "        \"llm_context\": \"\\n\".join(llm_context_parts),\n",
    "        \"next_columns_to_search\": unique_next_columns,\n",
    "        \"cte_transformations\": cte_transformations,\n",
    "        \"full_lineage\": target_column_info\n",
    "    }\n",
    "\n",
    "\n",
    "# Test functions for Jupyter\n",
    "def test_nested_cte_fix():\n",
    "    \"\"\"Test that nested CTEs are properly resolved to external sources\"\"\"\n",
    "    \n",
    "    # Test case: enriched_customers -> base_customers -> customers\n",
    "    sql = \"\"\"\n",
    "    WITH base_customers AS (\n",
    "        SELECT customer_id, customer_name\n",
    "        FROM customers\n",
    "    ),\n",
    "    enriched_customers AS (\n",
    "        SELECT customer_id, customer_name, 'active' as status\n",
    "        FROM base_customers  \n",
    "    )\n",
    "    SELECT customer_id, status\n",
    "    FROM enriched_customers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TESTING NESTED CTE RESOLUTION ===\")\n",
    "    print(\"Expected: customer_id should trace through both CTEs to 'customers' table\")\n",
    "    print()\n",
    "    \n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"ERROR: {result['error']}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if we found the ultimate external source\n",
    "    next_columns = result.get(\"next_columns_to_search\", [])\n",
    "    \n",
    "    print(\"Found dependencies:\")\n",
    "    for col in next_columns:\n",
    "        print(f\"  - {col['table']}.{col['column']} ({col['level']})\")\n",
    "    \n",
    "    # Success criteria: should find 'customers' table as external source\n",
    "    external_sources = [col for col in next_columns if col['table'] == 'customers']\n",
    "    \n",
    "    if external_sources:\n",
    "        print(\"✅ SUCCESS: Found 'customers' as external source!\")\n",
    "        print(f\"   Details: {external_sources[0]}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ FAILED: Should have found 'customers' table as external source\")\n",
    "        print(\"   Currently stops at intermediate CTE instead of tracing to ultimate source\")\n",
    "        \n",
    "        # Show debug info\n",
    "        print(\"\\nDEBUG - Full LLM Context:\")\n",
    "        print(result.get(\"llm_context\", \"No context\"))\n",
    "        return False\n",
    "\n",
    "def test_simple_cte_still_works():\n",
    "    \"\"\"Ensure simple CTE case still works after our changes\"\"\"\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    WITH customer_base AS (\n",
    "        SELECT customer_id, customer_name\n",
    "        FROM customers\n",
    "    )\n",
    "    SELECT customer_id\n",
    "    FROM customer_base\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== TESTING SIMPLE CTE (REGRESSION TEST) ===\")\n",
    "    \n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"ERROR: {result['error']}\")\n",
    "        return False\n",
    "    \n",
    "    next_columns = result.get(\"next_columns_to_search\", [])\n",
    "    external_sources = [col for col in next_columns if col['table'] == 'customers']\n",
    "    \n",
    "    if external_sources:\n",
    "        print(\"✅ SUCCESS: Simple CTE still works!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ FAILED: Simple CTE broken\")\n",
    "        return False\n",
    "\n",
    "# Quick test runner\n",
    "def run_nested_cte_tests():\n",
    "    print(\"Testing nested CTE resolution fix...\\n\")\n",
    "    \n",
    "    # Run tests\n",
    "    nested_ok = test_nested_cte_fix()\n",
    "    simple_ok = test_simple_cte_still_works()\n",
    "    \n",
    "    print(f\"\\n=== RESULTS ===\")\n",
    "    print(f\"Nested CTE resolution: {'✅ PASS' if nested_ok else '❌ FAIL'}\")\n",
    "    print(f\"Simple CTE regression: {'✅ PASS' if simple_ok else '❌ FAIL'}\")\n",
    "    \n",
    "    if nested_ok and simple_ok:\n",
    "        print(\"\\n🎉 All tests passed! Ready to integrate with main DBT tracer.\")\n",
    "    else:\n",
    "        print(\"\\n🔧 Still needs fixes before integration.\")\n",
    "    \n",
    "    return nested_ok and simple_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3094fc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nested CTE resolution fix...\n",
      "\n",
      "=== TESTING NESTED CTE RESOLUTION ===\n",
      "Expected: customer_id should trace through both CTEs to 'customers' table\n",
      "\n",
      "Found dependencies:\n",
      "  - customers.customer_id (external_via_cte)\n",
      "✅ SUCCESS: Found 'customers' as external source!\n",
      "   Details: {'table': 'customers', 'column': 'customer_id', 'context': 'External source for customer_id via CTE enriched_customers', 'level': 'external_via_cte', 'cte_intermediate': 'enriched_customers'}\n",
      "\n",
      "=== TESTING SIMPLE CTE (REGRESSION TEST) ===\n",
      "✅ SUCCESS: Simple CTE still works!\n",
      "\n",
      "=== RESULTS ===\n",
      "Nested CTE resolution: ✅ PASS\n",
      "Simple CTE regression: ✅ PASS\n",
      "\n",
      "🎉 All tests passed! Ready to integrate with main DBT tracer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the functions from the artifact above, then run:\n",
    "run_nested_cte_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5af2c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple lineage tests...\n",
      "\n",
      "=== TEST 7: Column Extraction ===\n",
      "Extracted columns: 1 select statements\n",
      "  Select 0: 2 columns\n",
      "    - customer_id: direct\n",
      "    - customer_name: direct\n",
      "\n",
      "=== TEST 1: Simple SELECT ===\n",
      "Result: SUCCESS\n",
      "Next columns: [{'table': 'customers', 'column': 'customer_id', 'context': 'External table dependency for customer_id', 'level': 'external_table', 'source_reason': 'external_source_table'}]\n",
      "\n",
      "=== TEST 2: SELECT * ===\n",
      "Result: SUCCESS\n",
      "Next columns: [{'table': 'customers', 'column': 'customer_id', 'context': 'External table dependency for customer_id', 'level': 'external_table', 'source_reason': 'external_source_table'}]\n",
      "\n",
      "=== TEST 3: Simple CTE ===\n",
      "Result: SUCCESS\n",
      "Next columns: [{'table': 'customers', 'column': 'customer_id', 'context': 'External source for customer_id via CTE customer_base', 'level': 'external_via_cte', 'cte_intermediate': 'customer_base'}]\n",
      "CTE transformations: 1\n",
      "\n",
      "=== TEST 4: CTE with SELECT * ===\n",
      "Result: SUCCESS\n",
      "Next columns: [{'table': 'customers', 'column': 'customer_id', 'context': 'External source for customer_id via CTE customer_base', 'level': 'external_via_cte', 'cte_intermediate': 'customer_base'}]\n",
      "\n",
      "=== TEST 5: Multiple CTEs ===\n",
      "Result: SUCCESS\n",
      "Next columns: [{'table': 'customers', 'column': 'customer_id', 'context': 'External source for customer_id via CTE enriched_customers', 'level': 'external_via_cte', 'cte_intermediate': 'enriched_customers'}]\n",
      "CTE transformations: 1\n",
      "\n",
      "=== TEST 6: DBT-style CTEs ===\n",
      "Result: SUCCESS\n",
      "Next columns: [{'table': 'schema_name.table_name1', 'column': 'col1', 'context': 'External source for col1 via CTE cte_main', 'level': 'external_via_cte', 'cte_intermediate': 'cte_main'}]\n",
      "\n",
      "Tests complete!\n"
     ]
    }
   ],
   "source": [
    "def test_simple_select():\n",
    "    \"\"\"Test 1: Basic SELECT\"\"\"\n",
    "    sql = \"\"\"\n",
    "    SELECT customer_id, customer_name\n",
    "    FROM customers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 1: Simple SELECT ===\")\n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    print(\"Result:\", result.get(\"error\", \"SUCCESS\"))\n",
    "    print(\"Next columns:\", result.get(\"next_columns_to_search\", []))\n",
    "    print()\n",
    "\n",
    "def test_select_star():\n",
    "    \"\"\"Test 2: SELECT * \"\"\"\n",
    "    sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM customers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 2: SELECT * ===\")\n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    print(\"Result:\", result.get(\"error\", \"SUCCESS\"))\n",
    "    print(\"Next columns:\", result.get(\"next_columns_to_search\", []))\n",
    "    print()\n",
    "\n",
    "def test_simple_cte():\n",
    "    \"\"\"Test 3: Simple CTE\"\"\"\n",
    "    sql = \"\"\"\n",
    "    WITH customer_base AS (\n",
    "        SELECT customer_id, customer_name\n",
    "        FROM customers\n",
    "    )\n",
    "    SELECT customer_id\n",
    "    FROM customer_base\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 3: Simple CTE ===\")\n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    print(\"Result:\", result.get(\"error\", \"SUCCESS\"))\n",
    "    print(\"Next columns:\", result.get(\"next_columns_to_search\", []))\n",
    "    print(\"CTE transformations:\", len(result.get(\"cte_transformations\", [])))\n",
    "    print()\n",
    "\n",
    "def test_cte_with_star():\n",
    "    \"\"\"Test 4: CTE with SELECT *\"\"\"\n",
    "    sql = \"\"\"\n",
    "    WITH customer_base AS (\n",
    "        SELECT *\n",
    "        FROM customers\n",
    "    )\n",
    "    SELECT customer_id\n",
    "    FROM customer_base\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 4: CTE with SELECT * ===\")\n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    print(\"Result:\", result.get(\"error\", \"SUCCESS\"))\n",
    "    print(\"Next columns:\", result.get(\"next_columns_to_search\", []))\n",
    "    print()\n",
    "\n",
    "def test_multiple_ctes():\n",
    "    \"\"\"Test 5: Multiple CTEs\"\"\"\n",
    "    sql = \"\"\"\n",
    "    WITH base_customers AS (\n",
    "        SELECT customer_id, customer_name\n",
    "        FROM customers\n",
    "    ),\n",
    "    enriched_customers AS (\n",
    "        SELECT customer_id, customer_name, 'active' as status\n",
    "        FROM base_customers\n",
    "    )\n",
    "    SELECT customer_id, status\n",
    "    FROM enriched_customers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 5: Multiple CTEs ===\")\n",
    "    result = trace_column_lineage(sql, \"customer_id\")\n",
    "    print(\"Result:\", result.get(\"error\", \"SUCCESS\"))\n",
    "    print(\"Next columns:\", result.get(\"next_columns_to_search\", []))\n",
    "    print(\"CTE transformations:\", len(result.get(\"cte_transformations\", [])))\n",
    "    print()\n",
    "\n",
    "def test_dbt_style_ctes():\n",
    "    \"\"\"Test 6: DBT-style CTEs (simplified version of your complex query)\"\"\"\n",
    "    sql = \"\"\"\n",
    "    WITH __dbt__cte__table1 AS (\n",
    "        SELECT col1, col2, col3\n",
    "        FROM schema_name.table_name1\n",
    "    ),\n",
    "    cte_main AS (\n",
    "        SELECT col1, col2\n",
    "        FROM __dbt__cte__table1\n",
    "    )\n",
    "    SELECT col1\n",
    "    FROM cte_main\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 6: DBT-style CTEs ===\")\n",
    "    result = trace_column_lineage(sql, \"col1\")\n",
    "    print(\"Result:\", result.get(\"error\", \"SUCCESS\"))\n",
    "    print(\"Next columns:\", result.get(\"next_columns_to_search\", []))\n",
    "    print()\n",
    "\n",
    "def test_extract_columns():\n",
    "    \"\"\"Test 7: Just test column extraction\"\"\"\n",
    "    sql = \"\"\"\n",
    "    SELECT customer_id, customer_name\n",
    "    FROM customers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TEST 7: Column Extraction ===\")\n",
    "    try:\n",
    "        columns = extract_snowflake_columns(sql)\n",
    "        print(\"Extracted columns:\", len(columns), \"select statements\")\n",
    "        for i, select_cols in enumerate(columns):\n",
    "            print(f\"  Select {i}: {len(select_cols)} columns\")\n",
    "            for col in select_cols:\n",
    "                print(f\"    - {col['target_column']}: {col['type']}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running simple lineage tests...\\n\")\n",
    "    \n",
    "    # Run tests in order of complexity\n",
    "    test_extract_columns()\n",
    "    test_simple_select()\n",
    "    test_select_star()\n",
    "    test_simple_cte()\n",
    "    test_cte_with_star()\n",
    "    test_multiple_ctes()\n",
    "    test_dbt_style_ctes()\n",
    "    \n",
    "    print(\"Tests complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755632d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
