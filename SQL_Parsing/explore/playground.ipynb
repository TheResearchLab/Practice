{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f98cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlglot in c:\\users\\aaron\\documents\\repos\\practice\\practice_env\\lib\\site-packages (27.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlglot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62555486",
   "metadata": {},
   "source": [
    "# Evaluation steps\n",
    "\n",
    " \n",
    "\n",
    "### Model Eval\n",
    "   - Unions\n",
    "\n",
    "   - CTEs & Recursive CTEs\n",
    "\n",
    "   - Time travel syntax\n",
    "\n",
    "   - Sub-queries\n",
    "\n",
    " \n",
    "\n",
    "### Column Eval\n",
    "  - Aliases\n",
    "\n",
    "  - \"SELECT *\"\n",
    "\n",
    "  - Calculated/Multi-column fields\n",
    "\n",
    "  - Window Functions\n",
    "\n",
    "    - Qualified Column Refs\n",
    "\n",
    " \n",
    "\n",
    "### Other\n",
    "\n",
    "   - Masking salt key in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffc193",
   "metadata": {},
   "source": [
    "# Models Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bac002",
   "metadata": {},
   "source": [
    "## Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc0e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake tables found:\n",
      "  - ECOMMERCE_DB.SALES.ONLINE_ORDERS\n",
      "  - ECOMMERCE_DB.SALES.RETAIL_SALES\n",
      "  - MOBILE_APP_DB.TRANSACTIONS.MOBILE_TRANSACTIONS\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "\"\"\"\n",
    "Can I assume all snowflake compiled models will be formatted as db.schema.tbl? I think?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example UNION query in Snowflake syntax (no quotes)\n",
    "union_query = \"\"\"\n",
    "SELECT CUSTOMER_ID, ORDER_DATE, 'online' AS CHANNEL\n",
    "FROM ECOMMERCE_DB.SALES.ONLINE_ORDERS\n",
    "WHERE ORDER_DATE >= '2024-01-01'\n",
    "UNION ALL\n",
    "SELECT CUSTOMER_ID, PURCHASE_DATE AS ORDER_DATE, 'retail' AS CHANNEL\n",
    "FROM ECOMMERCE_DB.SALES.RETAIL_SALES\n",
    "WHERE PURCHASE_DATE >= '2024-01-01'\n",
    "UNION\n",
    "SELECT CUST_ID AS CUSTOMER_ID, TRANSACTION_DATE AS ORDER_DATE, 'mobile' AS CHANNEL\n",
    "FROM MOBILE_APP_DB.TRANSACTIONS.MOBILE_TRANSACTIONS\n",
    "WHERE TRANSACTION_DATE >= '2024-01-01'\n",
    "\"\"\"\n",
    "\n",
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    tables = set()\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            # Build full table name: DATABASE.SCHEMA.TABLE (no quotes)\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            tables.add(full_name)\n",
    "    return sorted(tables)\n",
    "\n",
    "# Test extraction\n",
    "tables = extract_snowflake_tables(union_query)\n",
    "print(\"Snowflake tables found:\")\n",
    "for t in tables:\n",
    "    print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587a46f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  - db1.schema1.tableA\n",
      "  - db2.schema2.tableB\n",
      "\n",
      "Test case 2:\n",
      "  - analytics.inactive_users\n",
      "  - analytics.orders\n",
      "  - analytics.users\n",
      "\n",
      "Test case 3:\n",
      "  - marketing.leads\n",
      "  - recent_orders\n",
      "  - sales.customers\n",
      "  - sales.orders\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    # 1. Simple UNION with single tables\n",
    "    \"\"\"\n",
    "    SELECT id FROM db1.schema1.tableA\n",
    "    UNION\n",
    "    SELECT id FROM db2.schema2.tableB\n",
    "    \"\"\",\n",
    "\n",
    "    # 2. UNION ALL with JOIN and subquery\n",
    "    \"\"\"\n",
    "    SELECT u.user_id, o.order_id\n",
    "    FROM analytics.users u\n",
    "    JOIN analytics.orders o ON u.user_id = o.user_id\n",
    "    UNION ALL\n",
    "    SELECT user_id, NULL\n",
    "    FROM analytics.inactive_users\n",
    "    WHERE last_login < '2024-01-01'\n",
    "    \"\"\",\n",
    "\n",
    "    # 3. UNION with nested SELECT and CTE\n",
    "    \"\"\"\n",
    "    WITH recent_orders AS (\n",
    "        SELECT order_id, customer_id\n",
    "        FROM sales.orders\n",
    "        WHERE order_date > '2025-01-01'\n",
    "    )\n",
    "    SELECT customer_id FROM recent_orders\n",
    "    UNION\n",
    "    SELECT customer_id FROM sales.customers\n",
    "    WHERE signup_date > '2025-01-01'\n",
    "    UNION ALL\n",
    "    SELECT customer_id FROM marketing.leads\n",
    "    WHERE source = 'web'\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    tables = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dfe4d",
   "metadata": {},
   "source": [
    "## CTEs and Recursive CTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d70fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  - db1.schema1.tableA\n",
      "  - db2.schema2.tableB\n",
      "\n",
      "Test case 2:\n",
      "  - analytics.inactive_users\n",
      "  - analytics.orders\n",
      "  - analytics.users\n",
      "\n",
      "Test case 3:\n",
      "  - marketing.leads\n",
      "  - recent_orders\n",
      "  - sales.customers\n",
      "  - sales.orders\n",
      "\n",
      "Test case 4:\n",
      "  - active_customers\n",
      "  - crm_db.marketing.leads\n",
      "  - crm_db.sales.customers\n",
      "  - crm_db.sales.orders\n",
      "  - crm_db.sales.products\n",
      "  - crm_db.sales.returns\n",
      "  - recent_orders\n"
     ]
    }
   ],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH active_customers AS (\n",
    "        SELECT customer_id\n",
    "        FROM crm_db.sales.customers\n",
    "        WHERE status = 'active'\n",
    "    ),\n",
    "    recent_orders AS (\n",
    "        SELECT order_id, customer_id\n",
    "        FROM crm_db.sales.orders\n",
    "        WHERE order_date > '2025-01-01'\n",
    "    ),\n",
    "    top_products AS (\n",
    "        SELECT product_id\n",
    "        FROM crm_db.sales.products\n",
    "        WHERE rating > 4.5\n",
    "    )\n",
    "    SELECT ac.customer_id, ro.order_id\n",
    "    FROM active_customers ac\n",
    "    JOIN recent_orders ro ON ac.customer_id = ro.customer_id\n",
    "    UNION\n",
    "    SELECT customer_id, NULL\n",
    "    FROM crm_db.marketing.leads\n",
    "    WHERE source = 'web'\n",
    "    UNION ALL\n",
    "    SELECT NULL, order_id\n",
    "    FROM recent_orders\n",
    "    WHERE order_id NOT IN (SELECT order_id FROM crm_db.sales.returns)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    tables = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17aafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  - db1.schema1.tableA\n",
      "  - db2.schema2.tableB\n",
      "\n",
      "Test case 2:\n",
      "  - analytics.inactive_users\n",
      "  - analytics.orders\n",
      "  - analytics.users\n",
      "\n",
      "Test case 3:\n",
      "  - marketing.leads\n",
      "  - recent_orders\n",
      "  - sales.customers\n",
      "  - sales.orders\n",
      "\n",
      "Test case 4:\n",
      "  - active_customers\n",
      "  - crm_db.marketing.leads\n",
      "  - crm_db.sales.customers\n",
      "  - crm_db.sales.orders\n",
      "  - crm_db.sales.products\n",
      "  - crm_db.sales.returns\n",
      "  - recent_orders\n",
      "\n",
      "Test case 5:\n",
      "  - __dbt__cte__dummy_data\n",
      "  - dummy_schema.dummy_dim\n",
      "  - dummy_schema.dummy_table\n",
      "  - get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        dd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN dummy_schema.dummy_dim dd ON gd.dummy_ver_name = dd.dummy_ver_name\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    tables = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53c31d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  Physical tables:\n",
      "    - db1.schema1.tableA\n",
      "    - db2.schema2.tableB\n",
      "  CTE names:\n",
      "\n",
      "Test case 2:\n",
      "  Physical tables:\n",
      "    - analytics.inactive_users\n",
      "    - analytics.orders\n",
      "    - analytics.users\n",
      "  CTE names:\n",
      "\n",
      "Test case 3:\n",
      "  Physical tables:\n",
      "    - marketing.leads\n",
      "    - sales.customers\n",
      "    - sales.orders\n",
      "  CTE names:\n",
      "    - recent_orders\n",
      "\n",
      "Test case 4:\n",
      "  Physical tables:\n",
      "    - crm_db.marketing.leads\n",
      "    - crm_db.sales.customers\n",
      "    - crm_db.sales.orders\n",
      "    - crm_db.sales.products\n",
      "    - crm_db.sales.returns\n",
      "  CTE names:\n",
      "    - active_customers\n",
      "    - recent_orders\n",
      "    - top_products\n",
      "\n",
      "Test case 5:\n",
      "  Physical tables:\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    tables = set()\n",
    "    cte_names = set()\n",
    "\n",
    "    # Collect CTE names\n",
    "    for node in parsed.find_all(exp.CTE):\n",
    "        if node.alias:\n",
    "            cte_names.add(node.alias)\n",
    "\n",
    "    # Collect all table references\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            tables.add(full_name)\n",
    "\n",
    "    # Separate physical tables from CTEs\n",
    "    physical_tables = [t for t in tables if t not in cte_names]\n",
    "    return sorted(physical_tables), sorted(cte_names)\n",
    "\n",
    "\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    physical_tables, cte_names = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    print(\"  Physical tables:\")\n",
    "    for t in physical_tables:\n",
    "        print(f\"    - {t}\")\n",
    "    print(\"  CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"    - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06db9a4",
   "metadata": {},
   "source": [
    "## Timestamp Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22cb355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  Physical tables:\n",
      "    - db1.schema1.tableA\n",
      "    - db2.schema2.tableB\n",
      "  CTE names:\n",
      "\n",
      "Test case 2:\n",
      "  Physical tables:\n",
      "    - analytics.inactive_users\n",
      "    - analytics.orders\n",
      "    - analytics.users\n",
      "  CTE names:\n",
      "\n",
      "Test case 3:\n",
      "  Physical tables:\n",
      "    - marketing.leads\n",
      "    - sales.customers\n",
      "    - sales.orders\n",
      "  CTE names:\n",
      "    - recent_orders\n",
      "\n",
      "Test case 4:\n",
      "  Physical tables:\n",
      "    - crm_db.marketing.leads\n",
      "    - crm_db.sales.customers\n",
      "    - crm_db.sales.orders\n",
      "    - crm_db.sales.products\n",
      "    - crm_db.sales.returns\n",
      "  CTE names:\n",
      "    - active_customers\n",
      "    - recent_orders\n",
      "    - top_products\n",
      "\n",
      "Test case 5:\n",
      "  Physical tables:\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "\n",
      "Test case 6:\n",
      "  Physical tables:\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        dd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN dummy_schema.dummy_dim dd ON gd.dummy_ver_name = dd.dummy_ver_name\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    physical_tables, cte_names = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    print(\"  Physical tables:\")\n",
    "    for t in physical_tables:\n",
    "        print(f\"    - {t}\")\n",
    "    print(\"  CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"    - {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2f25c",
   "metadata": {},
   "source": [
    "## Derived/Sub-query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95601eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries.append(\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        gd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef,\n",
    "        sub.latest_status\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            MAX(status_date) AS latest_status\n",
    "        FROM dummy_schema.dummy_status\n",
    "        WHERE status_code IN (\n",
    "            SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
    "        )\n",
    "        GROUP BY dummy_ver_name\n",
    "    ) sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
    "    WHERE gd.dummy_coef > (\n",
    "        SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5530a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case 1:\n",
      "  Source/target tables (all physical tables):\n",
      "    - db1.schema1.tableA\n",
      "    - db2.schema2.tableB\n",
      "  CTE names:\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "\n",
      "Test case 2:\n",
      "  Source/target tables (all physical tables):\n",
      "    - analytics.inactive_users\n",
      "    - analytics.orders\n",
      "    - analytics.users\n",
      "  CTE names:\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    u -> analytics.users\n",
      "    o -> analytics.orders\n",
      "\n",
      "Test case 3:\n",
      "  Source/target tables (all physical tables):\n",
      "    - marketing.leads\n",
      "    - sales.customers\n",
      "    - sales.orders\n",
      "  CTE names:\n",
      "    - recent_orders\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "\n",
      "Test case 4:\n",
      "  Source/target tables (all physical tables):\n",
      "    - crm_db.marketing.leads\n",
      "    - crm_db.sales.customers\n",
      "    - crm_db.sales.orders\n",
      "    - crm_db.sales.products\n",
      "    - crm_db.sales.returns\n",
      "  CTE names:\n",
      "    - active_customers\n",
      "    - recent_orders\n",
      "    - top_products\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    ac -> active_customers\n",
      "    ro -> recent_orders\n",
      "\n",
      "Test case 5:\n",
      "  Source/target tables (all physical tables):\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    gd -> get_dummy_data\n",
      "    dd -> dummy_schema.dummy_dim\n",
      "\n",
      "Test case 6:\n",
      "  Source/target tables (all physical tables):\n",
      "    - dummy_schema.dummy_dim\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "  Table aliases:\n",
      "    gd -> get_dummy_data\n",
      "    dd -> dummy_schema.dummy_dim\n",
      "\n",
      "Test case 7:\n",
      "  Source/target tables (all physical tables):\n",
      "    - dummy_schema.dummy_status\n",
      "    - dummy_schema.dummy_table\n",
      "  CTE names:\n",
      "    - __dbt__cte__dummy_data\n",
      "    - get_dummy_data\n",
      "  JOIN/derived subquery tables (valuable for lineage):\n",
      "    - dummy_schema.dummy_status\n",
      "  WHERE subquery tables (not useful for lineage):\n",
      "    - dummy_schema.dummy_table\n",
      "    - dummy_schema.status_codes\n",
      "  Table aliases:\n",
      "    gd -> get_dummy_data\n"
     ]
    }
   ],
   "source": [
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    cte_names = set()\n",
    "    table_aliases = dict()\n",
    "    all_physical_tables = set()\n",
    "    join_subquery_tables = set()\n",
    "    where_subquery_tables = set()\n",
    "    cte_source_tables = set()\n",
    "\n",
    "    # Collect CTE names and their source tables\n",
    "    for cte in parsed.find_all(exp.CTE):\n",
    "        if cte.alias:\n",
    "            cte_names.add(cte.alias)\n",
    "        # Find tables referenced inside CTE definitions\n",
    "        for node in cte.find_all(exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            if full_name not in cte_names:\n",
    "                cte_source_tables.add(full_name)\n",
    "\n",
    "    # Helper to get full table name\n",
    "    def get_full_name(node):\n",
    "        db = node.catalog or \"\"\n",
    "        schema = node.db or \"\"\n",
    "        name = node.name\n",
    "        if db and schema:\n",
    "            return f\"{db}.{schema}.{name}\"\n",
    "        elif schema:\n",
    "            return f\"{schema}.{name}\"\n",
    "        else:\n",
    "            return name\n",
    "\n",
    "    # Collect all physical tables (not CTEs) anywhere in the query\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            full_name = get_full_name(node)\n",
    "            if full_name not in cte_names:\n",
    "                all_physical_tables.add(full_name)\n",
    "            if node.alias:\n",
    "                table_aliases[node.alias] = full_name\n",
    "\n",
    "    # Collect tables in JOIN subqueries and derived tables\n",
    "    for join in parsed.find_all(exp.Join):\n",
    "        for subquery in join.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        join_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    # Collect tables in WHERE subqueries\n",
    "    for where in parsed.find_all(exp.Where):\n",
    "        for subquery in where.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        where_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    valuable_join_tables = sorted(join_subquery_tables - where_subquery_tables)\n",
    "\n",
    "    # Only include as source/target if:\n",
    "    # - referenced in a CTE definition (cte_source_tables)\n",
    "    # - or referenced outside of WHERE subqueries (i.e., not only in where_subquery_tables)\n",
    "    source_target_tables = sorted(\n",
    "        t for t in all_physical_tables\n",
    "        if t in cte_source_tables or t not in where_subquery_tables\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        source_target_tables,\n",
    "        sorted(cte_names),\n",
    "        valuable_join_tables,\n",
    "        sorted(where_subquery_tables),\n",
    "        table_aliases\n",
    "    )\n",
    "\n",
    "# Example usage and test logic:\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = extract_snowflake_tables(q)\n",
    "    print(f\"\\nTest case {i}:\")\n",
    "    print(\"  Source/target tables (all physical tables):\")\n",
    "    for t in target_tables:\n",
    "        print(f\"    - {t}\")\n",
    "    print(\"  CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"    - {c}\")\n",
    "    print(\"  JOIN/derived subquery tables (valuable for lineage):\")\n",
    "    for j in join_subquery_tables:\n",
    "        print(f\"    - {j}\")\n",
    "    print(\"  WHERE subquery tables (not useful for lineage):\")\n",
    "    for w in where_subquery_tables:\n",
    "        print(f\"    - {w}\")\n",
    "    print(\"  Table aliases:\")\n",
    "    for alias, table in table_aliases.items():\n",
    "        print(f\"    {alias} -> {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbb336",
   "metadata": {},
   "source": [
    "# Columns Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23457fb0",
   "metadata": {},
   "source": [
    "## Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2427349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_column': 'a', 'expression': 'a', 'source_columns': [('', 'a')], 'type': 'direct'}\n",
      "{'target_column': 'sum_col', 'expression': 'b + c AS sum_col', 'source_columns': [('', 'c'), ('', 'b')], 'type': 'calculated'}\n",
      "{'target_column': 'const_col', 'expression': \"'foo' AS const_col\", 'source_columns': [], 'type': 'constant'}\n",
      "{'target_column': 'd_alias', 'expression': 't1.d AS d_alias', 'source_columns': [('t1', 'd')], 'type': 'calculated'}\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of dicts, each describing an output column.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    columns = []\n",
    "\n",
    "    # Helper to get the string representation of an expression\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    # Helper to recursively collect all column references in an expression\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # node.table can be None if unqualified\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Find the outermost SELECT (not inside a subquery)\n",
    "    select = parsed\n",
    "    while not isinstance(select, exp.Select) and select:\n",
    "        select = select.args.get(\"this\") if hasattr(select, \"args\") else None\n",
    "\n",
    "    if not isinstance(select, exp.Select):\n",
    "        # Try to find any SELECT if not top-level\n",
    "        select = next(parsed.find_all(exp.Select), None)\n",
    "\n",
    "    if select:\n",
    "        for proj in select.expressions:\n",
    "            # Target/output column name\n",
    "            alias = proj.alias_or_name\n",
    "            # Raw SQL for the expression\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            # Source columns referenced in the expression\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            # Type: direct, calculated, or constant\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            columns.append({\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "    return columns\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    example_query = \"\"\"\n",
    "    SELECT\n",
    "        a,\n",
    "        b + c AS sum_col,\n",
    "        'foo' AS const_col,\n",
    "        t1.d AS d_alias\n",
    "    FROM my_schema.my_table t1\n",
    "    \"\"\"\n",
    "    cols = extract_snowflake_columns(example_query)\n",
    "    for col in cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24920054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source/target tables (all physical tables):\n",
      "  - my_schema.my_table\n",
      "CTE names:\n",
      "Table aliases:\n",
      "  t1 -> my_schema.my_table\n",
      "\n",
      "Columns lineage:\n",
      "Target column: a\n",
      "  Expression: a\n",
      "  Source columns: [('', 'a')]\n",
      "  Resolved source columns: [('', 'a')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: sum_col\n",
      "  Expression: b + c AS sum_col\n",
      "  Source columns: [('', 'c'), ('', 'b')]\n",
      "  Resolved source columns: [('', 'c'), ('', 'b')]\n",
      "  Type: calculated\n",
      "\n",
      "Target column: const_col\n",
      "  Expression: 'foo' AS const_col\n",
      "  Source columns: []\n",
      "  Resolved source columns: []\n",
      "  Type: constant\n",
      "\n",
      "Target column: d_alias\n",
      "  Expression: t1.d AS d_alias\n",
      "  Source columns: [('t1', 'd')]\n",
      "  Resolved source columns: [('my_schema.my_table', 'd')]\n",
      "  Type: calculated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "def extract_snowflake_tables(sql_query):\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    cte_names = set()\n",
    "    table_aliases = dict()\n",
    "    all_physical_tables = set()\n",
    "    join_subquery_tables = set()\n",
    "    where_subquery_tables = set()\n",
    "    cte_source_tables = set()\n",
    "\n",
    "    # Collect CTE names and their source tables\n",
    "    for cte in parsed.find_all(exp.CTE):\n",
    "        if cte.alias:\n",
    "            cte_names.add(cte.alias)\n",
    "        # Find tables referenced inside CTE definitions\n",
    "        for node in cte.find_all(exp.Table):\n",
    "            db = node.catalog or \"\"\n",
    "            schema = node.db or \"\"\n",
    "            name = node.name\n",
    "            if db and schema:\n",
    "                full_name = f\"{db}.{schema}.{name}\"\n",
    "            elif schema:\n",
    "                full_name = f\"{schema}.{name}\"\n",
    "            else:\n",
    "                full_name = name\n",
    "            if full_name not in cte_names:\n",
    "                cte_source_tables.add(full_name)\n",
    "\n",
    "    # Helper to get full table name\n",
    "    def get_full_name(node):\n",
    "        db = node.catalog or \"\"\n",
    "        schema = node.db or \"\"\n",
    "        name = node.name\n",
    "        if db and schema:\n",
    "            return f\"{db}.{schema}.{name}\"\n",
    "        elif schema:\n",
    "            return f\"{schema}.{name}\"\n",
    "        else:\n",
    "            return name\n",
    "\n",
    "    # Collect all physical tables (not CTEs) anywhere in the query\n",
    "    for node in parsed.walk():\n",
    "        if isinstance(node, exp.Table):\n",
    "            full_name = get_full_name(node)\n",
    "            if full_name not in cte_names:\n",
    "                all_physical_tables.add(full_name)\n",
    "            if node.alias:\n",
    "                table_aliases[node.alias] = full_name\n",
    "\n",
    "    # Collect tables in JOIN subqueries and derived tables\n",
    "    for join in parsed.find_all(exp.Join):\n",
    "        for subquery in join.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        join_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    # Collect tables in WHERE subqueries\n",
    "    for where in parsed.find_all(exp.Where):\n",
    "        for subquery in where.find_all(exp.Subquery):\n",
    "            for node in subquery.walk():\n",
    "                if isinstance(node, exp.Table):\n",
    "                    tbl = get_full_name(node)\n",
    "                    if tbl not in cte_names:\n",
    "                        where_subquery_tables.add(tbl)\n",
    "                    if node.alias:\n",
    "                        table_aliases[node.alias] = tbl\n",
    "\n",
    "    valuable_join_tables = sorted(join_subquery_tables - where_subquery_tables)\n",
    "\n",
    "    # Only include as source/target if:\n",
    "    # - referenced in a CTE definition (cte_source_tables)\n",
    "    # - or referenced outside of WHERE subqueries (i.e., not only in where_subquery_tables)\n",
    "    source_target_tables = sorted(\n",
    "        t for t in all_physical_tables\n",
    "        if t in cte_source_tables or t not in where_subquery_tables\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        source_target_tables,\n",
    "        sorted(cte_names),\n",
    "        valuable_join_tables,\n",
    "        sorted(where_subquery_tables),\n",
    "        table_aliases\n",
    "    )\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of dicts, each describing an output column.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    columns = []\n",
    "\n",
    "    # Helper to get the string representation of an expression\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    # Helper to recursively collect all column references in an expression\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # node.table can be None if unqualified\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Find the outermost SELECT (not inside a subquery)\n",
    "    select = parsed\n",
    "    while not isinstance(select, exp.Select) and select:\n",
    "        select = select.args.get(\"this\") if hasattr(select, \"args\") else None\n",
    "\n",
    "    if not isinstance(select, exp.Select):\n",
    "        # Try to find any SELECT if not top-level\n",
    "        select = next(parsed.find_all(exp.Select), None)\n",
    "\n",
    "    if select:\n",
    "        for proj in select.expressions:\n",
    "            # Target/output column name\n",
    "            alias = proj.alias_or_name\n",
    "            # Raw SQL for the expression\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            # Source columns referenced in the expression\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            # Type: direct, calculated, or constant\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            columns.append({\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "    return columns\n",
    "\n",
    "# Example usage and integration:\n",
    "test_sql = \"\"\"\n",
    "SELECT\n",
    "    a,\n",
    "    b + c AS sum_col,\n",
    "    'foo' AS const_col,\n",
    "    t1.d AS d_alias\n",
    "FROM my_schema.my_table t1\n",
    "\"\"\"\n",
    "\n",
    "# Extract tables and aliases\n",
    "tables_result = extract_snowflake_tables(test_sql)\n",
    "source_target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = tables_result\n",
    "\n",
    "# Extract columns\n",
    "columns_result = extract_snowflake_columns(test_sql)\n",
    "\n",
    "# Resolve source tables for each column using table_aliases\n",
    "for col in columns_result:\n",
    "    resolved_sources = []\n",
    "    for alias, col_name in col[\"source_columns\"]:\n",
    "        if alias in table_aliases:\n",
    "            resolved_sources.append((table_aliases[alias], col_name))\n",
    "        elif alias is None and len(source_target_tables) == 1:\n",
    "            # Unqualified column, only one table in FROM\n",
    "            resolved_sources.append((source_target_tables[0], col_name))\n",
    "        else:\n",
    "            resolved_sources.append((alias, col_name))  # Could be None or a CTE\n",
    "    col[\"resolved_source_columns\"] = resolved_sources\n",
    "\n",
    "# Print results\n",
    "print(\"Source/target tables (all physical tables):\")\n",
    "for t in source_target_tables:\n",
    "    print(f\"  - {t}\")\n",
    "print(\"CTE names:\")\n",
    "for c in cte_names:\n",
    "    print(f\"  - {c}\")\n",
    "print(\"Table aliases:\")\n",
    "for alias, table in table_aliases.items():\n",
    "    print(f\"  {alias} -> {table}\")\n",
    "\n",
    "print(\"\\nColumns lineage:\")\n",
    "for col in columns_result:\n",
    "    print(f\"Target column: {col['target_column']}\")\n",
    "    print(f\"  Expression: {col['expression']}\")\n",
    "    print(f\"  Source columns: {col['source_columns']}\")\n",
    "    print(f\"  Resolved source columns: {col['resolved_source_columns']}\")\n",
    "    print(f\"  Type: {col['type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10eecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEW TEST CASE ===\n",
      "All table variables:\n",
      "  source_target_tables: ['dummy_schema.dummy_status', 'dummy_schema.dummy_table']\n",
      "  cte_names: ['__dbt__cte__dummy_data', 'get_dummy_data']\n",
      "  join_subquery_tables: ['dummy_schema.dummy_status']\n",
      "  where_subquery_tables: ['dummy_schema.dummy_table', 'dummy_schema.status_codes']\n",
      "  table_aliases: {'gd': 'get_dummy_data'}\n",
      "\n",
      "Source/target tables (all physical tables):\n",
      "  - dummy_schema.dummy_status\n",
      "  - dummy_schema.dummy_table\n",
      "CTE names:\n",
      "  - __dbt__cte__dummy_data\n",
      "  - get_dummy_data\n",
      "Table aliases:\n",
      "  gd -> get_dummy_data\n",
      "\n",
      "Columns lineage:\n",
      "Target column: dummy_id\n",
      "  Expression: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "  Source columns: [('gd', 'dummy_ver_name'), ('gd', 'dummy_var_name'), ('gd', 'dummy_pop_name'), ('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name'), ('get_dummy_data', 'dummy_var_name'), ('get_dummy_data', 'dummy_pop_name'), ('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: calculated\n",
      "\n",
      "Target column: dummy_key\n",
      "  Expression: gd.dummy_key\n",
      "  Source columns: [('gd', 'dummy_key')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_key')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_ver_name\n",
      "  Expression: gd.dummy_ver_name\n",
      "  Source columns: [('gd', 'dummy_ver_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_pop_name\n",
      "  Expression: gd.dummy_pop_name\n",
      "  Source columns: [('gd', 'dummy_pop_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_pop_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_level_cd\n",
      "  Expression: gd.dummy_level_cd\n",
      "  Source columns: [('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_var_name\n",
      "  Expression: gd.dummy_var_name\n",
      "  Source columns: [('gd', 'dummy_var_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_var_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_coef\n",
      "  Expression: gd.dummy_coef\n",
      "  Source columns: [('gd', 'dummy_coef')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_coef')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: latest_status\n",
      "  Expression: gd.latest_status\n",
      "  Source columns: [('gd', 'latest_status')]\n",
      "  Resolved source columns: [('get_dummy_data', 'latest_status')]\n",
      "  Type: direct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage and integration:\n",
    "test_sql = \"\"\"\n",
    "SELECT\n",
    "    a,\n",
    "    b + c AS sum_col,\n",
    "    'foo' AS const_col,\n",
    "    t1.d AS d_alias\n",
    "FROM my_schema.my_table t1\n",
    "\"\"\"\n",
    "\n",
    "# Add the complex CTE/subquery example as a test case\n",
    "test_sql_2 = \"\"\"\n",
    "WITH __dbt__cte__dummy_data AS (\n",
    "    SELECT\n",
    "        upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "        upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "        upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "        upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "        nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "    FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
    "),\n",
    "get_dummy_data AS (\n",
    "    SELECT\n",
    "        dummy_ver_name,\n",
    "        dummy_pop_name,\n",
    "        dummy_level_cd,\n",
    "        dummy_var_name,\n",
    "        dummy_coef\n",
    "    FROM __dbt__cte__dummy_data\n",
    ")\n",
    "SELECT\n",
    "    COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "    gd.dummy_key,\n",
    "    gd.dummy_ver_name,\n",
    "    gd.dummy_pop_name,\n",
    "    gd.dummy_level_cd,\n",
    "    gd.dummy_var_name,\n",
    "    gd.dummy_coef,\n",
    "    gd.latest_status\n",
    "FROM get_dummy_data gd\n",
    "INNER JOIN (\n",
    "    SELECT\n",
    "        dummy_ver_name,\n",
    "        MAX(status_date) AS latest_status\n",
    "    FROM dummy_schema.dummy_status\n",
    "    WHERE status_code IN (\n",
    "        SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
    "    )\n",
    "    GROUP BY dummy_ver_name\n",
    ") sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
    "WHERE gd.dummy_coef > (\n",
    "    SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "for sql in [test_sql_2]:\n",
    "    print(\"\\n=== NEW TEST CASE ===\")\n",
    "    tables_result = extract_snowflake_tables(sql)\n",
    "    source_target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = tables_result\n",
    "\n",
    "    # --- Add this block to show all table-related variables ---\n",
    "    print(\"All table variables:\")\n",
    "    print(f\"  source_target_tables: {source_target_tables}\")\n",
    "    print(f\"  cte_names: {cte_names}\")\n",
    "    print(f\"  join_subquery_tables: {join_subquery_tables}\")\n",
    "    print(f\"  where_subquery_tables: {where_subquery_tables}\")\n",
    "    print(f\"  table_aliases: {table_aliases}\")\n",
    "    # If you want to see all_physical_tables and cte_source_tables, you need to modify extract_snowflake_tables to return them as well.\n",
    "    # For now, only the above are available from the return value.\n",
    "    print()\n",
    "\n",
    "    columns_result = extract_snowflake_columns(sql)\n",
    "\n",
    "    for col in columns_result:\n",
    "        resolved_sources = []\n",
    "        for alias, col_name in col[\"source_columns\"]:\n",
    "            if alias in table_aliases:\n",
    "                resolved_sources.append((table_aliases[alias], col_name))\n",
    "            elif alias is None and len(source_target_tables) == 1:\n",
    "                resolved_sources.append((source_target_tables[0], col_name))\n",
    "            else:\n",
    "                resolved_sources.append((alias, col_name))\n",
    "        col[\"resolved_source_columns\"] = resolved_sources\n",
    "\n",
    "    print(\"Source/target tables (all physical tables):\")\n",
    "    for t in source_target_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"  - {c}\")\n",
    "    print(\"Table aliases:\")\n",
    "    for alias, table in table_aliases.items():\n",
    "        print(f\"  {alias} -> {table}\")\n",
    "\n",
    "    print(\"\\nColumns lineage:\")\n",
    "    for col in columns_result:\n",
    "        print(f\"Target column: {col['target_column']}\")\n",
    "        print(f\"  Expression: {col['expression']}\")\n",
    "        print(f\"  Source columns: {col['source_columns']}\")\n",
    "        print(f\"  Resolved source columns: {col['resolved_source_columns']}\")\n",
    "        print(f\"  Type: {col['type']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0222ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEW TEST CASE ===\n",
      "\n",
      "    WITH __dbt__cte__dummy_data AS (\n",
      "        SELECT\n",
      "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
      "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
      "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
      "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
      "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
      "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
      "    ),\n",
      "    get_dummy_data AS (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            dummy_pop_name,\n",
      "            dummy_level_cd,\n",
      "            dummy_var_name,\n",
      "            dummy_coef\n",
      "        FROM __dbt__cte__dummy_data\n",
      "    )\n",
      "    SELECT\n",
      "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
      "        gd.dummy_key,\n",
      "        gd.dummy_ver_name,\n",
      "        gd.dummy_pop_name,\n",
      "        gd.dummy_level_cd,\n",
      "        gd.dummy_var_name,\n",
      "        gd.dummy_coef,\n",
      "        sub.latest_status\n",
      "    FROM get_dummy_data gd\n",
      "    INNER JOIN (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            MAX(status_date) AS latest_status\n",
      "        FROM dummy_schema.dummy_status\n",
      "        WHERE status_code IN (\n",
      "            SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
      "        )\n",
      "        GROUP BY dummy_ver_name\n",
      "    ) sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
      "    WHERE gd.dummy_coef > (\n",
      "        SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
      "    )\n",
      "    \n",
      "All table variables:\n",
      "  source_target_tables: ['dummy_schema.dummy_status', 'dummy_schema.dummy_table']\n",
      "  cte_names: ['__dbt__cte__dummy_data', 'get_dummy_data']\n",
      "  join_subquery_tables: ['dummy_schema.dummy_status']\n",
      "  where_subquery_tables: ['dummy_schema.dummy_table', 'dummy_schema.status_codes']\n",
      "  table_aliases: {'gd': 'get_dummy_data'}\n",
      "\n",
      "Source/target tables (all physical tables):\n",
      "  - dummy_schema.dummy_status\n",
      "  - dummy_schema.dummy_table\n",
      "CTE names:\n",
      "  - __dbt__cte__dummy_data\n",
      "  - get_dummy_data\n",
      "Table aliases:\n",
      "  gd -> get_dummy_data\n",
      "\n",
      "Columns lineage:\n",
      "Target column: dummy_id\n",
      "  Expression: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "  Source columns: [('gd', 'dummy_ver_name'), ('gd', 'dummy_var_name'), ('gd', 'dummy_pop_name'), ('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name'), ('get_dummy_data', 'dummy_var_name'), ('get_dummy_data', 'dummy_pop_name'), ('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: calculated\n",
      "\n",
      "Target column: dummy_key\n",
      "  Expression: gd.dummy_key\n",
      "  Source columns: [('gd', 'dummy_key')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_key')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_ver_name\n",
      "  Expression: gd.dummy_ver_name\n",
      "  Source columns: [('gd', 'dummy_ver_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_ver_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_pop_name\n",
      "  Expression: gd.dummy_pop_name\n",
      "  Source columns: [('gd', 'dummy_pop_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_pop_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_level_cd\n",
      "  Expression: gd.dummy_level_cd\n",
      "  Source columns: [('gd', 'dummy_level_cd')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_level_cd')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_var_name\n",
      "  Expression: gd.dummy_var_name\n",
      "  Source columns: [('gd', 'dummy_var_name')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_var_name')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: dummy_coef\n",
      "  Expression: gd.dummy_coef\n",
      "  Source columns: [('gd', 'dummy_coef')]\n",
      "  Resolved source columns: [('get_dummy_data', 'dummy_coef')]\n",
      "  Type: direct\n",
      "\n",
      "Target column: latest_status\n",
      "  Expression: sub.latest_status\n",
      "  Source columns: [('sub', 'latest_status')]\n",
      "  Resolved source columns: [('sub', 'latest_status')]\n",
      "  Type: direct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sql in test_queries[6:7]:\n",
    "    print(\"\\n=== NEW TEST CASE ===\")\n",
    "    print(sql)\n",
    "    tables_result = extract_snowflake_tables(sql)\n",
    "    source_target_tables, cte_names, join_subquery_tables, where_subquery_tables, table_aliases = tables_result\n",
    "\n",
    "    # --- Add this block to show all table-related variables ---\n",
    "    print(\"All table variables:\")\n",
    "    print(f\"  source_target_tables: {source_target_tables}\")\n",
    "    print(f\"  cte_names: {cte_names}\")\n",
    "    print(f\"  join_subquery_tables: {join_subquery_tables}\")\n",
    "    print(f\"  where_subquery_tables: {where_subquery_tables}\")\n",
    "    print(f\"  table_aliases: {table_aliases}\")\n",
    "    # If you want to see all_physical_tables and cte_source_tables, you need to modify extract_snowflake_tables to return them as well.\n",
    "    # For now, only the above are available from the return value.\n",
    "    print()\n",
    "\n",
    "    columns_result = extract_snowflake_columns(sql)\n",
    "\n",
    "    for col in columns_result:\n",
    "        resolved_sources = []\n",
    "        for alias, col_name in col[\"source_columns\"]:\n",
    "            if alias in table_aliases:\n",
    "                resolved_sources.append((table_aliases[alias], col_name))\n",
    "            elif alias is None and len(source_target_tables) == 1:\n",
    "                resolved_sources.append((source_target_tables[0], col_name))\n",
    "            else:\n",
    "                resolved_sources.append((alias, col_name))\n",
    "        col[\"resolved_source_columns\"] = resolved_sources\n",
    "\n",
    "    print(\"Source/target tables (all physical tables):\")\n",
    "    for t in source_target_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"CTE names:\")\n",
    "    for c in cte_names:\n",
    "        print(f\"  - {c}\")\n",
    "    print(\"Table aliases:\")\n",
    "    for alias, table in table_aliases.items():\n",
    "        print(f\"  {alias} -> {table}\")\n",
    "\n",
    "    print(\"\\nColumns lineage:\")\n",
    "    for col in columns_result:\n",
    "        print(f\"Target column: {col['target_column']}\")\n",
    "        print(f\"  Expression: {col['expression']}\")\n",
    "        print(f\"  Source columns: {col['source_columns']}\")\n",
    "        print(f\"  Resolved source columns: {col['resolved_source_columns']}\")\n",
    "        print(f\"  Type: {col['type']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6edc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No source column identified because col is coming from 3 tables?\n",
    "# How to align query alias \"sub\" with source table from within derived query? \n",
    "# Need to work on table to column association when is a column\n",
    "# for models that can't find their source is there a source search based on the tables in the model?\n",
    "# enforce best practice to add this aliases to all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdabf120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Query 1 ===\n",
      "\n",
      "    WITH __dbt__cte__dummy_data AS (\n",
      "        SELECT\n",
      "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
      "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
      "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
      "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
      "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
      "        FROM dummy_schema.dummy_table AT (TIMESTAMP => '2025-07-31 00:00:00')\n",
      "    ),\n",
      "    get_dummy_data AS (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            dummy_pop_name,\n",
      "            dummy_level_cd,\n",
      "            dummy_var_name,\n",
      "            dummy_coef\n",
      "        FROM __dbt__cte__dummy_data\n",
      "    )\n",
      "    SELECT\n",
      "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
      "        gd.dummy_key,\n",
      "        gd.dummy_ver_name,\n",
      "        gd.dummy_pop_name,\n",
      "        gd.dummy_level_cd,\n",
      "        gd.dummy_var_name,\n",
      "        gd.dummy_coef,\n",
      "        sub.latest_status\n",
      "    FROM get_dummy_data gd\n",
      "    INNER JOIN (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            MAX(status_date) AS latest_status\n",
      "        FROM dummy_schema.dummy_status\n",
      "        WHERE status_code IN (\n",
      "            SELECT code FROM dummy_schema.status_codes WHERE is_active = 1\n",
      "        )\n",
      "        GROUP BY dummy_ver_name\n",
      "    ) sub ON gd.dummy_ver_name = sub.dummy_ver_name\n",
      "    WHERE gd.dummy_coef > (\n",
      "        SELECT AVG(dummy_coef) FROM dummy_schema.dummy_table WHERE dummy_level_cd = gd.dummy_level_cd\n",
      "    )\n",
      "    \n",
      "\n",
      "  SELECT branch 1:\n",
      "    Target column: dummy_id\n",
      "      Expression: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "      Source columns: [('gd', 'dummy_ver_name'), ('gd', 'dummy_var_name'), ('gd', 'dummy_pop_name'), ('gd', 'dummy_level_cd')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_key\n",
      "      Expression: gd.dummy_key\n",
      "      Source columns: [('gd', 'dummy_key')]\n",
      "      Type: direct\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: gd.dummy_ver_name\n",
      "      Source columns: [('gd', 'dummy_ver_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_pop_name\n",
      "      Expression: gd.dummy_pop_name\n",
      "      Source columns: [('gd', 'dummy_pop_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_level_cd\n",
      "      Expression: gd.dummy_level_cd\n",
      "      Source columns: [('gd', 'dummy_level_cd')]\n",
      "      Type: direct\n",
      "    Target column: dummy_var_name\n",
      "      Expression: gd.dummy_var_name\n",
      "      Source columns: [('gd', 'dummy_var_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_coef\n",
      "      Expression: gd.dummy_coef\n",
      "      Source columns: [('gd', 'dummy_coef')]\n",
      "      Type: direct\n",
      "    Target column: latest_status\n",
      "      Expression: sub.latest_status\n",
      "      Source columns: [('sub', 'latest_status')]\n",
      "      Type: direct\n",
      "  SELECT branch 2:\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: dummy_ver_name\n",
      "      Source columns: [('', 'dummy_ver_name')]\n",
      "      Type: direct\n",
      "    Target column: latest_status\n",
      "      Expression: MAX(status_date) AS latest_status\n",
      "      Source columns: [('', 'status_date')]\n",
      "      Type: calculated\n",
      "  SELECT branch 3:\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VER_NAME') AS TEXT), '')) AS dummy_ver_name\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_pop_name\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_POP_NAME') AS TEXT), '')) AS dummy_pop_name\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_level_cd\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_LEVEL_CD') AS TEXT), '')) AS dummy_level_cd\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_var_name\n",
      "      Expression: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VAR_NAME') AS TEXT), '')) AS dummy_var_name\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "    Target column: dummy_coef\n",
      "      Expression: CAST(NULLIF(CAST(GET_PATH(v, 'DUMMY_COEF') AS TEXT), '') AS DECIMAL(8, 3)) AS dummy_coef\n",
      "      Source columns: [('', 'v')]\n",
      "      Type: calculated\n",
      "  SELECT branch 4:\n",
      "    Target column: dummy_ver_name\n",
      "      Expression: dummy_ver_name\n",
      "      Source columns: [('', 'dummy_ver_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_pop_name\n",
      "      Expression: dummy_pop_name\n",
      "      Source columns: [('', 'dummy_pop_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_level_cd\n",
      "      Expression: dummy_level_cd\n",
      "      Source columns: [('', 'dummy_level_cd')]\n",
      "      Type: direct\n",
      "    Target column: dummy_var_name\n",
      "      Expression: dummy_var_name\n",
      "      Source columns: [('', 'dummy_var_name')]\n",
      "      Type: direct\n",
      "    Target column: dummy_coef\n",
      "      Expression: dummy_coef\n",
      "      Source columns: [('', 'dummy_coef')]\n",
      "      Type: direct\n",
      "  SELECT branch 5:\n",
      "    Target column: \n",
      "      Expression: AVG(dummy_coef)\n",
      "      Source columns: [('', 'dummy_coef')]\n",
      "      Type: calculated\n",
      "  SELECT branch 6:\n",
      "    Target column: code\n",
      "      Expression: code\n",
      "      Source columns: [('', 'code')]\n",
      "      Type: direct\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import expressions as exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of dicts, each describing an output column.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    columns = []\n",
    "\n",
    "    # Helper to get the string representation of an expression\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    # Helper to recursively collect all column references in an expression\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Find all SELECTs in the query (including UNION branches, subqueries, etc.)\n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "\n",
    "    all_columns = []\n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        all_columns.append(select_columns)\n",
    "    return all_columns\n",
    "\n",
    "# Example usage:\n",
    "for i, sql in enumerate(test_queries[6:7], 1):\n",
    "    print(f\"\\n=== Test Query {i} ===\")\n",
    "    print(f'{sql}\\n')\n",
    "    all_columns = extract_snowflake_columns(sql)\n",
    "    for select_idx, select_columns in enumerate(all_columns):\n",
    "        print(f\"  SELECT branch {select_idx+1}:\")\n",
    "        for col in select_columns:\n",
    "            print(f\"    Target column: {col['target_column']}\")\n",
    "            print(f\"      Expression: {col['expression']}\")\n",
    "            print(f\"      Source columns: {col['source_columns']}\")\n",
    "            print(f\"      Type: {col['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8e96e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Query 1 ===\n",
      "\n",
      "\n",
      "    SELECT u.user_id, o.order_id\n",
      "    FROM analytics.users u\n",
      "    JOIN analytics.orders o ON u.user_id = o.user_id\n",
      "    UNION ALL\n",
      "    SELECT user_id, NULL\n",
      "    FROM analytics.inactive_users\n",
      "    WHERE last_login < '2024-01-01'\n",
      "    \n",
      "\n",
      "  SELECT branch 1:\n",
      "    Target column: user_id\n",
      "      Expression: u.user_id\n",
      "      Source columns: [('u', 'user_id')]\n",
      "      Resolved source columns: [('analytics.users', 'u', 'user_id')]\n",
      "      Type: direct\n",
      "    Target column: order_id\n",
      "      Expression: o.order_id\n",
      "      Source columns: [('o', 'order_id')]\n",
      "      Resolved source columns: [('o', 'order_id')]\n",
      "      Type: direct\n",
      "  SELECT branch 2:\n",
      "    Target column: user_id\n",
      "      Expression: user_id\n",
      "      Source columns: [('', 'user_id')]\n",
      "      Resolved source columns: [('analytics.inactive_users', 'user_id')]\n",
      "      Type: direct\n",
      "    Target column: NULL\n",
      "      Expression: NULL\n",
      "      Source columns: []\n",
      "      Resolved source columns: []\n",
      "      Type: constant\n"
     ]
    }
   ],
   "source": [
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                sources.add((node.table, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        \"\"\"\n",
    "        tables = {}\n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                if db and schema:\n",
    "                    full_name = f\"{db}.{schema}.{name}\"\n",
    "                elif schema:\n",
    "                    full_name = f\"{schema}.{name}\"\n",
    "                else:\n",
    "                    full_name = name\n",
    "                alias = base.alias or name\n",
    "                tables[alias.lower()] = (full_name, alias)\n",
    "            # JOINed tables\n",
    "            for join in from_expr.find_all(exp.Join):\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = join_table.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "        return tables\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    all_columns = []\n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            for tbl_alias, col_name in source_columns:\n",
    "                if (not tbl_alias or tbl_alias == \"\") and only_table:\n",
    "                    resolved_sources.append((only_table, col_name))\n",
    "                elif tbl_alias:\n",
    "                    tbl_alias_lc = tbl_alias.lower()\n",
    "                    if tbl_alias_lc in from_tables:\n",
    "                        full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                        resolved_sources.append((full_table, real_alias, col_name))\n",
    "                    else:\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                else:\n",
    "                    resolved_sources.append((tbl_alias, col_name))\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        all_columns.append(select_columns)\n",
    "    return all_columns\n",
    "\n",
    "for i, sql in enumerate(test_queries[1:2], 1):\n",
    "    print(f\"\\n=== Test Query {i} ===\")\n",
    "    print(f'\\n{sql}\\n')\n",
    "    all_columns = extract_snowflake_columns(sql)\n",
    "    for select_idx, select_columns in enumerate(all_columns):\n",
    "        print(f\"  SELECT branch {select_idx+1}:\")\n",
    "        for col in select_columns:\n",
    "            print(f\"    Target column: {col['target_column']}\")\n",
    "            print(f\"      Expression: {col['expression']}\")\n",
    "            print(f\"      Source columns: {col['source_columns']}\")\n",
    "            print(f\"      Resolved source columns: {col.get('resolved_source_columns', [])}\")\n",
    "            print(f\"      Type: {col['type']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55b011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Query 1 ===\n",
      "\n",
      "\n",
      "    SELECT id FROM db1.schema1.tableA\n",
      "    UNION\n",
      "    SELECT id FROM db2.schema2.tableB\n",
      "    \n",
      "\n",
      "  SELECT branch 1:\n",
      "    Target column: id\n",
      "      Expression: id\n",
      "      Source columns: [('', 'id')]\n",
      "      Resolved source columns: [('db1.schema1.tableA', 'id')]\n",
      "      Type: direct\n",
      "  SELECT branch 2:\n",
      "    Target column: id\n",
      "      Expression: id\n",
      "      Source columns: [('', 'id')]\n",
      "      Resolved source columns: [('db2.schema2.tableB', 'id')]\n",
      "      Type: direct\n"
     ]
    }
   ],
   "source": [
    "# claude attempt\n",
    "\n",
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # Get table alias/name - could be empty string\n",
    "                table_ref = node.table if node.table else \"\"\n",
    "                sources.add((table_ref, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        \"\"\"\n",
    "        tables = {}\n",
    "        \n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                if db and schema:\n",
    "                    full_name = f\"{db}.{schema}.{name}\"\n",
    "                elif schema:\n",
    "                    full_name = f\"{schema}.{name}\"\n",
    "                else:\n",
    "                    full_name = name\n",
    "                alias = base.alias or name\n",
    "                tables[alias.lower()] = (full_name, alias)\n",
    "        \n",
    "        # JOINs are stored at the SELECT level, not FROM level\n",
    "        joins = select.args.get(\"joins\")\n",
    "        if joins:\n",
    "            for join in joins:\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = join_table.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "                        \n",
    "        return tables\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    all_columns = []\n",
    "    \n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        \n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            \n",
    "            for tbl_alias, col_name in source_columns:\n",
    "                if not tbl_alias and only_table:\n",
    "                    # No table alias and only one table - use that table\n",
    "                    resolved_sources.append((only_table, col_name))\n",
    "                elif tbl_alias:\n",
    "                    # Has table alias - look it up in from_tables\n",
    "                    tbl_alias_lc = tbl_alias.lower()\n",
    "                    if tbl_alias_lc in from_tables:\n",
    "                        full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                        resolved_sources.append((full_table, real_alias, col_name))\n",
    "                    else:\n",
    "                        # Alias not found in from_tables - keep as is\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                else:\n",
    "                    # No table alias and multiple tables - ambiguous\n",
    "                    resolved_sources.append((tbl_alias, col_name))\n",
    "            \n",
    "            # Determine column type\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            \n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        \n",
    "        all_columns.append(select_columns)\n",
    "    \n",
    "    return all_columns\n",
    "\n",
    "# Test with your example query\n",
    "test_query = \"\"\"\n",
    "SELECT u.user_id, o.order_id\n",
    "FROM analytics.users u\n",
    "JOIN analytics.orders o ON u.user_id = o.user_id\n",
    "UNION ALL\n",
    "SELECT user_id, NULL\n",
    "FROM analytics.inactive_users\n",
    "WHERE last_login < '2024-01-01'\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Test Query 1 ===\")\n",
    "print(f'\\n{test_queries[0]}\\n')\n",
    "all_columns = extract_snowflake_columns(test_queries[0])\n",
    "\n",
    "for select_idx, select_columns in enumerate(all_columns):\n",
    "    print(f\"  SELECT branch {select_idx+1}:\")\n",
    "    for col in select_columns:\n",
    "        print(f\"    Target column: {col['target_column']}\")\n",
    "        print(f\"      Expression: {col['expression']}\")\n",
    "        print(f\"      Source columns: {col['source_columns']}\")\n",
    "        print(f\"      Resolved source columns: {col.get('resolved_source_columns', [])}\")\n",
    "        print(f\"      Type: {col['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f2f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a799445",
   "metadata": {},
   "source": [
    "## Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5437869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # Get table alias/name - could be empty string\n",
    "                table_ref = node.table if node.table else \"\"\n",
    "                sources.add((table_ref, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select, cte_registry=None):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        Now also considers CTEs in the registry\n",
    "        \"\"\"\n",
    "        if cte_registry is None:\n",
    "            cte_registry = {}\n",
    "            \n",
    "        tables = {}\n",
    "        \n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                \n",
    "                # Check if this is a CTE first\n",
    "                if name.lower() in cte_registry:\n",
    "                    # This is a CTE reference\n",
    "                    alias = base.alias or name\n",
    "                    tables[alias.lower()] = (f\"CTE:{name}\", alias)\n",
    "                else:\n",
    "                    # Regular table\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = base.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "        \n",
    "        # JOINs are stored at the SELECT level, not FROM level\n",
    "        joins = select.args.get(\"joins\")\n",
    "        if joins:\n",
    "            for join in joins:\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    \n",
    "                    # Check if this is a CTE first\n",
    "                    if name.lower() in cte_registry:\n",
    "                        # This is a CTE reference\n",
    "                        alias = join_table.alias or name\n",
    "                        tables[alias.lower()] = (f\"CTE:{name}\", alias)\n",
    "                    else:\n",
    "                        # Regular table\n",
    "                        if db and schema:\n",
    "                            full_name = f\"{db}.{schema}.{name}\"\n",
    "                        elif schema:\n",
    "                            full_name = f\"{schema}.{name}\"\n",
    "                        else:\n",
    "                            full_name = name\n",
    "                        alias = join_table.alias or name\n",
    "                        tables[alias.lower()] = (full_name, alias)\n",
    "                        \n",
    "        return tables\n",
    "    \n",
    "    # Build CTE registry first\n",
    "    cte_registry = {}\n",
    "    with_clause = parsed.args.get(\"with\")\n",
    "    if with_clause:\n",
    "        for cte in with_clause.expressions:\n",
    "            cte_name = cte.alias\n",
    "            cte_query = cte.this  # The SELECT part of the CTE\n",
    "            cte_registry[cte_name.lower()] = cte_query\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    all_columns = []\n",
    "    \n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select, cte_registry)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        \n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            \n",
    "            for tbl_alias, col_name in source_columns:\n",
    "                if not tbl_alias and only_table:\n",
    "                    # No table alias and only one table - use that table\n",
    "                    resolved_sources.append((only_table, col_name))\n",
    "                elif tbl_alias:\n",
    "                    # Has table alias - look it up in from_tables\n",
    "                    tbl_alias_lc = tbl_alias.lower()\n",
    "                    if tbl_alias_lc in from_tables:\n",
    "                        full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                        resolved_sources.append((full_table, real_alias, col_name))\n",
    "                    else:\n",
    "                        # Alias not found in from_tables - keep as is\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                else:\n",
    "                    # No table alias and multiple tables - ambiguous\n",
    "                    resolved_sources.append((tbl_alias, col_name))\n",
    "            \n",
    "            # Determine column type\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            \n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        \n",
    "        all_columns.append(select_columns)\n",
    "    \n",
    "    return all_columns\n",
    "\n",
    "\n",
    "def trace_column_lineage(sql_query, target_column_name):\n",
    "    \"\"\"\n",
    "    Traces a specific column through all transformations and builds LLM-ready context.\n",
    "    FIXED: Now properly handles both 'column' and 'columns' keys in CTE transformations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse and build CTE registry\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    cte_registry = {}\n",
    "    with_clause = parsed.args.get(\"with\")\n",
    "    if with_clause:\n",
    "        for cte in with_clause.expressions:\n",
    "            cte_name = cte.alias\n",
    "            cte_query = cte.this\n",
    "            cte_registry[cte_name.lower()] = cte_query\n",
    "    \n",
    "    # Get basic column analysis\n",
    "    base_columns = extract_snowflake_columns(sql_query)\n",
    "    \n",
    "    # Find the target column in the final output\n",
    "    target_column_info = None\n",
    "    target_select_branch = None\n",
    "    \n",
    "    for select_idx, select_columns in enumerate(base_columns):\n",
    "        for col_info in select_columns:\n",
    "            if col_info['target_column'].lower() == target_column_name.lower():\n",
    "                target_column_info = col_info\n",
    "                target_select_branch = select_idx + 1\n",
    "                break\n",
    "        if target_column_info:\n",
    "            break\n",
    "    \n",
    "    if not target_column_info:\n",
    "        return {\n",
    "            \"error\": f\"Column '{target_column_name}' not found in query output\",\n",
    "            \"llm_context\": f\"The column '{target_column_name}' was not found in the final query output.\",\n",
    "            \"next_columns_to_search\": [],\n",
    "            \"full_lineage\": {}\n",
    "        }\n",
    "    \n",
    "    # Build LLM context\n",
    "    llm_context_parts = []\n",
    "    next_columns = []\n",
    "    cte_transformations = []  # Track internal CTE transformations\n",
    "    \n",
    "    # Basic column information\n",
    "    llm_context_parts.append(f\"COLUMN: {target_column_name}\")\n",
    "    llm_context_parts.append(f\"EXPRESSION: {target_column_info['expression']}\")\n",
    "    llm_context_parts.append(f\"TRANSFORMATION TYPE: {target_column_info['type']}\")\n",
    "    \n",
    "    if target_select_branch:\n",
    "        llm_context_parts.append(f\"FOUND IN: SELECT branch {target_select_branch}\")\n",
    "    \n",
    "    # Process resolved sources and trace through CTEs\n",
    "    resolved_sources = target_column_info.get('resolved_source_columns', [])\n",
    "    \n",
    "    if resolved_sources:\n",
    "        llm_context_parts.append(\"\\nSOURCE ANALYSIS:\")\n",
    "        \n",
    "        # Group resolved sources by table to detect multiple dependencies from same table/CTE\n",
    "        sources_by_table = {}\n",
    "        for source in resolved_sources:\n",
    "            if len(source) >= 3:  # (table, alias, column)\n",
    "                table, alias, column = source[:3]\n",
    "                table_key = table\n",
    "            elif len(source) == 2:  # (table, column)\n",
    "                table, column = source\n",
    "                alias = table\n",
    "                table_key = table\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if table_key not in sources_by_table:\n",
    "                sources_by_table[table_key] = []\n",
    "            sources_by_table[table_key].append((table, alias, column))\n",
    "        \n",
    "        # Process each table's dependencies\n",
    "        for table_key, table_sources in sources_by_table.items():\n",
    "            if len(table_sources) == 1:\n",
    "                # Single dependency from this table\n",
    "                table, alias, column = table_sources[0]\n",
    "                \n",
    "                # Check if this is a CTE reference\n",
    "                if table.startswith(\"CTE:\"):\n",
    "                    cte_name = table.replace(\"CTE:\", \"\")\n",
    "                    llm_context_parts.append(f\"  └─ CTE REFERENCE: {alias}.{column} → {cte_name}.{column}\")\n",
    "                    \n",
    "                    # Trace through the CTE\n",
    "                    if cte_name.lower() in cte_registry:\n",
    "                        cte_query = cte_registry[cte_name.lower()]\n",
    "                        cte_sql = cte_query.sql(dialect=\"snowflake\")\n",
    "                        llm_context_parts.append(f\"  └─ TRACING CTE '{cte_name}' (INTRA-FILE TRANSFORMATION):\")\n",
    "                        \n",
    "                        # Recursively analyze the CTE\n",
    "                        cte_trace = trace_column_lineage(cte_sql, column)\n",
    "                        if \"error\" not in cte_trace:\n",
    "                            # Add CTE transformation info\n",
    "                            cte_transformations.append({\n",
    "                                \"cte_name\": cte_name,\n",
    "                                \"column\": column,\n",
    "                                \"transformation_type\": \"intra_file_cte\",\n",
    "                                \"details\": cte_trace.get(\"llm_context\", \"\"),\n",
    "                                \"dependencies\": cte_trace.get(\"next_columns_to_search\", [])\n",
    "                            })\n",
    "                            \n",
    "                            # Add CTE's external dependencies to our next_columns (not the CTE itself)\n",
    "                            for cte_dep in cte_trace.get(\"next_columns_to_search\", []):\n",
    "                                next_columns.append({\n",
    "                                    \"table\": cte_dep[\"table\"],\n",
    "                                    \"column\": cte_dep[\"column\"],\n",
    "                                    \"context\": f\"External source for {target_column_name} via CTE {cte_name}\",\n",
    "                                    \"level\": \"external_via_cte\",\n",
    "                                    \"cte_intermediate\": cte_name\n",
    "                                })\n",
    "                            \n",
    "                            # Add CTE context to LLM output\n",
    "                            cte_context_lines = cte_trace[\"llm_context\"].split('\\n')\n",
    "                            for line in cte_context_lines:\n",
    "                                if line.strip():\n",
    "                                    llm_context_parts.append(f\"    {line}\")\n",
    "                        else:\n",
    "                            llm_context_parts.append(f\"    ERROR tracing CTE: {cte_trace['error']}\")\n",
    "                    else:\n",
    "                        llm_context_parts.append(f\"    WARNING: CTE '{cte_name}' not found in registry\")\n",
    "                        \n",
    "                else:\n",
    "                    # Regular table reference\n",
    "                    llm_context_parts.append(f\"  └─ EXTERNAL TABLE: {table}.{column} (referenced as {alias}.{column})\")\n",
    "                    next_columns.append({\n",
    "                        \"table\": table,\n",
    "                        \"column\": column,\n",
    "                        \"context\": f\"External table dependency for {target_column_name}\",\n",
    "                        \"level\": \"external_table\"\n",
    "                    })\n",
    "            \n",
    "            else:\n",
    "                # Multiple dependencies from same table - group them\n",
    "                table_name = table_key.replace(\"CTE:\", \"\") if table_key.startswith(\"CTE:\") else table_key\n",
    "                columns = [col for _, _, col in table_sources]\n",
    "                \n",
    "                llm_context_parts.append(f\"  └─ MULTIPLE DEPENDENCIES FROM {table_name}:\")\n",
    "                for table, alias, column in table_sources:\n",
    "                    llm_context_parts.append(f\"    • {alias}.{column}\")\n",
    "                \n",
    "                if table_key.startswith(\"CTE:\"):\n",
    "                    # Multiple CTE dependencies - consolidate them\n",
    "                    cte_name = table_key.replace(\"CTE:\", \"\")\n",
    "                    llm_context_parts.append(f\"  └─ CONSOLIDATED CTE ANALYSIS for '{cte_name}':\")\n",
    "                    \n",
    "                    if cte_name.lower() in cte_registry:\n",
    "                        cte_query = cte_registry[cte_name.lower()]\n",
    "                        cte_sql = cte_query.sql(dialect=\"snowflake\")\n",
    "                        \n",
    "                        # Get all unique external dependencies from this CTE\n",
    "                        all_cte_deps = set()\n",
    "                        cte_column_details = []\n",
    "                        \n",
    "                        for table, alias, column in table_sources:\n",
    "                            cte_trace = trace_column_lineage(cte_sql, column)\n",
    "                            if \"error\" not in cte_trace:\n",
    "                                cte_column_details.append({\n",
    "                                    \"column\": column,\n",
    "                                    \"trace\": cte_trace\n",
    "                                })\n",
    "                                \n",
    "                                # Collect external dependencies\n",
    "                                for cte_dep in cte_trace.get(\"next_columns_to_search\", []):\n",
    "                                    dep_key = (cte_dep[\"table\"], cte_dep[\"column\"])\n",
    "                                    all_cte_deps.add(dep_key)\n",
    "                        \n",
    "                        # Add consolidated CTE transformation\n",
    "                        cte_transformations.append({\n",
    "                            \"cte_name\": cte_name,\n",
    "                            \"columns\": columns,  # Multiple columns - use 'columns' key\n",
    "                            \"transformation_type\": \"consolidated_cte\",\n",
    "                            \"details\": f\"CTE processes {len(columns)} columns: {', '.join(columns)}\",\n",
    "                            \"column_details\": cte_column_details\n",
    "                        })\n",
    "                        \n",
    "                        # Add unique external dependencies\n",
    "                        for dep_table, dep_column in all_cte_deps:\n",
    "                            next_columns.append({\n",
    "                                \"table\": dep_table,\n",
    "                                \"column\": dep_column,\n",
    "                                \"context\": f\"External source for {target_column_name} via consolidated CTE {cte_name}\",\n",
    "                                \"level\": \"external_via_cte\",\n",
    "                                \"cte_intermediate\": cte_name\n",
    "                            })\n",
    "                        \n",
    "                        # Show consolidated CTE analysis\n",
    "                        llm_context_parts.append(f\"    └─ CTE '{cte_name}' processes {len(columns)} output columns\")\n",
    "                        llm_context_parts.append(f\"    └─ External dependencies: {len(all_cte_deps)} unique sources\")\n",
    "                        \n",
    "                else:\n",
    "                    # Multiple dependencies from regular table\n",
    "                    for table, alias, column in table_sources:\n",
    "                        next_columns.append({\n",
    "                            \"table\": table,\n",
    "                            \"column\": column,\n",
    "                            \"context\": f\"External table dependency for {target_column_name}\",\n",
    "                            \"level\": \"external_table\"\n",
    "                        })\n",
    "    \n",
    "    # Show CTE transformations summary with safe access to column/columns\n",
    "    if cte_transformations:\n",
    "        llm_context_parts.append(f\"\\nINTRA-FILE CTE TRANSFORMATIONS:\")\n",
    "        for cte_info in cte_transformations:\n",
    "            # FIXED: Handle both 'column' and 'columns' keys safely\n",
    "            if 'column' in cte_info:\n",
    "                column_info = cte_info['column']\n",
    "            elif 'columns' in cte_info:\n",
    "                columns_list = cte_info['columns']\n",
    "                if isinstance(columns_list, list):\n",
    "                    column_info = ', '.join(columns_list)\n",
    "                else:\n",
    "                    column_info = str(columns_list)\n",
    "            else:\n",
    "                column_info = 'unknown'\n",
    "            \n",
    "            llm_context_parts.append(f\"  CTE '{cte_info['cte_name']}' transforms {column_info}\")\n",
    "            llm_context_parts.append(f\"    └─ Type: {cte_info.get('transformation_type', 'unknown')}\")\n",
    "    \n",
    "    # Show CTE definitions if relevant\n",
    "    if cte_registry:\n",
    "        llm_context_parts.append(f\"\\nAVAILABLE CTEs IN THIS FILE:\")\n",
    "        for cte_name in cte_registry.keys():\n",
    "            llm_context_parts.append(f\"  - {cte_name}\")\n",
    "    \n",
    "    # Remove duplicates from next_columns\n",
    "    unique_next_columns = []\n",
    "    seen = set()\n",
    "    for col in next_columns:\n",
    "        key = (col['table'], col['column'], col['level'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_next_columns.append(col)\n",
    "    \n",
    "    return {\n",
    "        \"llm_context\": \"\\n\".join(llm_context_parts),\n",
    "        \"next_columns_to_search\": unique_next_columns,\n",
    "        \"cte_transformations\": cte_transformations,\n",
    "        \"full_lineage\": target_column_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16160f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "create or replace temporary table \"PH_IDEA_PLAY\".\"AAR58466\".\"SNP_ETG__dbt_tmp\"\n",
    "\n",
    "         as\n",
    "\n",
    "        (with snapshot_query as (\n",
    "\n",
    " \n",
    "\n",
    "       \n",
    "\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "SELECT *\n",
    "\n",
    "  FROM ph_idea_play.aar58466.wrk_etg_final\n",
    "\n",
    " \n",
    "\n",
    "    ),\n",
    "\n",
    " \n",
    "\n",
    "    snapshotted_data as (\n",
    "\n",
    " \n",
    "\n",
    "        select *,\n",
    "\n",
    "            etg_cd as dbt_unique_key\n",
    "\n",
    " \n",
    "\n",
    "        from \"PH_IDEA_PLAY\".\"AAR58466\".\"SNP_ETG\"\n",
    "\n",
    "        where dbt_valid_to is null\n",
    "\n",
    " \n",
    "\n",
    "    ),\n",
    "\n",
    " \n",
    "\n",
    "    insertions_source_data as (\n",
    "\n",
    " \n",
    "\n",
    "        select\n",
    "\n",
    "            *,\n",
    "\n",
    "            etg_cd as dbt_unique_key,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_updated_at,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_valid_from,\n",
    "\n",
    "            nullif(to_timestamp_ntz(convert_timezone('UTC', current_timestamp())), to_timestamp_ntz(convert_timezone('UTC', current_timestamp()))) as dbt_valid_to,\n",
    "\n",
    "            md5(coalesce(cast(etg_cd as varchar ), '')\n",
    "\n",
    "         || '|' || coalesce(cast(to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as varchar ), '')\n",
    "\n",
    "        ) as dbt_scd_id\n",
    "\n",
    " \n",
    "\n",
    "        from snapshot_query\n",
    "\n",
    "    ),\n",
    "\n",
    " \n",
    "\n",
    "    updates_source_data as (\n",
    "\n",
    " \n",
    "\n",
    "        select\n",
    "\n",
    "            *,\n",
    "\n",
    "            etg_cd as dbt_unique_key,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_updated_at,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_valid_from,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_valid_to\n",
    "\n",
    " \n",
    "\n",
    "        from snapshot_query\n",
    "\n",
    "    ),\n",
    "\n",
    " \n",
    "\n",
    "    deletes_source_data as (\n",
    "\n",
    " \n",
    "\n",
    "        select\n",
    "\n",
    "            *,\n",
    "\n",
    "            etg_cd as dbt_unique_key\n",
    "\n",
    "        from snapshot_query\n",
    "\n",
    "    ),\n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "    insertions as (\n",
    "\n",
    " \n",
    "\n",
    "        select\n",
    "\n",
    "            'insert' as dbt_change_type,\n",
    "\n",
    "            source_data.*\n",
    "\n",
    " \n",
    "\n",
    "        from insertions_source_data as source_data\n",
    "\n",
    "        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key\n",
    "\n",
    "        where snapshotted_data.dbt_unique_key is null\n",
    "\n",
    "           or (\n",
    "\n",
    "                snapshotted_data.dbt_unique_key is not null\n",
    "\n",
    "            and (\n",
    "\n",
    "                (snapshotted_data.\"ETG_CD\" != source_data.\"ETG_CD\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_CD\" is null) and not (source_data.\"ETG_CD\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_CD\" is null) and (source_data.\"ETG_CD\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_DESC\" != source_data.\"ETG_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_DESC\" is null) and not (source_data.\"ETG_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_DESC\" is null) and (source_data.\"ETG_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_SHORT_DESC\" != source_data.\"ETG_SHORT_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_SHORT_DESC\" is null) and not (source_data.\"ETG_SHORT_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_SHORT_DESC\" is null) and (source_data.\"ETG_SHORT_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_BASE_CLASS_CD\" != source_data.\"ETG_BASE_CLASS_CD\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_BASE_CLASS_CD\" is null) and not (source_data.\"ETG_BASE_CLASS_CD\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_BASE_CLASS_CD\" is null) and (source_data.\"ETG_BASE_CLASS_CD\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_BASE_CLASS_DESC\" != source_data.\"ETG_BASE_CLASS_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_BASE_CLASS_DESC\" is null) and not (source_data.\"ETG_BASE_CLASS_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_BASE_CLASS_DESC\" is null) and (source_data.\"ETG_BASE_CLASS_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"MED_CLEAN_DAYS_NUM\" != source_data.\"MED_CLEAN_DAYS_NUM\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"MED_CLEAN_DAYS_NUM\" is null) and not (source_data.\"MED_CLEAN_DAYS_NUM\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"MED_CLEAN_DAYS_NUM\" is null) and (source_data.\"MED_CLEAN_DAYS_NUM\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"DRUG_PRE_EPI_DAYS_NUM\" != source_data.\"DRUG_PRE_EPI_DAYS_NUM\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null) and not (source_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null) and (source_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"DRUG_POST_EPI_DAYS_NUM\" != source_data.\"DRUG_POST_EPI_DAYS_NUM\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"DRUG_POST_EPI_DAYS_NUM\" is null) and not (source_data.\"DRUG_POST_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"DRUG_POST_EPI_DAYS_NUM\" is null) and (source_data.\"DRUG_POST_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"CHRONIC_CONDITION_BOOL\" != source_data.\"CHRONIC_CONDITION_BOOL\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"CHRONIC_CONDITION_BOOL\" is null) and not (source_data.\"CHRONIC_CONDITION_BOOL\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"CHRONIC_CONDITION_BOOL\" is null) and (source_data.\"CHRONIC_CONDITION_BOOL\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_ADJUSTED_COMM_BOOL\" != source_data.\"SEV_ADJUSTED_COMM_BOOL\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_ADJUSTED_COMM_BOOL\" is null) and not (source_data.\"SEV_ADJUSTED_COMM_BOOL\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_ADJUSTED_COMM_BOOL\" is null) and (source_data.\"SEV_ADJUSTED_COMM_BOOL\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_COMM_1_FCTR\" != source_data.\"SEV_THRESHOLD_COMM_1_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_COMM_2_FCTR\" != source_data.\"SEV_THRESHOLD_COMM_2_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_COMM_3_FCTR\" != source_data.\"SEV_THRESHOLD_COMM_3_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_LEVEL_COMM_CNT\" != source_data.\"SEV_LEVEL_COMM_CNT\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_LEVEL_COMM_CNT\" is null) and not (source_data.\"SEV_LEVEL_COMM_CNT\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_LEVEL_COMM_CNT\" is null) and (source_data.\"SEV_LEVEL_COMM_CNT\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" != source_data.\"SEV_ADJUSTED_MEDICARE_BOOL\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null) and not (source_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null) and (source_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" != source_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" != source_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" != source_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_LEVEL_MEDICARE_CNT\" != source_data.\"SEV_LEVEL_MEDICARE_CNT\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_LEVEL_MEDICARE_CNT\" is null) and not (source_data.\"SEV_LEVEL_MEDICARE_CNT\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_LEVEL_MEDICARE_CNT\" is null) and (source_data.\"SEV_LEVEL_MEDICARE_CNT\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"MPC_CD\" != source_data.\"MPC_CD\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"MPC_CD\" is null) and not (source_data.\"MPC_CD\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"MPC_CD\" is null) and (source_data.\"MPC_CD\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"MPC_DESC\" != source_data.\"MPC_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"MPC_DESC\" is null) and not (source_data.\"MPC_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"MPC_DESC\" is null) and (source_data.\"MPC_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_PH_EXTERNAL_DESC\" != source_data.\"ETG_PH_EXTERNAL_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_PH_EXTERNAL_DESC\" is null) and not (source_data.\"ETG_PH_EXTERNAL_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_PH_EXTERNAL_DESC\" is null) and (source_data.\"ETG_PH_EXTERNAL_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_PH_INTERNAL_DESC\" != source_data.\"ETG_PH_INTERNAL_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_PH_INTERNAL_DESC\" is null) and not (source_data.\"ETG_PH_INTERNAL_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_PH_INTERNAL_DESC\" is null) and (source_data.\"ETG_PH_INTERNAL_DESC\" is null))\n",
    "\n",
    "        ))\n",
    "\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    " \n",
    "\n",
    "    ),\n",
    "\n",
    " \n",
    "\n",
    "    updates as (\n",
    "\n",
    " \n",
    "\n",
    "        select\n",
    "\n",
    "            'update' as dbt_change_type,\n",
    "\n",
    "            source_data.*,\n",
    "\n",
    "            snapshotted_data.dbt_scd_id\n",
    "\n",
    " \n",
    "\n",
    "        from updates_source_data as source_data\n",
    "\n",
    "        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key\n",
    "\n",
    "        where (\n",
    "\n",
    "            (snapshotted_data.\"ETG_CD\" != source_data.\"ETG_CD\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_CD\" is null) and not (source_data.\"ETG_CD\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_CD\" is null) and (source_data.\"ETG_CD\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_DESC\" != source_data.\"ETG_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_DESC\" is null) and not (source_data.\"ETG_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_DESC\" is null) and (source_data.\"ETG_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_SHORT_DESC\" != source_data.\"ETG_SHORT_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_SHORT_DESC\" is null) and not (source_data.\"ETG_SHORT_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_SHORT_DESC\" is null) and (source_data.\"ETG_SHORT_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_BASE_CLASS_CD\" != source_data.\"ETG_BASE_CLASS_CD\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_BASE_CLASS_CD\" is null) and not (source_data.\"ETG_BASE_CLASS_CD\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_BASE_CLASS_CD\" is null) and (source_data.\"ETG_BASE_CLASS_CD\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_BASE_CLASS_DESC\" != source_data.\"ETG_BASE_CLASS_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_BASE_CLASS_DESC\" is null) and not (source_data.\"ETG_BASE_CLASS_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_BASE_CLASS_DESC\" is null) and (source_data.\"ETG_BASE_CLASS_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"MED_CLEAN_DAYS_NUM\" != source_data.\"MED_CLEAN_DAYS_NUM\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"MED_CLEAN_DAYS_NUM\" is null) and not (source_data.\"MED_CLEAN_DAYS_NUM\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"MED_CLEAN_DAYS_NUM\" is null) and (source_data.\"MED_CLEAN_DAYS_NUM\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"DRUG_PRE_EPI_DAYS_NUM\" != source_data.\"DRUG_PRE_EPI_DAYS_NUM\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null) and not (source_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null) and (source_data.\"DRUG_PRE_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"DRUG_POST_EPI_DAYS_NUM\" != source_data.\"DRUG_POST_EPI_DAYS_NUM\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"DRUG_POST_EPI_DAYS_NUM\" is null) and not (source_data.\"DRUG_POST_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"DRUG_POST_EPI_DAYS_NUM\" is null) and (source_data.\"DRUG_POST_EPI_DAYS_NUM\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"CHRONIC_CONDITION_BOOL\" != source_data.\"CHRONIC_CONDITION_BOOL\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"CHRONIC_CONDITION_BOOL\" is null) and not (source_data.\"CHRONIC_CONDITION_BOOL\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"CHRONIC_CONDITION_BOOL\" is null) and (source_data.\"CHRONIC_CONDITION_BOOL\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_ADJUSTED_COMM_BOOL\" != source_data.\"SEV_ADJUSTED_COMM_BOOL\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_ADJUSTED_COMM_BOOL\" is null) and not (source_data.\"SEV_ADJUSTED_COMM_BOOL\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_ADJUSTED_COMM_BOOL\" is null) and (source_data.\"SEV_ADJUSTED_COMM_BOOL\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_COMM_1_FCTR\" != source_data.\"SEV_THRESHOLD_COMM_1_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_COMM_1_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_COMM_2_FCTR\" != source_data.\"SEV_THRESHOLD_COMM_2_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_COMM_2_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_COMM_3_FCTR\" != source_data.\"SEV_THRESHOLD_COMM_3_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_COMM_3_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_LEVEL_COMM_CNT\" != source_data.\"SEV_LEVEL_COMM_CNT\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_LEVEL_COMM_CNT\" is null) and not (source_data.\"SEV_LEVEL_COMM_CNT\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_LEVEL_COMM_CNT\" is null) and (source_data.\"SEV_LEVEL_COMM_CNT\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" != source_data.\"SEV_ADJUSTED_MEDICARE_BOOL\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null) and not (source_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null) and (source_data.\"SEV_ADJUSTED_MEDICARE_BOOL\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" != source_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_MEDICARE_1_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" != source_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_MEDICARE_2_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" != source_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null) and not (source_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null) and (source_data.\"SEV_THRESHOLD_MEDICARE_3_FCTR\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"SEV_LEVEL_MEDICARE_CNT\" != source_data.\"SEV_LEVEL_MEDICARE_CNT\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"SEV_LEVEL_MEDICARE_CNT\" is null) and not (source_data.\"SEV_LEVEL_MEDICARE_CNT\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"SEV_LEVEL_MEDICARE_CNT\" is null) and (source_data.\"SEV_LEVEL_MEDICARE_CNT\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"MPC_CD\" != source_data.\"MPC_CD\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"MPC_CD\" is null) and not (source_data.\"MPC_CD\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"MPC_CD\" is null) and (source_data.\"MPC_CD\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"MPC_DESC\" != source_data.\"MPC_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"MPC_DESC\" is null) and not (source_data.\"MPC_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"MPC_DESC\" is null) and (source_data.\"MPC_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_PH_EXTERNAL_DESC\" != source_data.\"ETG_PH_EXTERNAL_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_PH_EXTERNAL_DESC\" is null) and not (source_data.\"ETG_PH_EXTERNAL_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_PH_EXTERNAL_DESC\" is null) and (source_data.\"ETG_PH_EXTERNAL_DESC\" is null))\n",
    "\n",
    "        ) or snapshotted_data.\"ETG_PH_INTERNAL_DESC\" != source_data.\"ETG_PH_INTERNAL_DESC\"\n",
    "\n",
    "        or\n",
    "\n",
    "        (\n",
    "\n",
    "            ((snapshotted_data.\"ETG_PH_INTERNAL_DESC\" is null) and not (source_data.\"ETG_PH_INTERNAL_DESC\" is null))\n",
    "\n",
    "            or\n",
    "\n",
    "            ((not snapshotted_data.\"ETG_PH_INTERNAL_DESC\" is null) and (source_data.\"ETG_PH_INTERNAL_DESC\" is null))\n",
    "\n",
    "        ))\n",
    "\n",
    "        )\n",
    "\n",
    "    ),\n",
    "\n",
    " \n",
    "\n",
    "    deletes as (\n",
    "\n",
    " \n",
    "\n",
    "        select\n",
    "\n",
    "            'delete' as dbt_change_type,\n",
    "\n",
    "            source_data.*,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_valid_from,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_updated_at,\n",
    "\n",
    "            to_timestamp_ntz(convert_timezone('UTC', current_timestamp())) as dbt_valid_to,\n",
    "\n",
    "            snapshotted_data.dbt_scd_id\n",
    "\n",
    " \n",
    "\n",
    "        from snapshotted_data\n",
    "\n",
    "        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key\n",
    "\n",
    "        where source_data.dbt_unique_key is null\n",
    "\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    select * from insertions\n",
    "\n",
    "    union all\n",
    "\n",
    "    select * from updates\n",
    "\n",
    "    union all\n",
    "\n",
    "    select * from deletes\n",
    "\n",
    " \n",
    "\n",
    "        );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10772007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': \"Column 'etg_cd' not found in query output\",\n",
       " 'llm_context': \"The column 'etg_cd' was not found in the final query output.\",\n",
       " 'next_columns_to_search': [],\n",
       " 'full_lineage': {}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_column_lineage(query,\"etg_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0cc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
