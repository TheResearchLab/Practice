{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc022a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TEST QUERY 1\n",
      "====================================================================================================\n",
      "SQL:\n",
      "\n",
      "    WITH __dbt__cte__dummy_data AS (\n",
      "        SELECT\n",
      "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
      "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
      "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
      "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
      "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
      "        FROM dummy_schema.dummy_table\n",
      "    ),\n",
      "    get_dummy_data AS (\n",
      "        SELECT\n",
      "            dummy_ver_name,\n",
      "            dummy_pop_name,\n",
      "            dummy_level_cd,\n",
      "            dummy_var_name,\n",
      "            dummy_coef\n",
      "        FROM __dbt__cte__dummy_data\n",
      "    )\n",
      "    SELECT\n",
      "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
      "        dd.dummy_key,\n",
      "        gd.dummy_ver_name,\n",
      "        gd.dummy_pop_name,\n",
      "        gd.dummy_level_cd,\n",
      "        gd.dummy_var_name,\n",
      "        gd.dummy_coef\n",
      "    FROM get_dummy_data gd\n",
      "    INNER JOIN dummy_schema.dummy_dim dd ON gd.dummy_ver_name = dd.dummy_ver_name\n",
      "    \n",
      "\n",
      "BASIC COLUMN ANALYSIS:\n",
      "--------------------------------------------------\n",
      "SELECT branch 1:\n",
      "  dummy_id: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id (Type: calculated)\n",
      "  dummy_key: dd.dummy_key (Type: direct)\n",
      "  dummy_ver_name: gd.dummy_ver_name (Type: direct)\n",
      "  dummy_pop_name: gd.dummy_pop_name (Type: direct)\n",
      "  dummy_level_cd: gd.dummy_level_cd (Type: direct)\n",
      "  dummy_var_name: gd.dummy_var_name (Type: direct)\n",
      "  dummy_coef: gd.dummy_coef (Type: direct)\n",
      "SELECT branch 2:\n",
      "  dummy_ver_name: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VER_NAME') AS TEXT), '')) AS dummy_ver_name (Type: calculated)\n",
      "  dummy_pop_name: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_POP_NAME') AS TEXT), '')) AS dummy_pop_name (Type: calculated)\n",
      "  dummy_level_cd: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_LEVEL_CD') AS TEXT), '')) AS dummy_level_cd (Type: calculated)\n",
      "  dummy_var_name: UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VAR_NAME') AS TEXT), '')) AS dummy_var_name (Type: calculated)\n",
      "  dummy_coef: CAST(NULLIF(CAST(GET_PATH(v, 'DUMMY_COEF') AS TEXT), '') AS DECIMAL(8, 3)) AS dummy_coef (Type: calculated)\n",
      "SELECT branch 3:\n",
      "  dummy_ver_name: dummy_ver_name (Type: direct)\n",
      "  dummy_pop_name: dummy_pop_name (Type: direct)\n",
      "  dummy_level_cd: dummy_level_cd (Type: direct)\n",
      "  dummy_var_name: dummy_var_name (Type: direct)\n",
      "  dummy_coef: dummy_coef (Type: direct)\n",
      "\n",
      "FINAL OUTPUT COLUMNS: ['dummy_id', 'dummy_key', 'dummy_ver_name', 'dummy_pop_name', 'dummy_level_cd', 'dummy_var_name', 'dummy_coef']\n",
      "\n",
      "TRACING LINEAGE FOR COLUMN: dummy_id\n",
      "--------------------------------------------------\n",
      "LLM CONTEXT:\n",
      "COLUMN: dummy_id\n",
      "EXPRESSION: COALESCE(CAST(gd.dummy_ver_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_pop_name AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_level_cd AS VARCHAR), '') || '~' || COALESCE(CAST(gd.dummy_var_name AS VARCHAR), '') AS dummy_id\n",
      "TRANSFORMATION TYPE: complex\n",
      "TRANSFORMATION DETAILS: Complex expression: Alias\n",
      "FOUND IN: SELECT branch 1\n",
      "DATA DEPENDENCIES: gd.dummy_var_name, gd.dummy_level_cd, gd.dummy_pop_name, gd.dummy_ver_name\n",
      "\n",
      "LINEAGE TRACE:\n",
      "\n",
      "Source: get_dummy_data.dummy_level_cd\n",
      "Resolved as: gd.dummy_level_cd\n",
      "  └─ BASE TABLE: dummy_schema.dummy_table.v\n",
      "  └─ CTE '__dbt__cte__dummy_data': UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_LEVEL_CD') AS TEXT), '')) AS dummy_level_cd\n",
      "     Transformation: calculated\n",
      "  └─ CTE 'get_dummy_data': dummy_level_cd\n",
      "     Transformation: direct\n",
      "\n",
      "Source: get_dummy_data.dummy_ver_name\n",
      "Resolved as: gd.dummy_ver_name\n",
      "  └─ BASE TABLE: dummy_schema.dummy_table.v\n",
      "  └─ CTE '__dbt__cte__dummy_data': UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VER_NAME') AS TEXT), '')) AS dummy_ver_name\n",
      "     Transformation: calculated\n",
      "  └─ CTE 'get_dummy_data': dummy_ver_name\n",
      "     Transformation: direct\n",
      "\n",
      "Source: get_dummy_data.dummy_var_name\n",
      "Resolved as: gd.dummy_var_name\n",
      "  └─ BASE TABLE: dummy_schema.dummy_table.v\n",
      "  └─ CTE '__dbt__cte__dummy_data': UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VAR_NAME') AS TEXT), '')) AS dummy_var_name\n",
      "     Transformation: calculated\n",
      "  └─ CTE 'get_dummy_data': dummy_var_name\n",
      "     Transformation: direct\n",
      "\n",
      "Source: get_dummy_data.dummy_pop_name\n",
      "Resolved as: gd.dummy_pop_name\n",
      "  └─ BASE TABLE: dummy_schema.dummy_table.v\n",
      "  └─ CTE '__dbt__cte__dummy_data': UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_POP_NAME') AS TEXT), '')) AS dummy_pop_name\n",
      "     Transformation: calculated\n",
      "  └─ CTE 'get_dummy_data': dummy_pop_name\n",
      "     Transformation: direct\n",
      "\n",
      "RELEVANT CTEs:\n",
      "\n",
      "CTE 'get_dummy_data':\n",
      "SELECT dummy_ver_name, dummy_pop_name, dummy_level_cd, dummy_var_name, dummy_coef FROM __dbt__cte__dummy_data\n",
      "\n",
      "CTE '__dbt__cte__dummy_data':\n",
      "SELECT UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VER_NAME') AS TEXT), '')) AS dummy_ver_name, UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_POP_NAME') AS TEXT), '')) AS dummy_pop_name, UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_LEVEL_CD') AS TEXT), '')) AS dummy_level_cd, UPPER(NULLIF(CAST(GET_PATH(v, 'DUMMY_VAR_NAME') AS TEXT), '')) AS dummy_var_name, CAST(NULLIF(CAST(GET_PATH(v, 'DUMMY_COEF') AS TEXT), '') AS DECIMAL(8, 3)) AS dummy_coef FROM dummy_schema.dummy_table\n",
      "\n",
      "NEXT COLUMNS TO SEARCH (13):\n",
      "  BASE TABLE:\n",
      "    - dummy_schema.dummy_table.v\n",
      "  EXPRESSION DEPENDENCY:\n",
      "    - get_dummy_data.dummy_var_name\n",
      "    - get_dummy_data.dummy_level_cd\n",
      "    - get_dummy_data.dummy_pop_name\n",
      "    - get_dummy_data.dummy_ver_name\n",
      "  CTE INTERMEDIATE:\n",
      "    - __dbt__cte__dummy_data.dummy_level_cd\n",
      "    - get_dummy_data.dummy_level_cd\n",
      "    - __dbt__cte__dummy_data.dummy_ver_name\n",
      "    - get_dummy_data.dummy_ver_name\n",
      "    - __dbt__cte__dummy_data.dummy_var_name\n",
      "    - get_dummy_data.dummy_var_name\n",
      "    - __dbt__cte__dummy_data.dummy_pop_name\n",
      "    - get_dummy_data.dummy_pop_name\n",
      "\n",
      "COMPLETE CONTEXT WITH EXTERNAL DATA:\n",
      "--------------------------------------------------\n",
      "Column 'dummy_id' is derived from 4 intermediate column(s) which ultimately trace to 1 base column(s) in 1 table(s): dummy_schema.dummy_table\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "\n",
    "def extract_snowflake_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extracts column lineage information from a Snowflake SQL query.\n",
    "    Returns a list of lists, each describing the output columns for each SELECT.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "\n",
    "    def expr_to_str(expr):\n",
    "        return expr.sql(dialect=\"snowflake\") if expr else None\n",
    "\n",
    "    def collect_source_columns(expr):\n",
    "        sources = set()\n",
    "        for node in expr.walk():\n",
    "            if isinstance(node, exp.Column):\n",
    "                # Get table alias/name - could be empty string\n",
    "                table_ref = node.table if node.table else \"\"\n",
    "                sources.add((table_ref, node.name))\n",
    "        return list(sources)\n",
    "\n",
    "    # Helper: get all tables in the FROM clause of a SELECT\n",
    "    def get_from_tables(select):\n",
    "        \"\"\"\n",
    "        Returns a dict mapping alias (lowercase) -> (full_table_name, alias)\n",
    "        \"\"\"\n",
    "        tables = {}\n",
    "        \n",
    "        from_expr = select.args.get(\"from\")\n",
    "        if from_expr:\n",
    "            # Base table\n",
    "            base = from_expr.args.get(\"this\")\n",
    "            if isinstance(base, exp.Table):\n",
    "                db = base.catalog or \"\"\n",
    "                schema = base.db or \"\"\n",
    "                name = base.name\n",
    "                if db and schema:\n",
    "                    full_name = f\"{db}.{schema}.{name}\"\n",
    "                elif schema:\n",
    "                    full_name = f\"{schema}.{name}\"\n",
    "                else:\n",
    "                    full_name = name\n",
    "                alias = base.alias or name\n",
    "                tables[alias.lower()] = (full_name, alias)\n",
    "        \n",
    "        # JOINs are stored at the SELECT level, not FROM level\n",
    "        joins = select.args.get(\"joins\")\n",
    "        if joins:\n",
    "            for join in joins:\n",
    "                join_table = join.args.get(\"this\")\n",
    "                if isinstance(join_table, exp.Table):\n",
    "                    db = join_table.catalog or \"\"\n",
    "                    schema = join_table.db or \"\"\n",
    "                    name = join_table.name\n",
    "                    if db and schema:\n",
    "                        full_name = f\"{db}.{schema}.{name}\"\n",
    "                    elif schema:\n",
    "                        full_name = f\"{schema}.{name}\"\n",
    "                    else:\n",
    "                        full_name = name\n",
    "                    alias = join_table.alias or name\n",
    "                    tables[alias.lower()] = (full_name, alias)\n",
    "                        \n",
    "        return tables\n",
    "    \n",
    "    selects = [node for node in parsed.walk() if isinstance(node, exp.Select)]\n",
    "    all_columns = []\n",
    "    \n",
    "    for idx, select in enumerate(selects):\n",
    "        select_columns = []\n",
    "        from_tables = get_from_tables(select)\n",
    "        only_table = list(from_tables.values())[0][0] if len(from_tables) == 1 else None\n",
    "        \n",
    "        for proj in select.expressions:\n",
    "            alias = proj.alias_or_name\n",
    "            expression_sql = expr_to_str(proj)\n",
    "            source_columns = collect_source_columns(proj)\n",
    "            resolved_sources = []\n",
    "            \n",
    "            for tbl_alias, col_name in source_columns:\n",
    "                if not tbl_alias and only_table:\n",
    "                    # No table alias and only one table - use that table\n",
    "                    resolved_sources.append((only_table, col_name))\n",
    "                elif tbl_alias:\n",
    "                    # Has table alias - look it up in from_tables\n",
    "                    tbl_alias_lc = tbl_alias.lower()\n",
    "                    if tbl_alias_lc in from_tables:\n",
    "                        full_table, real_alias = from_tables[tbl_alias_lc]\n",
    "                        resolved_sources.append((full_table, real_alias, col_name))\n",
    "                    else:\n",
    "                        # Alias not found in from_tables - keep as is\n",
    "                        resolved_sources.append((tbl_alias, col_name))\n",
    "                else:\n",
    "                    # No table alias and multiple tables - ambiguous\n",
    "                    resolved_sources.append((tbl_alias, col_name))\n",
    "            \n",
    "            # Determine column type\n",
    "            if isinstance(proj, exp.Column):\n",
    "                col_type = \"direct\"\n",
    "            elif proj.is_star:\n",
    "                col_type = \"star\"\n",
    "            elif not source_columns:\n",
    "                col_type = \"constant\"\n",
    "            else:\n",
    "                col_type = \"calculated\"\n",
    "            \n",
    "            select_columns.append({\n",
    "                \"select_idx\": idx,\n",
    "                \"target_column\": alias,\n",
    "                \"expression\": expression_sql,\n",
    "                \"source_columns\": source_columns,\n",
    "                \"resolved_source_columns\": resolved_sources,\n",
    "                \"type\": col_type\n",
    "            })\n",
    "        \n",
    "        all_columns.append(select_columns)\n",
    "    \n",
    "    return all_columns\n",
    "\n",
    "\n",
    "def analyze_column_transformations(sql_query):\n",
    "    \"\"\"\n",
    "    Analyzes column transformations and lineage through CTEs and complex expressions.\n",
    "    Returns detailed transformation information including data flow and control flow dependencies.\n",
    "    \"\"\"\n",
    "    parsed = sqlglot.parse_one(sql_query, dialect=\"snowflake\")\n",
    "    \n",
    "    # Step 1: Build CTE registry\n",
    "    def build_cte_registry(parsed_query):\n",
    "        \"\"\"Map CTE names to their SELECT definitions\"\"\"\n",
    "        cte_registry = {}\n",
    "        \n",
    "        # Find WITH clause\n",
    "        with_clause = parsed_query.args.get(\"with\")\n",
    "        if with_clause:\n",
    "            for cte in with_clause.expressions:\n",
    "                cte_name = cte.alias\n",
    "                cte_query = cte.this  # The SELECT part of the CTE\n",
    "                cte_registry[cte_name.lower()] = cte_query\n",
    "        \n",
    "        return cte_registry\n",
    "    \n",
    "    # Step 2: Analyze expressions for complex transformations\n",
    "    def analyze_expression(expr, available_tables):\n",
    "        \"\"\"\n",
    "        Analyze an expression to determine its transformation type and dependencies\n",
    "        \"\"\"\n",
    "        if isinstance(expr, exp.Column):\n",
    "            # Simple column reference\n",
    "            table_ref = expr.table if expr.table else \"\"\n",
    "            return {\n",
    "                \"type\": \"direct\",\n",
    "                \"expression\": expr.sql(dialect=\"snowflake\"),\n",
    "                \"data_flow_deps\": [(table_ref, expr.name)],\n",
    "                \"control_flow_deps\": [],\n",
    "                \"transformation_details\": \"Direct column reference\"\n",
    "            }\n",
    "        \n",
    "        elif isinstance(expr, exp.Binary):\n",
    "            # Arithmetic operations: a + b, a - b, etc.\n",
    "            left_analysis = analyze_expression(expr.left, available_tables)\n",
    "            right_analysis = analyze_expression(expr.right, available_tables)\n",
    "            \n",
    "            return {\n",
    "                \"type\": \"calculated\",\n",
    "                \"expression\": expr.sql(dialect=\"snowflake\"),\n",
    "                \"data_flow_deps\": left_analysis[\"data_flow_deps\"] + right_analysis[\"data_flow_deps\"],\n",
    "                \"control_flow_deps\": left_analysis[\"control_flow_deps\"] + right_analysis[\"control_flow_deps\"],\n",
    "                \"transformation_details\": f\"Binary operation: {expr.key}\"\n",
    "            }\n",
    "        \n",
    "        elif isinstance(expr, exp.Case):\n",
    "            # CASE WHEN statements\n",
    "            deps = []\n",
    "            control_deps = []\n",
    "            \n",
    "            # Analyze all WHEN conditions and values\n",
    "            for case in expr.args.get(\"ifs\", []):\n",
    "                condition_analysis = analyze_expression(case.this, available_tables)  # WHEN condition\n",
    "                value_analysis = analyze_expression(case.expression, available_tables)  # THEN value\n",
    "                \n",
    "                control_deps.extend(condition_analysis[\"data_flow_deps\"])  # Conditions are control flow\n",
    "                deps.extend(value_analysis[\"data_flow_deps\"])  # Values are data flow\n",
    "            \n",
    "            # Handle ELSE clause\n",
    "            if expr.args.get(\"default\"):\n",
    "                else_analysis = analyze_expression(expr.args.get(\"default\"), available_tables)\n",
    "                deps.extend(else_analysis[\"data_flow_deps\"])\n",
    "            \n",
    "            return {\n",
    "                \"type\": \"conditional\",\n",
    "                \"expression\": expr.sql(dialect=\"snowflake\"),\n",
    "                \"data_flow_deps\": deps,\n",
    "                \"control_flow_deps\": control_deps,\n",
    "                \"transformation_details\": \"CASE WHEN conditional logic\"\n",
    "            }\n",
    "        \n",
    "        elif isinstance(expr, exp.Window):\n",
    "            # Window functions: ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...)\n",
    "            func_analysis = analyze_expression(expr.this, available_tables) if expr.this else {\"data_flow_deps\": [], \"control_flow_deps\": []}\n",
    "            \n",
    "            partition_deps = []\n",
    "            order_deps = []\n",
    "            \n",
    "            # PARTITION BY columns are control flow\n",
    "            partition_by = expr.args.get(\"partition_by\")\n",
    "            if partition_by:\n",
    "                for partition_col in partition_by:\n",
    "                    part_analysis = analyze_expression(partition_col, available_tables)\n",
    "                    partition_deps.extend(part_analysis[\"data_flow_deps\"])\n",
    "            \n",
    "            # ORDER BY columns are control flow\n",
    "            order_by = expr.args.get(\"order\")\n",
    "            if order_by:\n",
    "                for order_col in order_by.expressions:\n",
    "                    order_analysis = analyze_expression(order_col, available_tables)\n",
    "                    order_deps.extend(order_analysis[\"data_flow_deps\"])\n",
    "            \n",
    "            return {\n",
    "                \"type\": \"window_function\",\n",
    "                \"expression\": expr.sql(dialect=\"snowflake\"),\n",
    "                \"data_flow_deps\": func_analysis.get(\"data_flow_deps\", []),\n",
    "                \"control_flow_deps\": partition_deps + order_deps,\n",
    "                \"transformation_details\": f\"Window function with partitioning and ordering\",\n",
    "                \"partition_by\": partition_deps,\n",
    "                \"order_by\": order_deps\n",
    "            }\n",
    "        \n",
    "        elif isinstance(expr, exp.Func):\n",
    "            # Aggregate functions: SUM, COUNT, etc.\n",
    "            deps = []\n",
    "            for arg in expr.expressions:\n",
    "                arg_analysis = analyze_expression(arg, available_tables)\n",
    "                deps.extend(arg_analysis[\"data_flow_deps\"])\n",
    "            \n",
    "            return {\n",
    "                \"type\": \"aggregated\",\n",
    "                \"expression\": expr.sql(dialect=\"snowflake\"),\n",
    "                \"data_flow_deps\": deps,\n",
    "                \"control_flow_deps\": [],\n",
    "                \"transformation_details\": f\"Aggregate function: {expr.key}\"\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # Fallback for other expression types\n",
    "            deps = []\n",
    "            for node in expr.walk():\n",
    "                if isinstance(node, exp.Column):\n",
    "                    table_ref = node.table if node.table else \"\"\n",
    "                    deps.append((table_ref, node.name))\n",
    "            \n",
    "            return {\n",
    "                \"type\": \"complex\",\n",
    "                \"expression\": expr.sql(dialect=\"snowflake\"),\n",
    "                \"data_flow_deps\": deps,\n",
    "                \"control_flow_deps\": [],\n",
    "                \"transformation_details\": f\"Complex expression: {type(expr).__name__}\"\n",
    "            }\n",
    "    \n",
    "    # Step 3: Resolve CTE references recursively\n",
    "    def resolve_cte_lineage(table_ref, column_name, cte_registry, visited=None):\n",
    "        \"\"\"\n",
    "        Recursively resolve a column reference through CTEs back to source tables\n",
    "        \"\"\"\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        \n",
    "        if table_ref in visited:\n",
    "            return [{\"error\": f\"Circular reference detected in CTE: {table_ref}\"}]\n",
    "        \n",
    "        if table_ref.lower() not in cte_registry:\n",
    "            # This is a base table, not a CTE\n",
    "            return [{\n",
    "                \"source_table\": table_ref,\n",
    "                \"source_column\": column_name,\n",
    "                \"transformation_type\": \"source\",\n",
    "                \"step\": \"base_table\"\n",
    "            }]\n",
    "        \n",
    "        visited.add(table_ref)\n",
    "        cte_query = cte_registry[table_ref.lower()]\n",
    "        \n",
    "        # Get column information from the CTE\n",
    "        cte_columns = extract_snowflake_columns(cte_query.sql(dialect=\"snowflake\"))\n",
    "        \n",
    "        lineage_chain = []\n",
    "        \n",
    "        # Find the column in the CTE's output\n",
    "        for select_branch in cte_columns:\n",
    "            for col_info in select_branch:\n",
    "                if col_info[\"target_column\"].lower() == column_name.lower():\n",
    "                    # Found the column, now trace its sources\n",
    "                    cte_step = {\n",
    "                        \"cte_name\": table_ref,\n",
    "                        \"transformation_type\": col_info[\"type\"],\n",
    "                        \"expression\": col_info[\"expression\"],\n",
    "                        \"step\": f\"cte_{table_ref}\"\n",
    "                    }\n",
    "                    \n",
    "                    # Recursively resolve each resolved source column (not just source_columns)\n",
    "                    resolved_sources = col_info.get(\"resolved_source_columns\", [])\n",
    "                    if resolved_sources:\n",
    "                        for resolved_source in resolved_sources:\n",
    "                            if len(resolved_source) >= 3:  # (full_table, alias, column)\n",
    "                                source_table, alias, source_col = resolved_source[:3]\n",
    "                                upstream_lineage = resolve_cte_lineage(source_table, source_col, cte_registry, visited.copy())\n",
    "                                lineage_chain.extend(upstream_lineage)\n",
    "                            elif len(resolved_source) == 2:  # (table, column)\n",
    "                                source_table, source_col = resolved_source\n",
    "                                upstream_lineage = resolve_cte_lineage(source_table, source_col, cte_registry, visited.copy())\n",
    "                                lineage_chain.extend(upstream_lineage)\n",
    "                    else:\n",
    "                        # Fall back to basic source_columns\n",
    "                        for source_table, source_col in col_info[\"source_columns\"]:\n",
    "                            if source_table:\n",
    "                                upstream_lineage = resolve_cte_lineage(source_table, source_col, cte_registry, visited.copy())\n",
    "                                lineage_chain.extend(upstream_lineage)\n",
    "                    \n",
    "                    lineage_chain.append(cte_step)\n",
    "                    break\n",
    "        \n",
    "        return lineage_chain\n",
    "    \n",
    "    # Step 4: Main analysis\n",
    "    cte_registry = build_cte_registry(parsed)\n",
    "    base_columns = extract_snowflake_columns(sql_query)\n",
    "    \n",
    "    transformation_analysis = []\n",
    "    \n",
    "    for select_idx, select_columns in enumerate(base_columns):\n",
    "        select_analysis = {\n",
    "            \"select_branch\": select_idx + 1,\n",
    "            \"columns\": []\n",
    "        }\n",
    "        \n",
    "        for col_info in select_columns:\n",
    "            try:\n",
    "                # Get the expression analysis\n",
    "                parsed_expr = sqlglot.parse_one(col_info[\"expression\"], dialect=\"snowflake\")\n",
    "                \n",
    "                # Build available tables context (this would need the from_tables info)\n",
    "                available_tables = {}  # This should be populated with the table context\n",
    "                \n",
    "                expr_analysis = analyze_expression(parsed_expr, available_tables)\n",
    "                \n",
    "                # Build full lineage chain using resolved sources\n",
    "                lineage_chains = []\n",
    "                \n",
    "                # Use resolved_source_columns if available, otherwise fall back to source_columns\n",
    "                resolved_sources = col_info.get(\"resolved_source_columns\", [])\n",
    "                if resolved_sources:\n",
    "                    for resolved_source in resolved_sources:\n",
    "                        if len(resolved_source) >= 3:  # (full_table, alias, column)\n",
    "                            source_table, alias, source_col = resolved_source[:3]\n",
    "                            lineage = resolve_cte_lineage(source_table, source_col, cte_registry)\n",
    "                            lineage_chains.append({\n",
    "                                \"source_reference\": f\"{source_table}.{source_col}\",\n",
    "                                \"resolved_reference\": f\"{alias}.{source_col}\" if alias != source_table else f\"{source_table}.{source_col}\",\n",
    "                                \"lineage_chain\": lineage\n",
    "                            })\n",
    "                        elif len(resolved_source) == 2:  # (table, column)\n",
    "                            source_table, source_col = resolved_source\n",
    "                            lineage = resolve_cte_lineage(source_table, source_col, cte_registry)\n",
    "                            lineage_chains.append({\n",
    "                                \"source_reference\": f\"{source_table}.{source_col}\",\n",
    "                                \"resolved_reference\": f\"{source_table}.{source_col}\",\n",
    "                                \"lineage_chain\": lineage\n",
    "                            })\n",
    "                else:\n",
    "                    # Fall back to basic source_columns\n",
    "                    for source_table, source_col in col_info[\"source_columns\"]:\n",
    "                        if source_table:\n",
    "                            lineage = resolve_cte_lineage(source_table, source_col, cte_registry)\n",
    "                            lineage_chains.append({\n",
    "                                \"source_reference\": f\"{source_table}.{source_col}\",\n",
    "                                \"resolved_reference\": f\"{source_table}.{source_col}\",\n",
    "                                \"lineage_chain\": lineage\n",
    "                            })\n",
    "                \n",
    "                column_analysis = {\n",
    "                    \"target_column\": col_info[\"target_column\"],\n",
    "                    \"expression\": col_info[\"expression\"],\n",
    "                    \"basic_type\": col_info[\"type\"],\n",
    "                    \"transformation_analysis\": expr_analysis,\n",
    "                    \"lineage_chains\": lineage_chains,\n",
    "                    \"resolved_sources\": col_info.get(\"resolved_source_columns\", [])\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Fallback for expressions we can't parse\n",
    "                column_analysis = {\n",
    "                    \"target_column\": col_info[\"target_column\"],\n",
    "                    \"expression\": col_info[\"expression\"],\n",
    "                    \"basic_type\": col_info[\"type\"],\n",
    "                    \"transformation_analysis\": {\n",
    "                        \"type\": \"parse_error\",\n",
    "                        \"expression\": col_info[\"expression\"],\n",
    "                        \"data_flow_deps\": col_info[\"source_columns\"],\n",
    "                        \"control_flow_deps\": [],\n",
    "                        \"transformation_details\": f\"Parse error: {str(e)}\"\n",
    "                    },\n",
    "                    \"lineage_chains\": [],\n",
    "                    \"resolved_sources\": col_info.get(\"resolved_source_columns\", [])\n",
    "                }\n",
    "            \n",
    "            select_analysis[\"columns\"].append(column_analysis)\n",
    "        \n",
    "        transformation_analysis.append(select_analysis)\n",
    "    \n",
    "    return {\n",
    "        \"cte_registry\": {name: cte.sql(dialect=\"snowflake\") for name, cte in cte_registry.items()},\n",
    "        \"transformation_analysis\": transformation_analysis\n",
    "    }\n",
    "\n",
    "def trace_column_lineage(sql_query, target_column_name):\n",
    "    \"\"\"\n",
    "    Traces a specific column through all transformations and builds LLM-ready context.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: The SQL query to analyze\n",
    "        target_column_name: Name of the final output column to trace\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"llm_context\": \"Human-readable description of the column's journey\",\n",
    "            \"next_columns_to_search\": [{\"table\": \"table_name\", \"column\": \"column_name\"}],\n",
    "            \"full_lineage\": {...}  # Detailed technical lineage\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the transformation analysis\n",
    "    analysis = analyze_column_transformations(sql_query)\n",
    "    \n",
    "    # Find the target column in the final output\n",
    "    target_column_info = None\n",
    "    target_select_branch = None\n",
    "    \n",
    "    for select_analysis in analysis['transformation_analysis']:\n",
    "        for col_analysis in select_analysis['columns']:\n",
    "            if col_analysis['target_column'].lower() == target_column_name.lower():\n",
    "                target_column_info = col_analysis\n",
    "                target_select_branch = select_analysis['select_branch']\n",
    "                break\n",
    "        if target_column_info:\n",
    "            break\n",
    "    \n",
    "    if not target_column_info:\n",
    "        return {\n",
    "            \"error\": f\"Column '{target_column_name}' not found in query output\",\n",
    "            \"llm_context\": f\"The column '{target_column_name}' was not found in the final query output.\",\n",
    "            \"next_columns_to_search\": [],\n",
    "            \"full_lineage\": {}\n",
    "        }\n",
    "    \n",
    "    # Build LLM context\n",
    "    llm_context_parts = []\n",
    "    next_columns = []\n",
    "    \n",
    "    # Start with the target column description\n",
    "    expr_analysis = target_column_info['transformation_analysis']\n",
    "    llm_context_parts.append(f\"COLUMN: {target_column_name}\")\n",
    "    llm_context_parts.append(f\"EXPRESSION: {target_column_info['expression']}\")\n",
    "    llm_context_parts.append(f\"TRANSFORMATION TYPE: {expr_analysis['type']}\")\n",
    "    llm_context_parts.append(f\"TRANSFORMATION DETAILS: {expr_analysis['transformation_details']}\")\n",
    "    \n",
    "    if target_select_branch:\n",
    "        llm_context_parts.append(f\"FOUND IN: SELECT branch {target_select_branch}\")\n",
    "    \n",
    "    # Add data flow and control flow information\n",
    "    if expr_analysis.get('data_flow_deps'):\n",
    "        data_deps = [f\"{table}.{col}\" if table else col for table, col in expr_analysis['data_flow_deps']]\n",
    "        llm_context_parts.append(f\"DATA DEPENDENCIES: {', '.join(data_deps)}\")\n",
    "    \n",
    "    if expr_analysis.get('control_flow_deps'):\n",
    "        control_deps = [f\"{table}.{col}\" if table else col for table, col in expr_analysis['control_flow_deps']]\n",
    "        llm_context_parts.append(f\"CONTROL DEPENDENCIES: {', '.join(control_deps)}\")\n",
    "    \n",
    "    # Add specific transformation context\n",
    "    if expr_analysis['type'] == 'window_function':\n",
    "        if expr_analysis.get('partition_by'):\n",
    "            partition_cols = [f\"{table}.{col}\" if table else col for table, col in expr_analysis['partition_by']]\n",
    "            llm_context_parts.append(f\"PARTITIONED BY: {', '.join(partition_cols)}\")\n",
    "        if expr_analysis.get('order_by'):\n",
    "            order_cols = [f\"{table}.{col}\" if table else col for table, col in expr_analysis['order_by']]\n",
    "            llm_context_parts.append(f\"ORDERED BY: {', '.join(order_cols)}\")\n",
    "    \n",
    "    # Process lineage chains to build context and find next columns\n",
    "    if target_column_info.get('lineage_chains'):\n",
    "        llm_context_parts.append(\"\\nLINEAGE TRACE:\")\n",
    "        \n",
    "        for chain in target_column_info['lineage_chains']:\n",
    "            llm_context_parts.append(f\"\\nSource: {chain['source_reference']}\")\n",
    "            if chain.get('resolved_reference') and chain['resolved_reference'] != chain['source_reference']:\n",
    "                llm_context_parts.append(f\"Resolved as: {chain['resolved_reference']}\")\n",
    "            \n",
    "            # Process each step in the lineage chain\n",
    "            for step_idx, step in enumerate(chain['lineage_chain']):\n",
    "                if 'error' in step:\n",
    "                    llm_context_parts.append(f\"  ERROR: {step['error']}\")\n",
    "                elif step['step'] == 'base_table':\n",
    "                    llm_context_parts.append(f\"  └─ BASE TABLE: {step['source_table']}.{step['source_column']}\")\n",
    "                    # This is a source table - add to next_columns\n",
    "                    next_columns.append({\n",
    "                        \"table\": step['source_table'],\n",
    "                        \"column\": step['source_column'],\n",
    "                        \"context\": f\"Ultimate source column for {target_column_name}\",\n",
    "                        \"level\": \"base_table\"\n",
    "                    })\n",
    "                else:\n",
    "                    # This is a CTE step - also add these as intermediate dependencies to search\n",
    "                    llm_context_parts.append(f\"  └─ CTE '{step['cte_name']}': {step['expression']}\")\n",
    "                    llm_context_parts.append(f\"     Transformation: {step['transformation_type']}\")\n",
    "                    \n",
    "                    # Add CTE as an intermediate dependency to search\n",
    "                    next_columns.append({\n",
    "                        \"table\": step['cte_name'],\n",
    "                        \"column\": chain['source_reference'].split('.')[-1],  # Extract column name\n",
    "                        \"context\": f\"Intermediate transformation for {target_column_name} via CTE {step['cte_name']}\",\n",
    "                        \"level\": \"cte_intermediate\"\n",
    "                    })\n",
    "    else:\n",
    "        # No lineage chains - check resolved sources directly\n",
    "        resolved_sources = target_column_info.get('resolved_sources', [])\n",
    "        if resolved_sources:\n",
    "            llm_context_parts.append(\"\\nDIRECT SOURCES:\")\n",
    "            for source in resolved_sources:\n",
    "                if len(source) >= 3:  # (table, alias, column)\n",
    "                    table, alias, column = source[:3]\n",
    "                    llm_context_parts.append(f\"  └─ {table}.{column} (referenced as {alias}.{column})\")\n",
    "                    next_columns.append({\n",
    "                        \"table\": table,\n",
    "                        \"column\": column,\n",
    "                        \"context\": f\"Direct source for {target_column_name}\",\n",
    "                        \"level\": \"direct\"\n",
    "                    })\n",
    "                elif len(source) == 2:  # (table, column)\n",
    "                    table, column = source\n",
    "                    llm_context_parts.append(f\"  └─ {table}.{column}\")\n",
    "                    next_columns.append({\n",
    "                        \"table\": table,\n",
    "                        \"column\": column,\n",
    "                        \"context\": f\"Direct source for {target_column_name}\",\n",
    "                        \"level\": \"direct\"\n",
    "                    })\n",
    "    \n",
    "    # IMPORTANT: Also add all data flow and control flow dependencies as searchable columns\n",
    "    all_dependencies = []\n",
    "    all_dependencies.extend(expr_analysis.get('data_flow_deps', []))\n",
    "    all_dependencies.extend(expr_analysis.get('control_flow_deps', []))\n",
    "    \n",
    "    for table_ref, col_name in all_dependencies:\n",
    "        if table_ref:  # Skip empty table references\n",
    "            # Try to resolve the table reference through resolved sources\n",
    "            resolved_table = table_ref\n",
    "            \n",
    "            # Check if this is a CTE reference that we need to resolve\n",
    "            resolved_sources = target_column_info.get('resolved_sources', [])\n",
    "            for source in resolved_sources:\n",
    "                if len(source) >= 3 and source[1] == table_ref:  # (full_table, alias, column)\n",
    "                    resolved_table = source[0]\n",
    "                    break\n",
    "                elif len(source) == 2 and table_ref in source[0]:  # Partial match\n",
    "                    resolved_table = source[0]\n",
    "                    break\n",
    "            \n",
    "            next_columns.append({\n",
    "                \"table\": resolved_table,\n",
    "                \"column\": col_name,\n",
    "                \"context\": f\"Expression dependency for {target_column_name} (referenced as {table_ref}.{col_name})\",\n",
    "                \"level\": \"expression_dependency\"\n",
    "            })\n",
    "    \n",
    "    # Add CTEs context if relevant\n",
    "    cte_registry = analysis.get('cte_registry', {})\n",
    "    if cte_registry:\n",
    "        referenced_ctes = []\n",
    "        for chain in target_column_info.get('lineage_chains', []):\n",
    "            for step in chain['lineage_chain']:\n",
    "                if step['step'] != 'base_table' and 'cte_name' in step:\n",
    "                    referenced_ctes.append(step['cte_name'])\n",
    "        \n",
    "        if referenced_ctes:\n",
    "            llm_context_parts.append(f\"\\nRELEVANT CTEs:\")\n",
    "            for cte_name in set(referenced_ctes):\n",
    "                if cte_name.lower() in cte_registry:\n",
    "                    llm_context_parts.append(f\"\\nCTE '{cte_name}':\")\n",
    "                    llm_context_parts.append(f\"{cte_registry[cte_name.lower()]}\")\n",
    "    \n",
    "    # Remove duplicates from next_columns but keep different levels\n",
    "    unique_next_columns = []\n",
    "    seen = set()\n",
    "    for col in next_columns:\n",
    "        key = (col['table'], col['column'], col['level'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_next_columns.append(col)\n",
    "    \n",
    "    return {\n",
    "        \"llm_context\": \"\\n\".join(llm_context_parts),\n",
    "        \"next_columns_to_search\": unique_next_columns,\n",
    "        \"full_lineage\": target_column_info\n",
    "    }\n",
    "\n",
    "\n",
    "def build_complete_column_context(sql_query, target_column_name, external_column_fetcher=None):\n",
    "    \"\"\"\n",
    "    Builds complete LLM context by recursively following all source columns.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: The SQL query to analyze\n",
    "        target_column_name: Name of the final output column to trace\n",
    "        external_column_fetcher: Optional function(table, column) -> context_string for external data\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"complete_context\": \"Full LLM-ready context including external sources\",\n",
    "            \"source_tables_analyzed\": [\"list\", \"of\", \"tables\"],\n",
    "            \"lineage_summary\": \"High-level summary of the data flow\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_trace = trace_column_lineage(sql_query, target_column_name)\n",
    "    \n",
    "    if 'error' in initial_trace:\n",
    "        return initial_trace\n",
    "    \n",
    "    context_parts = [initial_trace['llm_context']]\n",
    "    next_to_search = initial_trace['next_columns_to_search']\n",
    "    searched_tables = set()\n",
    "    \n",
    "    # If we have an external column fetcher, get additional context\n",
    "    if external_column_fetcher and next_to_search:\n",
    "        context_parts.append(\"\\n\" + \"=\"*60)\n",
    "        context_parts.append(\"EXTERNAL SOURCE COLUMN INFORMATION:\")\n",
    "        context_parts.append(\"=\"*60)\n",
    "        \n",
    "        # Group by level for better organization\n",
    "        base_tables = [col for col in next_to_search if col.get('level') == 'base_table']\n",
    "        cte_intermediates = [col for col in next_to_search if col.get('level') == 'cte_intermediate']\n",
    "        expression_deps = [col for col in next_to_search if col.get('level') == 'expression_dependency']\n",
    "        direct_sources = [col for col in next_to_search if col.get('level') == 'direct']\n",
    "        \n",
    "        # Process base tables first (ultimate sources)\n",
    "        if base_tables:\n",
    "            context_parts.append(\"\\nULTIMATE SOURCE TABLES:\")\n",
    "            for col_info in base_tables:\n",
    "                table = col_info['table']\n",
    "                column = col_info['column']\n",
    "                \n",
    "                if (table, column) not in searched_tables:\n",
    "                    searched_tables.add((table, column))\n",
    "                    \n",
    "                    try:\n",
    "                        external_context = external_column_fetcher(table, column)\n",
    "                        context_parts.append(f\"\\n  SOURCE: {table}.{column}\")\n",
    "                        context_parts.append(f\"  CONTEXT: {col_info['context']}\")\n",
    "                        context_parts.append(f\"  EXTERNAL INFO: {external_context}\")\n",
    "                    except Exception as e:\n",
    "                        context_parts.append(f\"\\n  SOURCE: {table}.{column}\")\n",
    "                        context_parts.append(f\"  CONTEXT: {col_info['context']}\")\n",
    "                        context_parts.append(f\"  ERROR fetching external info: {str(e)}\")\n",
    "        \n",
    "        # Process expression dependencies (intermediate transformations)\n",
    "        if expression_deps:\n",
    "            context_parts.append(\"\\nEXPRESSION DEPENDENCIES:\")\n",
    "            for col_info in expression_deps:\n",
    "                table = col_info['table']\n",
    "                column = col_info['column']\n",
    "                \n",
    "                if (table, column) not in searched_tables:\n",
    "                    searched_tables.add((table, column))\n",
    "                    \n",
    "                    try:\n",
    "                        external_context = external_column_fetcher(table, column)\n",
    "                        context_parts.append(f\"\\n  DEPENDENCY: {table}.{column}\")\n",
    "                        context_parts.append(f\"  CONTEXT: {col_info['context']}\")\n",
    "                        context_parts.append(f\"  EXTERNAL INFO: {external_context}\")\n",
    "                    except Exception as e:\n",
    "                        context_parts.append(f\"\\n  DEPENDENCY: {table}.{column}\")\n",
    "                        context_parts.append(f\"  CONTEXT: {col_info['context']}\")\n",
    "                        context_parts.append(f\"  ERROR fetching external info: {str(e)}\")\n",
    "        \n",
    "        # Process CTE intermediates and direct sources\n",
    "        remaining_cols = cte_intermediates + direct_sources\n",
    "        if remaining_cols:\n",
    "            context_parts.append(\"\\nOTHER DEPENDENCIES:\")\n",
    "            for col_info in remaining_cols:\n",
    "                table = col_info['table']\n",
    "                column = col_info['column']\n",
    "                \n",
    "                if (table, column) not in searched_tables:\n",
    "                    searched_tables.add((table, column))\n",
    "                    \n",
    "                    try:\n",
    "                        external_context = external_column_fetcher(table, column)\n",
    "                        context_parts.append(f\"\\n  {col_info.get('level', 'other').upper()}: {table}.{column}\")\n",
    "                        context_parts.append(f\"  CONTEXT: {col_info['context']}\")\n",
    "                        context_parts.append(f\"  EXTERNAL INFO: {external_context}\")\n",
    "                    except Exception as e:\n",
    "                        context_parts.append(f\"\\n  {col_info.get('level', 'other').upper()}: {table}.{column}\")\n",
    "                        context_parts.append(f\"  CONTEXT: {col_info['context']}\")\n",
    "                        context_parts.append(f\"  ERROR fetching external info: {str(e)}\")\n",
    "    \n",
    "    # Build lineage summary with more detail\n",
    "    all_base_sources = [col for col in next_to_search if col.get('level') == 'base_table']\n",
    "    all_expression_deps = [col for col in next_to_search if col.get('level') == 'expression_dependency']\n",
    "    \n",
    "    base_tables = list(set([col['table'] for col in all_base_sources]))\n",
    "    dep_tables = list(set([col['table'] for col in all_expression_deps]))\n",
    "    \n",
    "    if base_tables and dep_tables:\n",
    "        lineage_summary = f\"Column '{target_column_name}' is derived from {len(all_expression_deps)} intermediate column(s) which ultimately trace to {len(all_base_sources)} base column(s) in {len(base_tables)} table(s): {', '.join(base_tables)}\"\n",
    "    elif base_tables:\n",
    "        lineage_summary = f\"Column '{target_column_name}' is derived from {len(all_base_sources)} column(s) in {len(base_tables)} table(s): {', '.join(base_tables)}\"\n",
    "    else:\n",
    "        source_tables = list(set([col['table'] for col in next_to_search]))\n",
    "        lineage_summary = f\"Column '{target_column_name}' is derived from {len(next_to_search)} column(s) across {len(source_tables)} sources: {', '.join(source_tables)}\"\n",
    "    \n",
    "    return {\n",
    "        \"complete_context\": \"\\n\".join(context_parts),\n",
    "        \"source_tables_analyzed\": base_tables if base_tables else list(set([col['table'] for col in next_to_search])),\n",
    "        \"lineage_summary\": lineage_summary,\n",
    "        \"next_columns_to_search\": next_to_search\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the new functions\n",
    "# Test queries for comprehensive testing\n",
    "test_queries = [\n",
    "    # 1. Simple UNION with single tables\n",
    "    \"\"\"\n",
    "    SELECT id FROM db1.schema1.tableA\n",
    "    UNION\n",
    "    SELECT id FROM db2.schema2.tableB\n",
    "    \"\"\",\n",
    "\n",
    "    # 2. UNION ALL with JOIN and subquery\n",
    "    \"\"\"\n",
    "    SELECT u.user_id, o.order_id\n",
    "    FROM analytics.users u\n",
    "    JOIN analytics.orders o ON u.user_id = o.user_id\n",
    "    UNION ALL\n",
    "    SELECT user_id, NULL\n",
    "    FROM analytics.inactive_users\n",
    "    WHERE last_login < '2024-01-01'\n",
    "    \"\"\",\n",
    "\n",
    "    # 3. UNION with nested SELECT and CTE\n",
    "    \"\"\"\n",
    "    WITH recent_orders AS (\n",
    "        SELECT order_id, customer_id\n",
    "        FROM sales.orders\n",
    "        WHERE order_date > '2025-01-01'\n",
    "    )\n",
    "    SELECT customer_id FROM recent_orders\n",
    "    UNION\n",
    "    SELECT customer_id FROM sales.customers\n",
    "    WHERE signup_date > '2025-01-01'\n",
    "    UNION ALL\n",
    "    SELECT customer_id FROM marketing.leads\n",
    "    WHERE source = 'web'\n",
    "    \"\"\",\n",
    "\n",
    "    # 4. Complex CTE with multiple UNIONs\n",
    "    \"\"\"\n",
    "    WITH active_customers AS (\n",
    "        SELECT customer_id\n",
    "        FROM crm_db.sales.customers\n",
    "        WHERE status = 'active'\n",
    "    ),\n",
    "    recent_orders AS (\n",
    "        SELECT order_id, customer_id\n",
    "        FROM crm_db.sales.orders\n",
    "        WHERE order_date > '2025-01-01'\n",
    "    ),\n",
    "    top_products AS (\n",
    "        SELECT product_id\n",
    "        FROM crm_db.sales.products\n",
    "        WHERE rating > 4.5\n",
    "    )\n",
    "    SELECT ac.customer_id, ro.order_id\n",
    "    FROM active_customers ac\n",
    "    JOIN recent_orders ro ON ac.customer_id = ro.customer_id\n",
    "    UNION\n",
    "    SELECT customer_id, NULL\n",
    "    FROM crm_db.marketing.leads\n",
    "    WHERE source = 'web'\n",
    "    UNION ALL\n",
    "    SELECT NULL, order_id\n",
    "    FROM recent_orders\n",
    "    WHERE order_id NOT IN (SELECT order_id FROM crm_db.sales.returns)\n",
    "    \"\"\",\n",
    "\n",
    "    # 5. Complex DBT-style query with JSON parsing and advanced functions\n",
    "    \"\"\"\n",
    "    WITH __dbt__cte__dummy_data AS (\n",
    "        SELECT\n",
    "            upper(nullif(v:DUMMY_VER_NAME::STRING,'')) AS dummy_ver_name,\n",
    "            upper(nullif(v:DUMMY_POP_NAME::STRING,'')) AS dummy_pop_name,\n",
    "            upper(nullif(v:DUMMY_LEVEL_CD::STRING,'')) AS dummy_level_cd,\n",
    "            upper(nullif(v:DUMMY_VAR_NAME::STRING,'')) AS dummy_var_name,\n",
    "            nullif(v:DUMMY_COEF::STRING,'')::NUMBER(8,3) AS dummy_coef\n",
    "        FROM dummy_schema.dummy_table\n",
    "    ),\n",
    "    get_dummy_data AS (\n",
    "        SELECT\n",
    "            dummy_ver_name,\n",
    "            dummy_pop_name,\n",
    "            dummy_level_cd,\n",
    "            dummy_var_name,\n",
    "            dummy_coef\n",
    "        FROM __dbt__cte__dummy_data\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(gd.dummy_ver_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_pop_name::VARCHAR, '') || '~' || COALESCE(gd.dummy_level_cd::VARCHAR, '') || '~' || COALESCE(gd.dummy_var_name::VARCHAR, '') AS dummy_id,\n",
    "        dd.dummy_key,\n",
    "        gd.dummy_ver_name,\n",
    "        gd.dummy_pop_name,\n",
    "        gd.dummy_level_cd,\n",
    "        gd.dummy_var_name,\n",
    "        gd.dummy_coef\n",
    "    FROM get_dummy_data gd\n",
    "    INNER JOIN dummy_schema.dummy_dim dd ON gd.dummy_ver_name = dd.dummy_ver_name\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Test function to run through all queries\n",
    "def test_all_queries():\n",
    "    \"\"\"Test all queries with both basic analysis and lineage tracing\"\"\"\n",
    "    \n",
    "    # Example external column fetcher\n",
    "    def mock_external_fetcher(table, column):\n",
    "        \"\"\"Mock function to simulate fetching external column information\"\"\"\n",
    "        mock_data = {\n",
    "            (\"db1.schema1.tableA\", \"id\"): \"Primary key for table A. Type: INTEGER.\",\n",
    "            (\"db2.schema2.tableB\", \"id\"): \"Primary key for table B. Type: INTEGER.\", \n",
    "            (\"analytics.users\", \"user_id\"): \"Unique user identifier. Type: INTEGER.\",\n",
    "            (\"analytics.orders\", \"order_id\"): \"Unique order identifier. Type: INTEGER.\",\n",
    "            (\"analytics.inactive_users\", \"user_id\"): \"User ID for inactive users. Type: INTEGER.\",\n",
    "            (\"sales.orders\", \"customer_id\"): \"Foreign key to customers. Type: INTEGER.\",\n",
    "            (\"sales.customers\", \"customer_id\"): \"Primary key for customers. Type: INTEGER.\",\n",
    "            (\"marketing.leads\", \"customer_id\"): \"Lead customer identifier. Type: INTEGER.\",\n",
    "            (\"crm_db.sales.customers\", \"customer_id\"): \"Primary customer ID. Type: INTEGER.\",\n",
    "            (\"crm_db.sales.orders\", \"order_id\"): \"Primary order ID. Type: INTEGER.\",\n",
    "            (\"dummy_schema.dummy_table\", \"v\"): \"JSON/VARIANT column containing nested data.\",\n",
    "            (\"dummy_schema.dummy_dim\", \"dummy_key\"): \"Dimension key for dummy data. Type: VARCHAR.\"\n",
    "        }\n",
    "        return mock_data.get((table, column), f\"No additional information available for {table}.{column}\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries[4:5], 1):\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"TEST QUERY {i}\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"SQL:\\n{query}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Basic column analysis\n",
    "            print(\"BASIC COLUMN ANALYSIS:\")\n",
    "            print(\"-\" * 50)\n",
    "            basic_analysis = extract_snowflake_columns(query)\n",
    "            \n",
    "            final_columns = []\n",
    "            for select_idx, select_columns in enumerate(basic_analysis):\n",
    "                print(f\"SELECT branch {select_idx+1}:\")\n",
    "                for col in select_columns:\n",
    "                    print(f\"  {col['target_column']}: {col['expression']} (Type: {col['type']})\")\n",
    "                    if select_idx == 0:  # Only trace columns from first (final) SELECT\n",
    "                        final_columns.append(col['target_column'])\n",
    "            \n",
    "            if final_columns:\n",
    "                print(f\"\\nFINAL OUTPUT COLUMNS: {final_columns}\")\n",
    "                \n",
    "                # Pick the first non-NULL column to trace\n",
    "                target_column = None\n",
    "                for col_name in final_columns:\n",
    "                    if col_name.upper() not in ['NULL', 'NONE']:\n",
    "                        target_column = col_name\n",
    "                        break\n",
    "                \n",
    "                if target_column:\n",
    "                    print(f\"\\nTRACING LINEAGE FOR COLUMN: {target_column}\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "                    lineage_trace = trace_column_lineage(query, target_column)\n",
    "                    \n",
    "                    if 'error' in lineage_trace:\n",
    "                        print(f\"ERROR: {lineage_trace['error']}\")\n",
    "                    else:\n",
    "                        print(\"LLM CONTEXT:\")\n",
    "                        print(lineage_trace['llm_context'])\n",
    "                        \n",
    "                        if lineage_trace['next_columns_to_search']:\n",
    "                            print(f\"\\nNEXT COLUMNS TO SEARCH ({len(lineage_trace['next_columns_to_search'])}):\")\n",
    "                            \n",
    "                            # Group by level for better display\n",
    "                            by_level = {}\n",
    "                            for col in lineage_trace['next_columns_to_search']:\n",
    "                                level = col.get('level', 'unknown')\n",
    "                                if level not in by_level:\n",
    "                                    by_level[level] = []\n",
    "                                by_level[level].append(col)\n",
    "                            \n",
    "                            level_order = ['base_table', 'expression_dependency', 'direct', 'cte_intermediate']\n",
    "                            for level in level_order:\n",
    "                                if level in by_level:\n",
    "                                    print(f\"  {level.upper().replace('_', ' ')}:\")\n",
    "                                    for col in by_level[level]:\n",
    "                                        print(f\"    - {col['table']}.{col['column']}\")\n",
    "                            \n",
    "                            print(\"\\nCOMPLETE CONTEXT WITH EXTERNAL DATA:\")\n",
    "                            print(\"-\" * 50)\n",
    "                            complete_context = build_complete_column_context(query, target_column, mock_external_fetcher)\n",
    "                            print(complete_context['lineage_summary'])\n",
    "                        else:\n",
    "                            print(\"\\nNo source columns to search externally.\")\n",
    "                else:\n",
    "                    print(\"\\nNo traceable columns found (all NULL).\")\n",
    "            else:\n",
    "                print(\"\\nNo final columns found.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing query {i}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        print(\"\\n\" * 2)\n",
    "\n",
    "# Run the comprehensive test\n",
    "test_all_queries()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
